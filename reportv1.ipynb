{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "talk about dual modality of data, \n",
    "check if there are strang trip durations\n",
    "split data before finding count\n",
    ".unique so that rows aren't repeated\n",
    "ridge or lasso?\n",
    "add comment about why start and end stations aren't the same\n",
    "<span style=\"color:red\">REMOVE THIS BEFORE FINAL VERSION</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Business Analytics Project Assignment \n",
    "\n",
    "### Group Members\n",
    "- Stanisław Howard, s231719\n",
    "- Alexis Van den Heede, s231860\n",
    "- Matthias Van Mechelen, s231739\n",
    "- Sven Palac, s231799\n",
    "- Salar Ravangouy Mytouei, s232038\n",
    "\n",
    "### Introduction & Data Analysis and Visualization\n",
    "This report presents an analysis of data and predictions for Citi Bike, a bike-sharing company based in New York, USA. The objective is to assist Citi Bike in optimizing its operations by examining current user trends and habits, utilizing this information to forecast demand for clusters of bike stations.\n",
    "\n",
    "The first section of the report, following this brief introduction, focuses on data analysis and preparation for subsequent sections through data cleaning and preprocessing. The second section presents the demand predictions derived from the initial data. Additionally, the third section outlines the exploratory component of the report, achieved by incorporating weather data into the initial dataset. Finally, the last section summarizes the conclusions drawn from this report.\n",
    "\n",
    "Additional explanation for the decisions made has been moved to Appendix A. Whenever a sentence is followed by (Appendix Ai), where i is a number, additional explanation to the sentence or previous sentences are provided in appendix A under the bullet point with tag Appendix Ai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stuff to add after reading peer reviews:  <span style=\"color:red\">REMOVE THIS BEFORE FINAL VERSION</span>\n",
    "- yes theree are birthyears of 1880 but that doesnt make the trips irrelevant, maybe joke of teenager that put old age or something, the trips are still relevant\n",
    "- Plot alexis sent Matthias about plotting the data per hour for each day, result of hist: you see a morning and evening peak on the weekdays and and no peaks in the weekends. \n",
    "- use elbow method for k means rather than just 20?\n",
    "- Are there NaNs in the gender column? (one of the reports stated that: So we are dealing with just over 4 million faulty values. That means almost 25% of our dataset cannot be used for gender specific analysis.Another observation is, that we have registerd almost 12 million trips undertaken by men, while we have only abbout 1.5 million trips undertaken by women. And the vast majority of these 1.5 million trips done by women between 50 and 55 years of age.)\n",
    "- Station 530 switched location 30-11-2018 (see peer review 3, but this is only one station, so maybe not worth mentioning)\n",
    "- look at peer report 3, very nice initial analysis, some more relevant than other, and they top it off with an insane prediciton model. \n",
    "- Add ones to the x_train\n",
    "- Transpose y values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data analysis starts by importing all the necessary Python libraries, loading the file with the bike-sharing data and quickly checking what the file includes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Trips_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete this later !!!!!!!!!!!!!!!!!!!!!!!\n",
    "df_copy = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun this to save time if you make a typo and need original df again, REMOVE LATER\n",
    "df = df_copy.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we import our data we need to inspect the content. As we can see a couple of things need to be handled immediately. For instance checking datatypes, missing values, duplicates and making sure the data is in the right format. We also need to handle the categorical values correctly. A brief comment in each code cell explains what was done in the specific cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's check the datatypes first\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column 0 is unnamed, change name to trip_id and convert column to index\n",
    "df.rename(columns={'Unnamed: 0':'trip_id'}, inplace=True)\n",
    "df.set_index('trip_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  convert start time and stop time to datetime objects\n",
    "df['starttime'] = pd.to_datetime(df['starttime'], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "df['stoptime'] = pd.to_datetime(df['stoptime'], format=\"%Y-%m-%d %H:%M:%S.%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of categorical datatype which is the user subscriber type\n",
    "df = pd.get_dummies(df, columns=['usertype'], dtype=int, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the datatypes have been handled, it is time to inspect actual values in the dataframe. We will look at the distribution of particular columns and check for outliers. Additionally, we will also check for duplicates and missing values. \n",
    "\n",
    "Let's first look at the locations of the start stations by plotting them to the map of New York City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting coordinates\n",
    "def plot_map(coords):\n",
    "    plt.scatter(coords[:,0], coords[:,1], s=0.75)\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.show()\n",
    "\n",
    "def plot_map_with_background(coords):\n",
    "    folder_path = 'data/nyc_folder'\n",
    "    shapefiles = [file for file in os.listdir(folder_path) if file.endswith('.shp')]\n",
    "    gdf = gpd.read_file(os.path.join(folder_path, shapefiles[0]))\n",
    "    ax = gdf.plot(figsize=(10, 10), alpha=0.1, edgecolor='k')\n",
    "    plt.title(f\"Visualization of Stations in NYC\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "\n",
    "    ### plot the coordinates on the map\n",
    "    plt.scatter(coords[:,0], coords[:,1], s=0.75, c='blue')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = df[['start_station_longitude','start_station_latitude']].values\n",
    "coords = np.unique(coords, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot initial coordinates\n",
    "plot_map(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the outlier which was identified as Montréal, Canada. Quickly remove it from the plot to get an overview of the rest of the stations which are expected to be located in New York City, USA. We will not yet delete the outlier data point from the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of outlier longitude > -73.6\n",
    "coords = coords[coords[:,0] < -73.6]\n",
    "print(coords.shape) #shape is 917 here as it still includes the grid anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the outlier we can see that the stations are located in the New York area. There still are a group of stations that seem odly located. Let's plot them on a map to see where they are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map_with_background(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very structured grid can be noticed in top right corner. After investigation these data points were NaNs, thus those data points need to be removed. We remove the Montréal, Canada outlier at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print where NaN is in data frame df, get rid of NaN (= get rid of gridded outliers), create copy of data frame df and start station coordinates\n",
    "print(df.isnull().sum())\n",
    "df = df[~np.isnan(df['start_station_id'])]\n",
    "df = df[~np.isnan(df['end_station_id'])]\n",
    "# get rid of Canada outlier\n",
    "df = df[df['start_station_longitude'] < -73.6]\n",
    "df = df[df['end_station_longitude'] < -73.6]\n",
    "# get latitude and longitude\n",
    "coords_start = df[['start_station_longitude','start_station_latitude']].values\n",
    "coords_start = np.unique(coords_start, axis=0)\n",
    "coords_start_copy = coords_start.copy()\n",
    "print(coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map_with_background(coords_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of distinct end stations does not match amount of start stations as seen from comparing shapes, create copy of end station coordinates.\n",
    "coords_end = df[['end_station_longitude','end_station_latitude']].values\n",
    "coords_end = np.unique(coords_end, axis=0)\n",
    "print(coords_end.shape)\n",
    "print(coords_start.shape)\n",
    "coords_end_copy = coords_end.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape comparison of start and end stations shows that the amount of distinct end stations does not match the number of start stations. The end stations will be plotted on the map of New York City. Additionally, the end stations that are not start station at the same time are going to be identified and highlighted by colouring them red on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map_with_background(coords_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight the end stations that are not in the start stations on the map of all end stations\n",
    "coords_end_not_in_start = []\n",
    "for i in range(len(coords_end)):\n",
    "    if coords_end[i] not in coords_start:\n",
    "        coords_end_not_in_start.append(coords_end[i])\n",
    "coords_end_not_in_start = np.array(coords_end_not_in_start)\n",
    "plt.scatter(coords_start[:,0], coords_start[:,1], s=0.75)\n",
    "plt.ylabel('Latitude')\n",
    "plt.xlabel('Longitude')\n",
    "plt.scatter(coords_end_not_in_start[:,0], coords_end_not_in_start[:,1], s=0.75, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are more end stations than start stations and all start stations are also an end station, the clustering should be done using the end stations in order to cluster every station considered in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look into the distribution of ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the ages of the users\n",
    "plt.hist(df['birth_year'], bins=120)\n",
    "plt.xlabel('Birth Year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Birth Years')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the distribution of ages dominated by the year 1969. Upon further inspection, we believe this is caused by automatically assigning the birth year 1969 to the users who are not logged in. Because of that, we will not use the birth year in prediction as we have no means of incorporating this data in a meaningful way.\n",
    "This concludes the data preparation so the prediction part of this report can be started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Challenge Part 1\n",
    "The prediction challenge part of this project starts with station clustering. It was decided that clustering is going to be done with sklearn library K-means algorithm with 20 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kmeans\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 20\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make kmeans model\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=random_state).fit(coords_end) # using coords here instead of df to increase speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained on coordinates instead of DataFrame to increase speed, then it is predicted to save labels on df, the model is trained on the same data as you predict the cluster for, so the cluster they belong to will be the same one as they belonged to during convergence of the K-means cluster.\n",
    "\n",
    "After the model converges, the clusters are illustrated assigning different colours to each cluster as well as marking their centroids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['drop_label'] = kmeans.predict(df[['end_station_longitude','end_station_latitude']].values)\n",
    "df['pick_label'] = kmeans.predict(df[['start_station_longitude','start_station_latitude']].values)\n",
    "df_copy2 = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with cluster colour & centriods\n",
    "plt.scatter(coords_end[:,0], coords_end[:,1], c=kmeans.labels_, s=5)\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c='red', marker=\"x\", s=50)\n",
    "plt.ylabel('Latitude')\n",
    "plt.xlabel('Longitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell for coords of starting stations and the fully edited dataframe\n",
    "coords_start = coords_start_copy.copy()\n",
    "coords_end = coords_end_copy.copy()\n",
    "df = df_copy2.copy()\n",
    "print(df.shape) # to check if no data loss, should be (17548339, 15)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Challenge Part 2\n",
    "\n",
    "For the prediction challenge posed in part 2 of the assignment, we will create a prediction model for all clusters, for both departures and arrivals, and for both weekday and weekend. (Appendix A1)\n",
    "\n",
    "We will also only train models for all clusters except the cluster where only arrivals have been observed, the cluster west of all other clusters in the figure above. (Appendix A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy full dataset so that you dont have to rerun the code from the top.\n",
    "df = df_copy2.copy()\n",
    "\n",
    "# Pick the clusters with the most amount of data, hence we leave the cluster with only arrivals out.\n",
    "n = 19 \n",
    "largest_cluster = df['drop_label'].value_counts().nlargest(n).index[:n]\n",
    "print(largest_cluster)\n",
    "\n",
    "# Get all data entries belonging to the largest clusters only \n",
    "df = df[df['pick_label'].isin(largest_cluster)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make dummies for all categorical gender variables, do not leave one out as it will be more convenient for the following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['gender'], dtype=int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code cell splits up the dataset into departures and arrivals data, which will allow us to make prediction models for both separately, which is necessary for part 3. (Appendix A3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant data features for prediction.  \n",
    "df_departures = df[['starttime', 'pick_label', 'tripduration', 'gender_0', 'gender_1', 'gender_2', 'usertype_Subscriber']].copy()\n",
    "df_arrivals = df[['stoptime', 'drop_label', 'tripduration', 'gender_0', 'gender_1', 'gender_2', 'usertype_Subscriber']].copy()\n",
    "\n",
    "# Rename label columns\n",
    "df_departures.rename(columns={'pick_label':'label'}, inplace=True)\n",
    "df_arrivals.rename(columns={'drop_label':'label'}, inplace=True)\n",
    "\n",
    "# Extract date and time components\n",
    "def extract_date_time_components(df, time_column):\n",
    "    df['hour'] = df[time_column].dt.hour\n",
    "    df['date'] = df[time_column].dt.date\n",
    "    df['month'] = df[time_column].dt.month\n",
    "    df.drop(columns=[time_column], inplace=True)\n",
    "\n",
    "extract_date_time_components(df_departures, 'starttime')\n",
    "extract_date_time_components(df_arrivals, 'stoptime')\n",
    "\n",
    "# Remove trip_id index, lost it's meaning\n",
    "df_departures.reset_index(drop=True, inplace=True)\n",
    "df_arrivals.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Function to split data into train and test based on month requirement.\n",
    "def split_train_test(df):\n",
    "    df_train = df[df['month'] < 11]\n",
    "    df_test = df[df['month'] >= 11]\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "# Count the number of rows with the same label, hour, date and save it in a new column. Function to compute the amount of departures/arrivals (depending on the dataset you look at)\n",
    "# for each label, hour, date.\n",
    "def count(df):\n",
    "    counts = df.groupby(['label', 'hour', 'date']).size().reset_index(name='count')\n",
    "    print(counts)\n",
    "    df = df.merge(counts, on=['label', 'hour', 'date'], how='left')\n",
    "    return df\n",
    "\n",
    "# Find the average trip duration for each label, hour, date and save it in a new column\n",
    "def avg_tripduration(df):\n",
    "    avg_tripduration = df.groupby(['label', 'hour', 'date'])['tripduration'].mean().reset_index(name='avg_tripduration')\n",
    "    print(avg_tripduration)\n",
    "    df = df.merge(avg_tripduration, on=['label', 'hour', 'date'], how='left')\n",
    "    return df\n",
    "\n",
    "# Find the percentage of males, females, and unknowns for each label, hour, date and save it in a new column\n",
    "def avg_gender(df):\n",
    "    percentage_gender_0 = df.groupby(['label', 'hour', 'date'])['gender_0'].mean().reset_index(name = 'percentage_gender_0')\n",
    "    percentage_gender_1 = df.groupby(['label', 'hour', 'date'])['gender_1'].mean().reset_index(name = 'percentage_gender_1')\n",
    "    percentage_gender_2 = df.groupby(['label', 'hour', 'date'])['gender_2'].mean().reset_index(name = 'percentage_gender_2')\n",
    "    print(percentage_gender_0)\n",
    "    print(percentage_gender_1)\n",
    "    print(percentage_gender_2)\n",
    "    df = df.merge(percentage_gender_0, on=['label', 'hour', 'date'], how='left')\n",
    "    df = df.merge(percentage_gender_1, on=['label', 'hour', 'date'], how='left')\n",
    "    df = df.merge(percentage_gender_2, on=['label', 'hour', 'date'], how='left')\n",
    "    return df\n",
    "    \n",
    "# Split data into train and test set\n",
    "df_departures_train, df_departures_test = split_train_test(df_departures)\n",
    "df_arrivals_train, df_arrivals_test = split_train_test(df_arrivals)\n",
    "\n",
    "# Count the number of rows with the same label, hour, date and save it in a new column\n",
    "df_departures_train = count(df_departures_train)\n",
    "df_departures_test = count(df_departures_test)\n",
    "df_arrivals_train = count(df_arrivals_train)\n",
    "df_arrivals_test = count(df_arrivals_test)\n",
    "\n",
    "# Find the average trip duration for each label, hour, date and save it in a new column\n",
    "df_departures_train = avg_tripduration(df_departures_train)\n",
    "df_departures_test = avg_tripduration(df_departures_test)\n",
    "df_arrivals_train = avg_tripduration(df_arrivals_train)\n",
    "df_arrivals_test = avg_tripduration(df_arrivals_test)\n",
    "\n",
    "# Find the average gender for each label, hour, date and save it in a new column\n",
    "df_departures_train = avg_gender(df_departures_train)\n",
    "df_departures_test = avg_gender(df_departures_test)\n",
    "df_arrivals_train = avg_gender(df_arrivals_train)\n",
    "df_arrivals_test = avg_gender(df_arrivals_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all unnecessary features, (Appendix A4). Sort data and drop all duplicate entries that remain after the groupby function is applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop month, yielded poor prediction quality\n",
    "df_departures_train.drop(columns=['month'], inplace=True)\n",
    "df_departures_test.drop(columns=['month'], inplace=True)\n",
    "df_arrivals_train.drop(columns=['month'], inplace=True)\n",
    "df_arrivals_test.drop(columns=['month'], inplace=True)\n",
    "\n",
    "# Drop tripduration, we will predict per label, hour, date, not per trip, so we only need avg_tripduration\n",
    "df_departures_train.drop(columns=['tripduration'], inplace=True)\n",
    "df_departures_test.drop(columns=['tripduration'], inplace=True)\n",
    "df_arrivals_train.drop(columns=['tripduration'], inplace=True)\n",
    "df_arrivals_test.drop(columns=['tripduration'], inplace=True)\n",
    "\n",
    "# Drop gender_0, gender_1 and gender_2 dummies, we will predict per label, hour, date, not per trip, so we only need the avg_gender.\n",
    "df_departures_train.drop(columns=['gender_0', 'gender_1', 'gender_2'], inplace=True)\n",
    "df_departures_test.drop(columns=['gender_0', 'gender_1', 'gender_2'], inplace=True)\n",
    "df_arrivals_train.drop(columns=['gender_0', 'gender_1', 'gender_2'], inplace=True)\n",
    "df_arrivals_test.drop(columns=['gender_0', 'gender_1', 'gender_2'], inplace=True)\n",
    "\n",
    "# Drop usertype_Subscriber, we have no aggregated data for this. We could have made a column with the percentage of subscribers per label, hour, date, but we decided to drop it for simplicity. \n",
    "# Later on we will also get rid of avg_tripduration since the model achieved nearly the same accuracy without it. Hence we decided adding these features would not yield improvement, if any. \n",
    "df_departures_train.drop(columns=['usertype_Subscriber'], inplace=True)\n",
    "df_departures_test.drop(columns=['usertype_Subscriber'], inplace=True)\n",
    "df_arrivals_train.drop(columns=['usertype_Subscriber'], inplace=True)\n",
    "df_arrivals_test.drop(columns=['usertype_Subscriber'], inplace=True)\n",
    "\n",
    "# Sort data by hour, date\n",
    "df_departures_train.sort_values(by=['date', 'hour'], inplace=True)\n",
    "df_departures_test.sort_values(by=['date', 'hour'], inplace=True)\n",
    "df_arrivals_train.sort_values(by=['date', 'hour'], inplace=True)\n",
    "df_arrivals_test.sort_values(by=['date', 'hour'], inplace=True)\n",
    "\n",
    "# Keep unique rows\n",
    "df_departures_train.drop_duplicates(inplace=True)\n",
    "df_departures_test.drop_duplicates(inplace=True)\n",
    "df_arrivals_train.drop_duplicates(inplace=True)\n",
    "df_arrivals_test.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the data is now: sorted correctly, grouped correctly, has the correct variables, had no duplicates, has data for all clusters. Check sizes as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_departures_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_departures_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arrivals_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arrivals_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check sizes \n",
    "print(df_departures_train.shape)\n",
    "print(df_departures_test.shape)\n",
    "print(df_arrivals_train.shape)\n",
    "print(df_arrivals_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is additionally split per cluster since that is necessary for training a model per cluster. From now on, the code gets intuitively complex ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data per label, n was the amount of largest clusters you desired. Note that in this step, we sort of \"relabel\" the clusters. The largest cluster with label x will get a new \"index\" 0.\n",
    "# To keep track of which cluster is which, we will have to convert back during visualization.\n",
    "for i in range(n):\n",
    "    globals()['df_departures_train_{}'.format(i)] = df_departures_train[df_departures_train['label'] == largest_cluster[i]]\n",
    "    globals()['df_departures_test_{}'.format(i)] = df_departures_test[df_departures_test['label'] == largest_cluster[i]]\n",
    "    globals()['df_arrivals_train_{}'.format(i)] = df_arrivals_train[df_arrivals_train['label'] == largest_cluster[i]]\n",
    "    globals()['df_arrivals_test_{}'.format(i)] = df_arrivals_test[df_arrivals_test['label'] == largest_cluster[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape check 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    print(\"Shapes of cluster {}:\".format(i))\n",
    "    print(globals()['df_departures_train_{}'.format(i)].shape)\n",
    "    print(globals()['df_departures_test_{}'.format(i)].shape)\n",
    "    print(globals()['df_arrivals_train_{}'.format(i)].shape)\n",
    "    print(globals()['df_arrivals_test_{}'.format(i)].shape)\n",
    "    print(\"\")\n",
    "\n",
    "# In the remaining dataset, we should see properly sorted data on date and hour, and only one cluster label. Note that the cluster label does not match the index of the variable, and that is due \n",
    "# to the sorting of the clusters according to size we performed at the start of part 2 and the step above.\n",
    "df_departures_train_0.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the departures dataset, we appear to be missing some datapoints. In 304 days (amount of days in the period from January to October) there are 7296 hours, however from analyzing the shapes of the different cluster datasets, we never actually have this amount. (Appendix A5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of the demand during a random weekday to show an example of demand peaks in the morning and evening rush hours\n",
    "weekday = df_departures_train_0[df_departures_train_0['date'] == dt.datetime(2018, 1, 1)]\n",
    "hours_weekday = weekday['hour']\n",
    "counts_weekday = weekday['count']\n",
    "\n",
    "plt.plot(hours_weekday, counts_weekday)\n",
    "plt.ylabel('Number of trips')\n",
    "plt.xlabel('Time')\n",
    "plt.title('Demand distribution')\n",
    "plt.show()\n",
    "\n",
    "# visualisation of the demand during a random weekend day to show an example of absence of demand peaks\n",
    "weekend = df_departures_train_0[df_departures_train_0['date'] == dt.datetime(2018, 1, 6)]\n",
    "hours_weekend = weekend['hour']\n",
    "counts_weekend = weekend['count']\n",
    "\n",
    "plt.plot(hours_weekday, counts_weekday)\n",
    "plt.ylabel('Number of trips')\n",
    "plt.xlabel('Time')\n",
    "plt.title('Demand distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Build lagged features code provided from stackoverflow to give each sub-dataset columns with lagged departures/arrivals (depending on the dataset you look at) counts. This yields better prediction.\n",
    "\n",
    "**Note that from now on, when departures/arrivals is used, we imply that, depending on the dataset used, you use data on departures or data on arrivals.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLaggedFeatures(s,columns, lag=24,dropna=True):\n",
    "    '''\n",
    "    From http://stackoverflow.com/questions/20410312/how-to-create-a-lagged-data-structure-using-pandas-dataframe\n",
    "    Builds a new DataFrame to facilitate regressing over all possible lagged features\n",
    "    '''\n",
    "    if type(s) is pd.DataFrame:\n",
    "        new_dict={}\n",
    "        for c in s.columns:\n",
    "            new_dict[c]=s[c]\n",
    "        for col_name in columns:\n",
    "            new_dict[col_name]=s[col_name]\n",
    "            # create lagged Series\n",
    "            for l in range(1,lag+1):\n",
    "                new_dict['%s_lag%d' %(col_name,l)]=s[col_name].shift(l)\n",
    "        res=pd.DataFrame(new_dict,index=s.index)\n",
    "\n",
    "    elif type(s) is pd.Series:\n",
    "        the_range=range(lag+1)\n",
    "        res=pd.concat([s.shift(i) for i in the_range],axis=1)\n",
    "        res.columns=['lag_%d' %i for i in the_range]\n",
    "    else:\n",
    "        print('Only works for DataFrame or Series')\n",
    "        return None\n",
    "    if dropna:\n",
    "        return res.dropna()\n",
    "    else:\n",
    "        return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lag to all cluster count column. highly advised for best prediction quality to choose 24 for both, however different lags can be chosen as well, all set up parametrically.\n",
    "n_lag_dep_train = 24\n",
    "n_lag_arr_train = 24\n",
    "\n",
    "for i in range(n):\n",
    "    globals()['departures_train_lagged{}'.format(i)] = buildLaggedFeatures(globals()['df_departures_train_{}'.format(i)], ['count'], lag=n_lag_dep_train)\n",
    "    globals()['arrivals_train_lagged{}'.format(i)] = buildLaggedFeatures(globals()['df_arrivals_train_{}'.format(i)], ['count'], lag=n_lag_arr_train)\n",
    "\n",
    "# Do same to test data\n",
    "n_lag_dep_test = n_lag_dep_train  #have to be the same for models to work, as otherwise you have a model fitted to taking in 30 features and then your test has 100...\n",
    "n_lag_arr_test = n_lag_arr_train\n",
    "\n",
    "for i in range(n):\n",
    "    globals()['departures_test_lagged{}'.format(i)] = buildLaggedFeatures(globals()['df_departures_test_{}'.format(i)], ['count'], lag=n_lag_dep_test)\n",
    "    globals()['arrivals_test_lagged{}'.format(i)] = buildLaggedFeatures(globals()['df_arrivals_test_{}'.format(i)], ['count'], lag=n_lag_arr_test)\n",
    "\n",
    "\n",
    "\n",
    "# Add lag to all cluster avg_tripduration column:\n",
    "n_lag_dep_train = 24\n",
    "n_lag_arr_train = 24\n",
    "\n",
    "for i in range(n):\n",
    "    globals()['departures_train_lagged_tripduration{}'.format(i)] = buildLaggedFeatures(globals()['df_departures_train_{}'.format(i)], ['avg_tripduration'], lag=n_lag_dep_train)\n",
    "    globals()['arrivals_train_lagged_tripduration{}'.format(i)] = buildLaggedFeatures(globals()['df_arrivals_test_{}'.format(i)], ['avg_tripduration'], lag=n_lag_arr_train)\n",
    "\n",
    "# Do same to test data\n",
    "n_lag_dep_test = n_lag_dep_train  #have to be the same for models to work, as otherwise you have a model fitted to taking in 30 features and then your test has 100...\n",
    "n_lag_arr_test = n_lag_arr_train\n",
    "\n",
    "for i in range(n):\n",
    "    globals()['departures_test_lagged_tripduration{}'.format(i)] = buildLaggedFeatures(globals()['df_departures_test_{}'.format(i)], ['avg_tripduration'], lag=n_lag_dep_test)\n",
    "    globals()['arrivals_test_lagged_tripduration{}'.format(i)] = buildLaggedFeatures(globals()['df_arrivals_test_{}'.format(i)], ['avg_tripduration'], lag=n_lag_arr_test)\n",
    "\n",
    "\n",
    "\n",
    "# Why lag 24? we have tried a tonne of combinations with different lag and different degree of poly added, but for all lag, degree 2 came out on top, and for degree 2, lag 24 gave by far the best \n",
    "# performance on all models. For lag 22 I got a >0.6 for a cluster with usual R^2 ~0.59, but it came with the cost of quite a bit of performance loss in the other models, while the model for \n",
    "# the poor performing cluster would only increase R^2 by 0.01 or so. So we decided to go with lag 24 for all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dates to datetime, in order to extract the \"day of week\" that date belongs to when building up the input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['departures_train_lagged{}'.format(i)]['date'] = pd.to_datetime(globals()['departures_train_lagged{}'.format(i)]['date'])\n",
    "globals()['departures_test_lagged{}'.format(i)]['date'] = pd.to_datetime(globals()['departures_test_lagged{}'.format(i)]['date'])\n",
    "globals()['arrivals_train_lagged{}'.format(i)]['date'] = pd.to_datetime(globals()['arrivals_train_lagged{}'.format(i)]['date'])\n",
    "globals()['arrivals_test_lagged{}'.format(i)]['date'] = pd.to_datetime(globals()['arrivals_test_lagged{}'.format(i)]['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create the skeleton for the input dataset of our model. More features based on this skeleton will be added later. An important note has to be made, provided in (Appendix A6). All features included in the dataset have been proven to yield the best predictions possible while keeping the input to the model limited (Appendix A7). To keep variable naming intuitive, the input data to our model will get the x variable assigned to it. The output will be assigned the y variable (later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all clusters training data: use np.c_ to concatenate. \n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_lagged{}'.format(i)] = np.c_[np.ones(len(globals()['departures_train_lagged{}'.format(i)]))]\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)] = np.c_[np.ones(len(globals()['arrivals_train_lagged{}'.format(i)]))]\n",
    "\n",
    "    # Add a column with the \"day of week\" to the x_departures_train variable. \n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())]\n",
    "\n",
    "    # convert \"day of week\" to dummy variables since it is a categorical, 0 = is_monday, 1 = is_tuesday, 2 = is_wednesday, 3 = is_thursday, 4 = is_friday, 5 = is_saturday, 6 = is_sunday\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 0 else 0)]\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 1 else 0)]\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 2 else 0)]\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 3 else 0)]\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 4 else 0)]\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 5 else 0)]\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 6 else 0)]\n",
    "\n",
    "    # Add the hour column to the x_departures_train variable\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['hour']]\n",
    "\n",
    "    # Add a column with the \"day of week\" to the x_arrivals_train variable.\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())]\n",
    "\n",
    "    # Convert \"day of week\" to dummy variables, 0 = is_monday, 1 = is_tuesday, 2 = is_wednesday, 3 = is_thursday, 4 = is_friday, 5 = is_saturday, 6 = is_sunday\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 0 else 0)]\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 1 else 0)]\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 2 else 0)]\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 3 else 0)]\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 4 else 0)]\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 5 else 0)]\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 6 else 0)]\n",
    "    \n",
    "    # Add the hour column to the x_arrivals_train variable\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['hour']]\n",
    "\n",
    "    # Add all the lagged counts to the x_train variable\n",
    "    for j in range(1, n_lag_dep_train+1):\n",
    "        globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['count_lag{}'.format(j)]]\n",
    "    for j in range(1, n_lag_arr_train+1):\n",
    "        globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['count_lag{}'.format(j)]]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# perform the same steps for the test data   \n",
    "    globals()['x_departures_test_lagged{}'.format(i)] = np.c_[np.ones(len(globals()['departures_test_lagged{}'.format(i)]))]\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)] = np.c_[np.ones(len(globals()['arrivals_test_lagged{}'.format(i)]))]\n",
    "    \n",
    "    # add a column with the \"day of week\" to the x_departures_test variable.\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())]\n",
    "\n",
    "    #convert \"day of week\" to dummy variables, 0 = is_monday, 1 = is_tuesday, 2 = is_wednesday, 3 = is_thursday, 4 = is_friday, 5 = is_saturday, 6 = is_sunday\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 0 else 0)]\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 1 else 0)]\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 2 else 0)]\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 3 else 0)]\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 4 else 0)]\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 5 else 0)]\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 6 else 0)]\n",
    "\n",
    "    #add the hour column to the x_departures_test variable\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['hour']]\n",
    "\n",
    "    # add a column with the \"day of week\" to the x_arrivals_train variable.\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())]\n",
    "\n",
    "    #convert \"day of week\" to dummy variables, 0 = is_monday, 1 = is_tuesday, 2 = is_wednesday, 3 = is_thursday, 4 = is_friday, 5 = is_saturday, 6 = is_sunday\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 0 else 0)]\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 1 else 0)]\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 2 else 0)]\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 3 else 0)]\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 4 else 0)]\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 5 else 0)]\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 6 else 0)]\n",
    "    \n",
    "    # Add the hour column to the x_arrivals_test variable\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['hour']]\n",
    "    \n",
    "    #also add all the lagged counts to the x_test variable\n",
    "    for j in range(1, n_lag_dep_test+1):\n",
    "        globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['count_lag{}'.format(j)]]\n",
    "    for j in range(1, n_lag_arr_test+1):\n",
    "        globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['count_lag{}'.format(j)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the correctness of the code. The first column should contain all ones, the second includes the \"day of week\" as a continuous variable (0-6), the next 7 contain the weekday as dummy variables, the tenth column is the hour, and the final 24 columns contain the lagged departures/arrivals counts. Whether the lagged counts have been added correctly can be answered by confirming that the value along the -45⁰ diagonal (CCW positive) is constant for the final 24 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(globals()['x_departures_train_lagged{}'.format(0)]).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape check 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    print(\"Shapes of cluster {}:\".format(i))\n",
    "    print(globals()['x_departures_train_lagged{}'.format(i)].shape)\n",
    "    print(globals()['x_departures_test_lagged{}'.format(i)].shape)\n",
    "    print(globals()['x_arrivals_train_lagged{}'.format(i)].shape)\n",
    "    print(globals()['x_arrivals_test_lagged{}'.format(i)].shape)\n",
    "    print(\" \")\n",
    "\n",
    "pd.DataFrame(departures_train_lagged0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to shape check 1, each of the cluster split datasets has exactly 30 datpoints less. This was induced by applying the lagged build-up function on the dataset. (Appendix A8) \n",
    "\n",
    "The amount of columns should, for all datasets, add up to 10 + n_lag. (Appendix A9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more features for better prediction: \n",
    "- polynomial of degree n applied to hour and all lagged departures/arrivals counts. \n",
    "- sine of hour\n",
    "- cosine of hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polynomial(x_data, n):\n",
    "    x_data_poly = x_data.copy()\n",
    "    for i in range(2, n+1):\n",
    "        x_data_poly = np.concatenate((x_data_poly, np.power(x_data[:,9:], i)), axis=1)\n",
    "    return x_data_poly\n",
    "# The reason for not applying the poly on the entire data is important: otherwise we also apply it on the first column with all ones, and the \"day of week\" dummy variables column. Applying degree n on\n",
    "# either of these columns adds no meaning to the input data and will thus be avoided.\n",
    "\n",
    "\n",
    "# Add sine to hour\n",
    "def add_sine(x_data):\n",
    "    x_data_sine = x_data.copy()\n",
    "    x_data_sine = np.concatenate((x_data_sine, np.sin(x_data)), axis = 1)\n",
    "    return x_data_sine\n",
    "\n",
    "# Add cossine to hour\n",
    "def add_cosine(x_data):\n",
    "    x_data_cosine = x_data.copy()\n",
    "    x_data_cosine = np.concatenate((x_data_cosine, np.cos(x_data)), axis = 1)\n",
    "    return x_data_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polynomial features to all clusters input data. Degree 2 has been proven to yield best prediction performance, other degrees can be selected as well.\n",
    "degree = 2\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_poly{}'.format(i)] = add_polynomial(globals()['x_departures_train_lagged{}'.format(i)], degree)\n",
    "    globals()['x_departures_test_poly{}'.format(i)] = add_polynomial(globals()['x_departures_test_lagged{}'.format(i)], degree)\n",
    "    globals()['x_arrivals_train_poly{}'.format(i)] = add_polynomial(globals()['x_arrivals_train_lagged{}'.format(i)], degree)\n",
    "    globals()['x_arrivals_test_poly{}'.format(i)] = add_polynomial(globals()['x_arrivals_test_lagged{}'.format(i)], degree)\n",
    "\n",
    "# Why degree 2? Prediction performance of the model has been evaluated for degrees 2-7, degree 2 ensured the most accurate predictions, and more specifically, for the models that did not meet \n",
    "# the 0.6 threshold, degree 2 ensured their largest possible r^2. No matter what amount of lag we used, degree 2 proved to be optimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the add_sine function on the the hour column of the input data. From data structure knowledge, the hour data is provided in the tenth column. \n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_sine{}'.format(i)] = np.insert(globals()['x_departures_train_poly{}'.format(i)],    9,    add_sine(globals()['x_departures_train_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "    globals()['x_departures_test_sine{}'.format(i)] = np.insert(globals()['x_departures_test_poly{}'.format(i)],    9,    add_sine(globals()['x_departures_test_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "    globals()['x_arrivals_train_sine{}'.format(i)] = np.insert(globals()['x_arrivals_train_poly{}'.format(i)],    9,    add_sine(globals()['x_arrivals_train_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "    globals()['x_arrivals_test_sine{}'.format(i)] = np.insert(globals()['x_arrivals_test_poly{}'.format(i)],    9,    add_sine(globals()['x_arrivals_test_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "\n",
    "\n",
    "# The add_sine column inserts [hour, sin(hour)] at index 9, and thus moves the original content of index 9 to index 11. This original content is the hour, which will thus be present twice in the\n",
    "# dataset. We will thus delete the content of column 11 to avoid such duplicates. \n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_sine{}'.format(i)] = np.delete(globals()['x_departures_train_sine{}'.format(i)], 11, 1)\n",
    "    globals()['x_departures_test_sine{}'.format(i)] = np.delete(globals()['x_departures_test_sine{}'.format(i)], 11, 1)\n",
    "    globals()['x_arrivals_train_sine{}'.format(i)] = np.delete(globals()['x_arrivals_train_sine{}'.format(i)], 11, 1)\n",
    "    globals()['x_arrivals_test_sine{}'.format(i)] = np.delete(globals()['x_arrivals_test_sine{}'.format(i)], 11, 1)\n",
    "\n",
    "\n",
    "# Apply the add_cosine function on the the hour column of the input data. From data structure knowledge, the hour data is still provided in the tenth column. \n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_cosine_sine{}'.format(i)] = np.insert(globals()['x_departures_train_sine{}'.format(i)],    9,    add_cosine(globals()['x_departures_train_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "    globals()['x_departures_test_cosine_sine{}'.format(i)] = np.insert(globals()['x_departures_test_sine{}'.format(i)],    9,    add_cosine(globals()['x_departures_test_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "    globals()['x_arrivals_train_cosine_sine{}'.format(i)] = np.insert(globals()['x_arrivals_train_sine{}'.format(i)],    9,    add_cosine(globals()['x_arrivals_train_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "    globals()['x_arrivals_test_cosine_sine{}'.format(i)] = np.insert(globals()['x_arrivals_test_sine{}'.format(i)],    9,    add_cosine(globals()['x_arrivals_test_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "\n",
    "# The add_cosine column inserts [hour, cos(hour)] at index 9, and thus moves the original content of index 9 to index 11. This original content is the hour, which will thus be present twice in the\n",
    "# dataset. We will thus delete the content of column 11 to avoid such duplicates. \n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_cosine_sine{}'.format(i)] = np.delete(globals()['x_departures_train_cosine_sine{}'.format(i)], 11, 1)\n",
    "    globals()['x_departures_test_cosine_sine{}'.format(i)] = np.delete(globals()['x_departures_test_cosine_sine{}'.format(i)], 11, 1)\n",
    "    globals()['x_arrivals_train_cosine_sine{}'.format(i)] = np.delete(globals()['x_arrivals_train_cosine_sine{}'.format(i)], 11, 1)\n",
    "    globals()['x_arrivals_test_cosine_sine{}'.format(i)] = np.delete(globals()['x_arrivals_test_cosine_sine{}'.format(i)], 11, 1)\n",
    "\n",
    "# Now the sine and cosine are present in the input dataset columns 10 and 9 respectively. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the features you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which features you desire for the input data,        \"poly\"        <->        \"poly + sine\"         <->         \"poly + sine + cosine\"\n",
    "features = \"poly + sine + cosine\"\n",
    "if features == \"poly\":\n",
    "    for i in range(n):\n",
    "        globals()['x_departures_train{}'.format(i)] = globals()['x_departures_train_poly{}'.format(i)]\n",
    "        globals()['x_departures_test{}'.format(i)] = globals()['x_departures_test_poly{}'.format(i)]\n",
    "        globals()['x_arrivals_train{}'.format(i)] = globals()['x_arrivals_train_poly{}'.format(i)]\n",
    "        globals()['x_arrivals_test{}'.format(i)] = globals()['x_arrivals_test_poly{}'.format(i)]\n",
    "elif features == \"poly + sine\":\n",
    "    for i in range(n):\n",
    "        globals()['x_departures_train{}'.format(i)] = globals()['x_departures_train_sine{}'.format(i)]\n",
    "        globals()['x_departures_test{}'.format(i)] = globals()['x_departures_test_sine{}'.format(i)]\n",
    "        globals()['x_arrivals_train{}'.format(i)] = globals()['x_arrivals_train_sine{}'.format(i)]\n",
    "        globals()['x_arrivals_test{}'.format(i)] = globals()['x_arrivals_test_sine{}'.format(i)]\n",
    "else:\n",
    "    for i in range(n):\n",
    "        globals()['x_departures_train{}'.format(i)] = globals()['x_departures_train_cosine_sine{}'.format(i)]\n",
    "        globals()['x_departures_test{}'.format(i)] = globals()['x_departures_test_cosine_sine{}'.format(i)]\n",
    "        globals()['x_arrivals_train{}'.format(i)] = globals()['x_arrivals_train_cosine_sine{}'.format(i)]\n",
    "        globals()['x_arrivals_test{}'.format(i)] = globals()['x_arrivals_test_cosine_sine{}'.format(i)]\n",
    "\n",
    "\n",
    "# It is advised to choose poly + sine + cosine, as this generally yielded the best possible prediction performance of the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape check 3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    print(\"Shapes of cluster {}:\".format(i))\n",
    "    print(globals()['x_departures_train{}'.format(i)].shape)\n",
    "    print(globals()['x_departures_test{}'.format(i)].shape)\n",
    "    print(globals()['x_arrivals_train{}'.format(i)].shape)\n",
    "    print(globals()['x_arrivals_test{}'.format(i)].shape)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the input datasets should not have changed since the last shape check, and it hasn't. This means our operations did not yield data loss at any stage. The amount of columns present in the input data should have changed according to the rules presented in (Appendix A10, A11, A12), and yield:\n",
    "\n",
    "<pre>\n",
    "- Poly:                       9 + poly_degree * (1 + n_lag)      (Appendix A10)\n",
    "- Poly + Sine:               10 + poly_degree * (1 + n_lag)      (Appendix A11)\n",
    "- Poly + Sine + Cosine:      11 + poly_degree * (1 + n_lag)      (Appendix A12)\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the departures/arrivals input data into weekday and weekend data. (Appendix 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split departures/arrivals input data into weekday and weekend departures/arrivals input data, for all clusters. The \"day of week\" continuous variable was stored in the second column.\n",
    "if features == \"poly\":\n",
    "    column = 1\n",
    "elif features == \"poly + sine\":\n",
    "    column = 1\n",
    "else:\n",
    "    column = 1\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_weekday{}'.format(i)] = globals()['x_departures_train{}'.format(i)][globals()['x_departures_train{}'.format(i)][:,column] < 5]\n",
    "    globals()['x_departures_train_weekend{}'.format(i)] = globals()['x_departures_train{}'.format(i)][globals()['x_departures_train{}'.format(i)][:,column] > 4]\n",
    "    globals()['x_departures_test_weekday{}'.format(i)] = globals()['x_departures_test{}'.format(i)][globals()['x_departures_test{}'.format(i)][:,column] < 5]\n",
    "    globals()['x_departures_test_weekend{}'.format(i)] = globals()['x_departures_test{}'.format(i)][globals()['x_departures_test{}'.format(i)][:,column] > 4]\n",
    "    \n",
    "    globals()['x_arrivals_train_weekday{}'.format(i)] = globals()['x_arrivals_train{}'.format(i)][globals()['x_arrivals_train{}'.format(i)][:,column] < 5]\n",
    "    globals()['x_arrivals_train_weekend{}'.format(i)] = globals()['x_arrivals_train{}'.format(i)][globals()['x_arrivals_train{}'.format(i)][:,column] > 4]\n",
    "    globals()['x_arrivals_test_weekday{}'.format(i)] = globals()['x_arrivals_test{}'.format(i)][globals()['x_arrivals_test{}'.format(i)][:,column] < 5]\n",
    "    globals()['x_arrivals_test_weekend{}'.format(i)] = globals()['x_arrivals_test{}'.format(i)][globals()['x_arrivals_test{}'.format(i)][:,column] > 4]\n",
    "\n",
    "    \n",
    "# Training the model using \"day of week\" data as a continuous variable is poor practice. For this reason, the second column, where the \"day of week\" data is stored in this manner will be removed. \n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_weekday{}'.format(i)] = np.delete(globals()['x_departures_train_weekday{}'.format(i)], 1, 1)\n",
    "    globals()['x_departures_train_weekend{}'.format(i)] = np.delete(globals()['x_departures_train_weekend{}'.format(i)], 1, 1)\n",
    "    globals()['x_departures_test_weekday{}'.format(i)] = np.delete(globals()['x_departures_test_weekday{}'.format(i)], 1, 1)\n",
    "    globals()['x_departures_test_weekend{}'.format(i)] = np.delete(globals()['x_departures_test_weekend{}'.format(i)], 1, 1)\n",
    "    \n",
    "    globals()['x_arrivals_train_weekday{}'.format(i)] = np.delete(globals()['x_arrivals_train_weekday{}'.format(i)], 1, 1)\n",
    "    globals()['x_arrivals_train_weekend{}'.format(i)] = np.delete(globals()['x_arrivals_train_weekend{}'.format(i)], 1, 1)\n",
    "    globals()['x_arrivals_test_weekday{}'.format(i)] = np.delete(globals()['x_arrivals_test_weekday{}'.format(i)], 1, 1)\n",
    "    globals()['x_arrivals_test_weekend{}'.format(i)] = np.delete(globals()['x_arrivals_test_weekend{}'.format(i)], 1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape check 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    print(\"Shapes of cluster {}:\".format(i))\n",
    "    print(globals()['x_departures_train_weekday{}'.format(i)].shape)\n",
    "    print(globals()['x_departures_train_weekend{}'.format(i)].shape)\n",
    "    print(globals()['x_departures_test_weekday{}'.format(i)].shape)\n",
    "    print(globals()['x_departures_test_weekend{}'.format(i)].shape)\n",
    "    print(globals()['x_arrivals_train_weekday{}'.format(i)].shape)\n",
    "    print(globals()['x_arrivals_train_weekend{}'.format(i)].shape)\n",
    "    print(globals()['x_arrivals_test_weekday{}'.format(i)].shape)\n",
    "    print(globals()['x_arrivals_test_weekend{}'.format(i)].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the \"day of week\" as continous variable column deleted, the updated rule for the amount of columns is:\n",
    "<pre>\n",
    "- Poly:                       8 + poly_degree * (1 + n_lag * 2)\n",
    "- Poly + sine:                9 + poly_degree * (1 + n_lag * 2)\n",
    "- Poly + sine + cosine:      10 + poly_degree * (1 + n_lag * 2)\n",
    "</pre>\n",
    "\n",
    "Which is the case for all datasets. The amount of rows varies significantly due to the weekday/weekend split and will not be validated for brevity's sake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the input data constructed, the target data for the model can be built up. (Appendix A14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create y variables for all clusters training data, add \"day of week\" to be able to split weekend/weekday, the reshapes are necessary to make column vectors for better intuition, as both \n",
    "# counts and \"day of week\" are 1D arrays.. We had to make \"day of week\" an array as you cannot reshape a series object.\n",
    "for i in range(n):\n",
    "    globals()['y_departures_train{}'.format(i)] = np.array(globals()['departures_train_lagged{}'.format(i)]['count']).reshape(-1,1)\n",
    "    globals()['y_departures_train{}'.format(i)]= np.concatenate((globals()['y_departures_train{}'.format(i)], np.array(globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())).reshape(-1,1)), axis = 1)\n",
    "    globals()['y_arrivals_train{}'.format(i)] = np.array(globals()['arrivals_train_lagged{}'.format(i)]['count']).reshape(-1,1)\n",
    "    globals()['y_arrivals_train{}'.format(i)]= np.concatenate((globals()['y_arrivals_train{}'.format(i)], np.array(globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())).reshape(-1,1)), axis = 1)\n",
    "\n",
    "#do same for test data\n",
    "for i in range(n):\n",
    "    globals()['y_departures_test{}'.format(i)] = np.array(globals()['departures_test_lagged{}'.format(i)]['count']).reshape(-1,1)\n",
    "    globals()['y_departures_test{}'.format(i)]= np.concatenate((globals()['y_departures_test{}'.format(i)], np.array(globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())).reshape(-1,1)), axis = 1)\n",
    "    globals()['y_arrivals_test{}'.format(i)] = np.array(globals()['arrivals_test_lagged{}'.format(i)]['count']).reshape(-1,1)\n",
    "    globals()['y_arrivals_test{}'.format(i)]= np.concatenate((globals()['y_arrivals_test{}'.format(i)], np.array(globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())).reshape(-1,1)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the y data into weekday and weekend data, looking at how the y data is built up, the \"day of week\" is stored in the second column, weekends correspond to continuous \"day of week\" variable value \n",
    "# 5 and 6.\n",
    "for i in range(n):\n",
    "    globals()['y_departures_train_weekday{}'.format(i)] = globals()['y_departures_train{}'.format(i)][globals()['y_departures_train{}'.format(i)][:,1] < 5]\n",
    "    globals()['y_departures_train_weekend{}'.format(i)] = globals()['y_departures_train{}'.format(i)][globals()['y_departures_train{}'.format(i)][:,1] > 4]\n",
    "    globals()['y_departures_test_weekday{}'.format(i)] = globals()['y_departures_test{}'.format(i)][globals()['y_departures_test{}'.format(i)][:,1] < 5]\n",
    "    globals()['y_departures_test_weekend{}'.format(i)] = globals()['y_departures_test{}'.format(i)][globals()['y_departures_test{}'.format(i)][:,1] > 4]\n",
    "    \n",
    "    globals()['y_arrivals_train_weekday{}'.format(i)] = globals()['y_arrivals_train{}'.format(i)][globals()['y_arrivals_train{}'.format(i)][:,1] < 5]\n",
    "    globals()['y_arrivals_train_weekend{}'.format(i)] = globals()['y_arrivals_train{}'.format(i)][globals()['y_arrivals_train{}'.format(i)][:,1] > 4]\n",
    "    globals()['y_arrivals_test_weekday{}'.format(i)] = globals()['y_arrivals_test{}'.format(i)][globals()['y_arrivals_test{}'.format(i)][:,1] < 5]\n",
    "    globals()['y_arrivals_test_weekend{}'.format(i)] = globals()['y_arrivals_test{}'.format(i)][globals()['y_arrivals_test{}'.format(i)][:,1] > 4]\n",
    "\n",
    "# Delete second col (= continuous \"day of week\" value) of y data, since including this is poor practice.\n",
    "for i in range(n):\n",
    "    globals()['y_departures_train_weekday{}'.format(i)] = np.delete(globals()['y_departures_train_weekday{}'.format(i)], 1, 1)\n",
    "    globals()['y_departures_train_weekend{}'.format(i)] = np.delete(globals()['y_departures_train_weekend{}'.format(i)], 1, 1)\n",
    "    globals()['y_departures_test_weekday{}'.format(i)] = np.delete(globals()['y_departures_test_weekday{}'.format(i)], 1, 1)\n",
    "    globals()['y_departures_test_weekend{}'.format(i)] = np.delete(globals()['y_departures_test_weekend{}'.format(i)], 1, 1)\n",
    "    \n",
    "    globals()['y_arrivals_train_weekday{}'.format(i)] = np.delete(globals()['y_arrivals_train_weekday{}'.format(i)], 1, 1)\n",
    "    globals()['y_arrivals_train_weekend{}'.format(i)] = np.delete(globals()['y_arrivals_train_weekend{}'.format(i)], 1, 1)\n",
    "    globals()['y_arrivals_test_weekday{}'.format(i)] = np.delete(globals()['y_arrivals_test_weekday{}'.format(i)], 1, 1)\n",
    "    globals()['y_arrivals_test_weekend{}'.format(i)] = np.delete(globals()['y_arrivals_test_weekend{}'.format(i)], 1, 1)\n",
    "\n",
    "# We need y to be a 1D array for the linear regression approach, so reshape again.\n",
    "for i in range(n):\n",
    "    globals()['y_departures_train_weekday{}'.format(i)] = globals()['y_departures_train_weekday{}'.format(i)].reshape(-1)\n",
    "    globals()['y_departures_train_weekend{}'.format(i)] = globals()['y_departures_train_weekend{}'.format(i)].reshape(-1)\n",
    "    globals()['y_departures_test_weekday{}'.format(i)] = globals()['y_departures_test_weekday{}'.format(i)].reshape(-1)\n",
    "    globals()['y_departures_test_weekend{}'.format(i)] = globals()['y_departures_test_weekend{}'.format(i)].reshape(-1)\n",
    "    \n",
    "    globals()['y_arrivals_train_weekday{}'.format(i)] = globals()['y_arrivals_train_weekday{}'.format(i)].reshape(-1)\n",
    "    globals()['y_arrivals_train_weekend{}'.format(i)] = globals()['y_arrivals_train_weekend{}'.format(i)].reshape(-1)\n",
    "    globals()['y_arrivals_test_weekday{}'.format(i)] = globals()['y_arrivals_test_weekday{}'.format(i)].reshape(-1)\n",
    "    globals()['y_arrivals_test_weekend{}'.format(i)] = globals()['y_arrivals_test_weekend{}'.format(i)].reshape(-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize all input (x) and target (y) data for both weekends and weekdays to get data on a similar scale. (Appendix A15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standerdize x and y data\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_weekday_std{}'.format(i)] = (globals()['x_departures_train_weekday{}'.format(i)][:,8:] - np.mean(globals()['x_departures_train_weekday{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_departures_train_weekday{}'.format(i)][:,8:], axis=0)\n",
    "    globals()['x_departures_train_weekend_std{}'.format(i)] = (globals()['x_departures_train_weekend{}'.format(i)][:,8:] - np.mean(globals()['x_departures_train_weekend{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_departures_train_weekend{}'.format(i)][:,8:], axis=0)\n",
    "    globals()['x_departures_test_weekday{}'.format(i)] = (globals()['x_departures_test_weekday{}'.format(i)][:,8:] - np.mean(globals()['x_departures_test_weekday{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_departures_test_weekday{}'.format(i)][:,8:], axis=0)\n",
    "    globals()['x_departures_test_weekend{}'.format(i)] = (globals()['x_departures_test_weekend{}'.format(i)][:,8:] - np.mean(globals()['x_departures_test_weekend{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_departures_test_weekend{}'.format(i)][:,8:], axis=0)\n",
    "\n",
    "    globals()['x_arrivals_train_weekday{}'.format(i)] = (globals()['x_arrivals_train_weekday{}'.format(i)][:,8:] - np.mean(globals()['x_arrivals_train_weekday{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_arrivals_train_weekday{}'.format(i)][:,8:], axis=0)\n",
    "    globals()['x_arrivals_train_weekend_std{}'.format(i)] = (globals()['x_arrivals_train_weekend{}'.format(i)][:,8:] - np.mean(globals()['x_arrivals_train_weekend{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_arrivals_train_weekend{}'.format(i)][:,8:], axis=0)\n",
    "    globals()['x_arrivals_test_weekday_std{}'.format(i)] = (globals()['x_arrivals_test_weekday{}'.format(i)][:,8:] - np.mean(globals()['x_arrivals_test_weekday{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_arrivals_test_weekday{}'.format(i)][:,8:], axis=0)\n",
    "    globals()['x_arrivals_test_weekend_std{}'.format(i)] = (globals()['x_arrivals_test_weekend{}'.format(i)][:,8:] - np.mean(globals()['x_arrivals_test_weekend{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_arrivals_test_weekend{}'.format(i)][:,8:], axis=0)\n",
    "\n",
    "    globals()['y_departures_train_weekday_std{}'.format(i)] = (globals()['y_departures_train_weekday{}'.format(i)] - np.mean(globals()['y_departures_train_weekday{}'.format(i)], axis=0)) / np.std(globals()['y_departures_train_weekday{}'.format(i)], axis=0)\n",
    "    globals()['y_departures_train_weekend_std{}'.format(i)] = (globals()['y_departures_train_weekend{}'.format(i)] - np.mean(globals()['y_departures_train_weekend{}'.format(i)], axis=0)) / np.std(globals()['y_departures_train_weekend{}'.format(i)], axis=0)\n",
    "    globals()['y_departures_test_weekday_std{}'.format(i)] = (globals()['y_departures_test_weekday{}'.format(i)] - np.mean(globals()['y_departures_test_weekday{}'.format(i)], axis=0)) / np.std(globals()['y_departures_test_weekday{}'.format(i)], axis=0)\n",
    "    globals()['y_departures_test_weekend_std{}'.format(i)] = (globals()['y_departures_test_weekend{}'.format(i)] - np.mean(globals()['y_departures_test_weekend{}'.format(i)], axis=0)) / np.std(globals()['y_departures_test_weekend{}'.format(i)], axis=0)\n",
    "\n",
    "    globals()['y_arrivals_train_weekday_std{}'.format(i)] = (globals()['y_arrivals_train_weekday{}'.format(i)] - np.mean(globals()['y_arrivals_train_weekday{}'.format(i)], axis=0)) / np.std(globals()['y_arrivals_train_weekday{}'.format(i)], axis=0)\n",
    "    globals()['y_arrivals_train_weekend_std{}'.format(i)] = (globals()['y_arrivals_train_weekend{}'.format(i)] - np.mean(globals()['y_arrivals_train_weekend{}'.format(i)], axis=0)) / np.std(globals()['y_arrivals_train_weekend{}'.format(i)], axis=0)\n",
    "    globals()['y_arrivals_test_weekday_std{}'.format(i)] = (globals()['y_arrivals_test_weekday{}'.format(i)] - np.mean(globals()['y_arrivals_test_weekday{}'.format(i)], axis=0)) / np.std(globals()['y_arrivals_test_weekday{}'.format(i)], axis=0)\n",
    "    globals()['y_arrivals_test_weekend_std{}'.format(i)] = (globals()['y_arrivals_test_weekend{}'.format(i)] - np.mean(globals()['y_arrivals_test_weekend{}'.format(i)], axis=0)) / np.std(globals()['y_arrivals_test_weekend{}'.format(i)], axis=0)\n",
    "    \n",
    "\n",
    "# Create a new dataset. add the ones column, and weekday/weekend dummies.\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_weekday_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_departures_train_weekday{}'.format(i)])), globals()['x_departures_train_weekday{}'.format(i)][:,1:8]]\n",
    "    globals()['x_departures_train_weekend_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_departures_train_weekend{}'.format(i)])), globals()['x_departures_train_weekend{}'.format(i)][:,1:8]]\n",
    "    globals()['x_departures_test_weekday_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_departures_test_weekday{}'.format(i)])), globals()['x_departures_test_weekday{}'.format(i)][:,1:8]]\n",
    "    globals()['x_departures_test_weekend_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_departures_test_weekend{}'.format(i)])), globals()['x_departures_test_weekend{}'.format(i)][:,1:8]]\n",
    "\n",
    "    globals()['x_arrivals_train_weekday_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_arrivals_train_weekday{}'.format(i)])), globals()['x_arrivals_train_weekday{}'.format(i)][:,1:8]]\n",
    "    globals()['x_arrivals_train_weekend_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_arrivals_train_weekend{}'.format(i)])), globals()['x_arrivals_train_weekend{}'.format(i)][:,1:8]]\n",
    "    globals()['x_arrivals_test_weekday_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_arrivals_test_weekday{}'.format(i)])), globals()['x_arrivals_test_weekday{}'.format(i)][:,1:8]]\n",
    "    globals()['x_arrivals_test_weekend_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_arrivals_test_weekend{}'.format(i)])), globals()['x_arrivals_test_weekend{}'.format(i)][:,1:8]]\n",
    "\n",
    "\n",
    "\n",
    "# Add the standerdized x data to the new dataset to achieve the final, standerdized input dataset.\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_weekday_std{}'.format(i)] = np.c_[globals()['x_departures_train_weekday_temp{}'.format(i)], globals()['x_departures_train_weekday_std{}'.format(i)]]\n",
    "    globals()['x_departures_train_weekend_std{}'.format(i)] = np.c_[globals()['x_departures_train_weekend_temp{}'.format(i)], globals()['x_departures_train_weekend_std{}'.format(i)]]\n",
    "    globals()['x_departures_test_weekday_std{}'.format(i)] = np.c_[globals()['x_departures_test_weekday_temp{}'.format(i)], globals()['x_departures_test_weekday_std{}'.format(i)]]\n",
    "    globals()['x_departures_test_weekend_std{}'.format(i)] = np.c_[globals()['x_departures_test_weekend_temp{}'.format(i)], globals()['x_departures_test_weekend_std{}'.format(i)]]\n",
    "\n",
    "    globals()['x_arrivals_train_weekday_std{}'.format(i)] = np.c_[globals()['x_arrivals_train_weekday_temp{}'.format(i)], globals()['x_arrivals_train_weekday_std{}'.format(i)]]\n",
    "    globals()['x_arrivals_train_weekend_std{}'.format(i)] = np.c_[globals()['x_arrivals_train_weekend_temp{}'.format(i)], globals()['x_arrivals_train_weekend_std{}'.format(i)]]\n",
    "    globals()['x_arrivals_test_weekday_std{}'.format(i)] = np.c_[globals()['x_arrivals_test_weekday_temp{}'.format(i)], globals()['x_arrivals_test_weekday_std{}'.format(i)]]\n",
    "    globals()['x_arrivals_test_weekend_std{}'.format(i)] = np.c_[globals()['x_arrivals_test_weekend_temp{}'.format(i)], globals()['x_arrivals_test_weekend_std{}'.format(i)]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if NaN's were created in the standardisation process. If there are, this means the standardisation process caused data loss. This is not the case.\n",
    "lst = []\n",
    "for i in range(n):\n",
    "    lst.append(np.count_nonzero(np.isnan(globals()[\"x_departures_train_weekday_std{}\".format(i)])))\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all input and target data preprocessed, the model can be trained and validated. (Appendix A16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the linear regression model\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "# Train weekday models\n",
    "for i in range(n):\n",
    "    globals()['model_departures_weekday{}'.format(i)] = Ridge().fit(globals()['x_departures_train_weekday_std{}'.format(i)], globals()['y_departures_train_weekday_std{}'.format(i)])\n",
    "    globals()['model_arrivals_weekday{}'.format(i)] = Ridge().fit(globals()['x_arrivals_train_weekday_std{}'.format(i)], globals()['y_arrivals_train_weekday_std{}'.format(i)])\n",
    "# Train weekend models\n",
    "for i in range(n):\n",
    "    globals()['model_departures_weekend{}'.format(i)] = Ridge().fit(globals()['x_departures_train_weekend_std{}'.format(i)], globals()['y_departures_train_weekend_std{}'.format(i)])\n",
    "    globals()['model_arrivals_weekend{}'.format(i)] = Ridge().fit(globals()['x_arrivals_train_weekend_std{}'.format(i)], globals()['y_arrivals_train_weekend_std{}'.format(i)])\n",
    "\n",
    "# Predict target variable for all clusters weekday test input data\n",
    "for i in range(n):\n",
    "    globals()['y_departures_pred_weekday{}'.format(i)] = globals()['model_departures_weekday{}'.format(i)].predict(globals()['x_departures_test_weekday_std{}'.format(i)])\n",
    "    globals()['y_arrivals_pred_weekday{}'.format(i)] = globals()['model_arrivals_weekday{}'.format(i)].predict(globals()['x_arrivals_test_weekday_std{}'.format(i)])\n",
    "# Predict target variable for all clusters weekend test input data\n",
    "for i in range(n):\n",
    "    globals()['y_departures_pred_weekend{}'.format(i)] = globals()['model_departures_weekend{}'.format(i)].predict(globals()['x_departures_test_weekend_std{}'.format(i)])\n",
    "    globals()['y_arrivals_pred_weekend{}'.format(i)] = globals()['model_arrivals_weekend{}'.format(i)].predict(globals()['x_arrivals_test_weekend_std{}'.format(i)])\n",
    "\n",
    "# Calculate r^2 for all clusters weekday test input data\n",
    "from sklearn.metrics import r2_score\n",
    "for i in range(n):\n",
    "    globals()['r2_departures_weekday{}'.format(i)] = r2_score(globals()['y_departures_test_weekday_std{}'.format(i)], globals()['y_departures_pred_weekday{}'.format(i)])\n",
    "    globals()['r2_arrivals_weekday{}'.format(i)] = r2_score(globals()['y_arrivals_test_weekday_std{}'.format(i)], globals()['y_arrivals_pred_weekday{}'.format(i)])\n",
    "# Calculate r^2 for all clusters weekend test input data\n",
    "for i in range(n):\n",
    "    globals()['r2_departures_weekend{}'.format(i)] = r2_score(globals()['y_departures_test_weekend_std{}'.format(i)], globals()['y_departures_pred_weekend{}'.format(i)])\n",
    "    globals()['r2_arrivals_weekend{}'.format(i)] = r2_score(globals()['y_arrivals_test_weekend_std{}'.format(i)], globals()['y_arrivals_pred_weekend{}'.format(i)])\n",
    "\n",
    "# Print r^2 for all clusters weekday test input data\n",
    "for i in range(n):\n",
    "    print('r2 departures weekday cluster {}: {}'.format(i, globals()['r2_departures_weekday{}'.format(i)]))\n",
    "    print('r2 arrivals weekday cluster {}: {}'.format(i, globals()['r2_arrivals_weekday{}'.format(i)]))\n",
    "# Print r^2 for all clusters weekend test input data\n",
    "for i in range(n):\n",
    "    print('r2 departures weekend cluster {}: {}'.format(i, globals()['r2_departures_weekend{}'.format(i)]))\n",
    "    print('r2 arrivals weekend cluster {}: {}'.format(i, globals()['r2_arrivals_weekend{}'.format(i)]))\n",
    "\n",
    "# Note that the cluster number does not correspond to the actual cluster number, as determined in part 1. This is due to our sorting of the clusters based on size (amount of data entries) at the start\n",
    "# of part 2. In order to correctly identify the cluster label, look at the order of the entries in the largest_cluster list. The first entry in this list corresponds to cluster with index 0, etc.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now visualize the performance of the prediction models for each cluster, departures/arrivals and weekday/weekend. Most left histogram bin = cluster with the most amount of data entries. The prediction performance will be given the correct corresponding cluster label as determined in Part 1.\n",
    "\n",
    "Please run the cell below two times for formatting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, 1)\n",
    "# Color the bars that fall below the 0.6 threshold red, the others blue.\n",
    "plt.bar(range(n), [globals()['r2_departures_weekday{}'.format(i)] for i in range(n)], color=['red' if globals()['r2_departures_weekday{}'.format(i)] < 0.6 else 'blue' for i in range(n)])\n",
    "plt.ylabel('r2')\n",
    "plt.xlabel('cluster')\n",
    "plt.title('r2 for departures weekday clusters'+', ' +features)\n",
    "# Put the label of the clusters on top of each bin in the histogram. Center the label, give some whitespace between histogram top and label to avoid overlap.\n",
    "# Reminder that we renamed the largest cluster to 0, so we had to account for this. \n",
    "for i in range(n):\n",
    "    plt.text(i, globals()['r2_departures_weekday{}'.format(i)]+0.01, largest_cluster[i], rotation=0, ha='center')\n",
    "# Remove xticks, no meaning\n",
    "plt.xticks([])\n",
    "# Put horizontal line at r^2 = 0.6\n",
    "plt.axhline(y=0.6, color='r', linestyle='-')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "# Color the bars that fall below the 0.6 threshold red, the others blue.\n",
    "plt.bar(range(n), [globals()['r2_arrivals_weekday{}'.format(i)] for i in range(n)], color=['red' if globals()['r2_arrivals_weekday{}'.format(i)] < 0.6 else 'blue' for i in range(n)])\n",
    "plt.ylabel('r2')\n",
    "plt.xlabel('cluster')\n",
    "plt.title('r2 for arrivals weekday clusters'+', ' + features)\n",
    "# Put the label of the clusters on top of each bin in the histogram. Center the label, give some whitespace between histogram top and label to avoid overlap.\n",
    "# Reminder that we renamed the largest cluster to 0, so we had to account for this.\n",
    "for i in range(n):\n",
    "    plt.text(i, globals()['r2_arrivals_weekday{}'.format(i)]+0.01, largest_cluster[i], rotation=0, ha='center')\n",
    "# Remove xticks, no meaning\n",
    "plt.xticks([])\n",
    "# Put horizontal line at r^2 = 0.6\n",
    "plt.axhline(y=0.6, color='r', linestyle='-')\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "# Color the bars that fall below the 0.6 threshold red, the others blue.\n",
    "plt.bar(range(n), [globals()['r2_departures_weekend{}'.format(i)] for i in range(n)], color=['red' if globals()['r2_departures_weekend{}'.format(i)] < 0.6 else 'blue' for i in range(n)])\n",
    "plt.ylabel('r2')\n",
    "plt.xlabel('cluster')\n",
    "plt.title('r2 for departures weekend clusters'+', ' +features)\n",
    "# Put the label of the clusters on top of each bin in the histogram. Center the label, give some whitespace between histogram top and label to avoid overlap.\n",
    "# Reminder that we renamed the largest cluster to 0, so we had to account for this.\n",
    "for i in range(n):\n",
    "    plt.text(i, globals()['r2_departures_weekend{}'.format(i)]+0.01, largest_cluster[i], rotation=0, ha='center')\n",
    "# Remove xticks, no meaning\n",
    "plt.xticks([])\n",
    "# Put horizontal line at r^2 = 0.6\n",
    "plt.axhline(y=0.6, color='r', linestyle='-')\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "# Color the bars that fall below the 0.6 threshold red, the others blue.\n",
    "plt.bar(range(n), [globals()['r2_arrivals_weekend{}'.format(i)] for i in range(n)], color=['red' if globals()['r2_arrivals_weekend{}'.format(i)] < 0.6 else 'blue' for i in range(n)])\n",
    "plt.ylabel('r2')\n",
    "plt.xlabel('cluster')\n",
    "plt.title('r2 for arrivals weekend clusters'+', ' +features)\n",
    "# Put the label of the clusters on top of each bin in the histogram. Center the label, give some whitespace between histogram top and label to avoid overlap.\n",
    "# Reminder that we renamed the largest cluster to 0, so we had to account for this.\n",
    "for i in range(n):\n",
    "    plt.text(i, globals()['r2_arrivals_weekend{}'.format(i)]+0.01, largest_cluster[i], rotation=0, ha='center')\n",
    "# Remove xticks, no meaning\n",
    "plt.xticks([])\n",
    "# Put horizontal line at r^2 = 0.6\n",
    "plt.axhline(y=0.6, color='r', linestyle='-')\n",
    "\n",
    "# Icrease whitespace between subplots and figure size\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "\n",
    "#Increase textsizes of title, axes, ticks, labels\n",
    "plt.rc('axes', titlesize=10)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=8)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=8)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=8)    # fontsize of the tick labels\n",
    "plt.rc('figure', titlesize=8)  # fontsize of the figure title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For intuition behind the plots, see (Appendix A17)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermezzo part: Making a prediction model for departures/arrivals counts²\n",
    "\n",
    "In this part, we will train a prediction model to predict the counts² variable, necessary in pt.3. (Appendix A18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-perform the linear regression approach introduced earlier, but now with target variable standerdized counts².\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_lagged{}'.format(i)] = np.c_[np.ones(len(globals()['departures_train_lagged{}'.format(i)]))]\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)] = np.c_[np.ones(len(globals()['arrivals_train_lagged{}'.format(i)]))]\n",
    "\n",
    "    # Add a column with the \"day of week\" to the x_departures_train variable. \n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())]\n",
    "\n",
    "    # convert \"day of week\" to dummy variables since it is a categorical, 0 = is_monday, 1 = is_tuesday, 2 = is_wednesday, 3 = is_thursday, 4 = is_friday, 5 = is_saturday, 6 = is_sunday\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 0 else 0)]\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 1 else 0)]\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 2 else 0)]\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 3 else 0)]\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 4 else 0)]\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 5 else 0)]\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 6 else 0)]\n",
    "\n",
    "    # Add the hour column to the x_departures_train variable\n",
    "    globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['hour']]\n",
    "\n",
    "    # Add a column with the \"day of week\" to the x_arrivals_train variable.\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())]\n",
    "\n",
    "    # Convert \"day of week\" to dummy variables, 0 = is_monday, 1 = is_tuesday, 2 = is_wednesday, 3 = is_thursday, 4 = is_friday, 5 = is_saturday, 6 = is_sunday\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 0 else 0)]\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 1 else 0)]\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 2 else 0)]\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 3 else 0)]\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 4 else 0)]\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 5 else 0)]\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 6 else 0)]\n",
    "    \n",
    "    # Add the hour column to the x_arrivals_train variable\n",
    "    globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['hour']]\n",
    "\n",
    "    # Add all the lagged counts to the x_train variable\n",
    "    for j in range(1, n_lag_dep_train+1):\n",
    "        globals()['x_departures_train_lagged{}'.format(i)]= np.c_[globals()['x_departures_train_lagged{}'.format(i)], globals()['departures_train_lagged{}'.format(i)]['count_lag{}'.format(j)]]\n",
    "    for j in range(1, n_lag_arr_train+1):\n",
    "        globals()['x_arrivals_train_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_train_lagged{}'.format(i)], globals()['arrivals_train_lagged{}'.format(i)]['count_lag{}'.format(j)]]\n",
    "\n",
    "\n",
    "\n",
    "# Perform the same steps for the test data     \n",
    "    globals()['x_departures_test_lagged{}'.format(i)] = np.c_[np.ones(len(globals()['departures_test_lagged{}'.format(i)]))]\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)] = np.c_[np.ones(len(globals()['arrivals_test_lagged{}'.format(i)]))]\n",
    "    \n",
    "    # Add a column with the \"day of week\" to the x_departures_test variable.\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())]\n",
    "\n",
    "    # Convert \"day of week\" to dummy variables, 0 = is_monday, 1 = is_tuesday, 2 = is_wednesday, 3 = is_thursday, 4 = is_friday, 5 = is_saturday, 6 = is_sunday\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 0 else 0)]\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 1 else 0)]\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 2 else 0)]\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 3 else 0)]\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 4 else 0)]\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 5 else 0)]\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 6 else 0)]\n",
    "\n",
    "    # Add the hour column to the x_departures_test variable\n",
    "    globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['hour']]\n",
    "\n",
    "    # Add a column with the \"day of week\" to the x_arrivals_train variable.\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())]\n",
    "\n",
    "    # Convert \"day of week\" to dummy variables, 0 = is_monday, 1 = is_tuesday, 2 = is_wednesday, 3 = is_thursday, 4 = is_friday, 5 = is_saturday, 6 = is_sunday\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 0 else 0)]\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 1 else 0)]\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 2 else 0)]\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 3 else 0)]\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 4 else 0)]\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 5 else 0)]\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: 1 if x.weekday() == 6 else 0)]\n",
    "    \n",
    "    # Add the hour column to the x_arrivals_test variable\n",
    "    globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['hour']]\n",
    "    \n",
    "    # Also add all the lagged counts to the x_test variable\n",
    "    for j in range(1, n_lag_dep_test+1):\n",
    "        globals()['x_departures_test_lagged{}'.format(i)]= np.c_[globals()['x_departures_test_lagged{}'.format(i)], globals()['departures_test_lagged{}'.format(i)]['count_lag{}'.format(j)]]\n",
    "    for j in range(1, n_lag_arr_test+1):\n",
    "        globals()['x_arrivals_test_lagged{}'.format(i)]= np.c_[globals()['x_arrivals_test_lagged{}'.format(i)], globals()['arrivals_test_lagged{}'.format(i)]['count_lag{}'.format(j)]]\n",
    "        \n",
    "\n",
    "\n",
    "# Add polynomial features\n",
    "degree = 2\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_poly{}'.format(i)] = add_polynomial(globals()['x_departures_train_lagged{}'.format(i)], degree)\n",
    "    globals()['x_departures_test_poly{}'.format(i)] = add_polynomial(globals()['x_departures_test_lagged{}'.format(i)], degree)\n",
    "    globals()['x_arrivals_train_poly{}'.format(i)] = add_polynomial(globals()['x_arrivals_train_lagged{}'.format(i)], degree)\n",
    "    globals()['x_arrivals_test_poly{}'.format(i)] = add_polynomial(globals()['x_arrivals_test_lagged{}'.format(i)], degree)\n",
    "\n",
    "\n",
    "\n",
    "# Add sine(hour)\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_sine{}'.format(i)] = np.insert(globals()['x_departures_train_poly{}'.format(i)],    9,    add_sine(globals()['x_departures_train_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "    globals()['x_departures_test_sine{}'.format(i)] = np.insert(globals()['x_departures_test_poly{}'.format(i)],    9,    add_sine(globals()['x_departures_test_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "    globals()['x_arrivals_train_sine{}'.format(i)] = np.insert(globals()['x_arrivals_train_poly{}'.format(i)],    9,    add_sine(globals()['x_arrivals_train_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "    globals()['x_arrivals_test_sine{}'.format(i)] = np.insert(globals()['x_arrivals_test_poly{}'.format(i)],    9,    add_sine(globals()['x_arrivals_test_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Delete hour duplicate column\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_sine{}'.format(i)] = np.delete(globals()['x_departures_train_sine{}'.format(i)], 11, 1)\n",
    "    globals()['x_departures_test_sine{}'.format(i)] = np.delete(globals()['x_departures_test_sine{}'.format(i)], 11, 1)\n",
    "    globals()['x_arrivals_train_sine{}'.format(i)] = np.delete(globals()['x_arrivals_train_sine{}'.format(i)], 11, 1)\n",
    "    globals()['x_arrivals_test_sine{}'.format(i)] = np.delete(globals()['x_arrivals_test_sine{}'.format(i)], 11, 1)\n",
    "\n",
    "\n",
    "\n",
    "# Add cos(hour)\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_cosine_sine{}'.format(i)] = np.insert(globals()['x_departures_train_sine{}'.format(i)],    9,    add_cosine(globals()['x_departures_train_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "    globals()['x_departures_test_cosine_sine{}'.format(i)] = np.insert(globals()['x_departures_test_sine{}'.format(i)],    9,    add_cosine(globals()['x_departures_test_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "    globals()['x_arrivals_train_cosine_sine{}'.format(i)] = np.insert(globals()['x_arrivals_train_sine{}'.format(i)],    9,    add_cosine(globals()['x_arrivals_train_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "    globals()['x_arrivals_test_cosine_sine{}'.format(i)] = np.insert(globals()['x_arrivals_test_sine{}'.format(i)],    9,    add_cosine(globals()['x_arrivals_test_poly{}'.format(i)][:,9].reshape(-1,1)).T,    axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Delete hour duplicate column\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_cosine_sine{}'.format(i)] = np.delete(globals()['x_departures_train_cosine_sine{}'.format(i)], 11, 1)\n",
    "    globals()['x_departures_test_cosine_sine{}'.format(i)] = np.delete(globals()['x_departures_test_cosine_sine{}'.format(i)], 11, 1)\n",
    "    globals()['x_arrivals_train_cosine_sine{}'.format(i)] = np.delete(globals()['x_arrivals_train_cosine_sine{}'.format(i)], 11, 1)\n",
    "    globals()['x_arrivals_test_cosine_sine{}'.format(i)] = np.delete(globals()['x_arrivals_test_cosine_sine{}'.format(i)], 11, 1)\n",
    "\n",
    "\n",
    "\n",
    "#select the features you want to use\n",
    "features = \"poly + sine + cosine\"\n",
    "if features == \"poly\":\n",
    "    for i in range(n):\n",
    "        globals()['x_departures_train{}'.format(i)] = globals()['x_departures_train_poly{}'.format(i)]\n",
    "        globals()['x_departures_test{}'.format(i)] = globals()['x_departures_test_poly{}'.format(i)]\n",
    "        globals()['x_arrivals_train{}'.format(i)] = globals()['x_arrivals_train_poly{}'.format(i)]\n",
    "        globals()['x_arrivals_test{}'.format(i)] = globals()['x_arrivals_test_poly{}'.format(i)]\n",
    "elif features == \"poly + sine\":\n",
    "    for i in range(n):\n",
    "        globals()['x_departures_train{}'.format(i)] = globals()['x_departures_train_sine{}'.format(i)]\n",
    "        globals()['x_departures_test{}'.format(i)] = globals()['x_departures_test_sine{}'.format(i)]\n",
    "        globals()['x_arrivals_train{}'.format(i)] = globals()['x_arrivals_train_sine{}'.format(i)]\n",
    "        globals()['x_arrivals_test{}'.format(i)] = globals()['x_arrivals_test_sine{}'.format(i)]\n",
    "else:\n",
    "    for i in range(n):\n",
    "        globals()['x_departures_train{}'.format(i)] = globals()['x_departures_train_cosine_sine{}'.format(i)]\n",
    "        globals()['x_departures_test{}'.format(i)] = globals()['x_departures_test_cosine_sine{}'.format(i)]\n",
    "        globals()['x_arrivals_train{}'.format(i)] = globals()['x_arrivals_train_cosine_sine{}'.format(i)]\n",
    "        globals()['x_arrivals_test{}'.format(i)] = globals()['x_arrivals_test_cosine_sine{}'.format(i)]\n",
    "\n",
    "\n",
    "\n",
    "# Split the input data into weekday and weekend data\n",
    "if features == \"poly\":\n",
    "    column = 1\n",
    "elif features == \"poly + sine\":\n",
    "    column = 1\n",
    "else:\n",
    "    column = 1\n",
    "\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_weekday{}'.format(i)] = globals()['x_departures_train{}'.format(i)][globals()['x_departures_train{}'.format(i)][:,column] < 5]\n",
    "    globals()['x_departures_train_weekend{}'.format(i)] = globals()['x_departures_train{}'.format(i)][globals()['x_departures_train{}'.format(i)][:,column] > 4]\n",
    "    globals()['x_departures_test_weekday{}'.format(i)] = globals()['x_departures_test{}'.format(i)][globals()['x_departures_test{}'.format(i)][:,column] < 5]\n",
    "    globals()['x_departures_test_weekend{}'.format(i)] = globals()['x_departures_test{}'.format(i)][globals()['x_departures_test{}'.format(i)][:,column] > 4]\n",
    "    \n",
    "    globals()['x_arrivals_train_weekday{}'.format(i)] = globals()['x_arrivals_train{}'.format(i)][globals()['x_arrivals_train{}'.format(i)][:,column] < 5]\n",
    "    globals()['x_arrivals_train_weekend{}'.format(i)] = globals()['x_arrivals_train{}'.format(i)][globals()['x_arrivals_train{}'.format(i)][:,column] > 4]\n",
    "    globals()['x_arrivals_test_weekday{}'.format(i)] = globals()['x_arrivals_test{}'.format(i)][globals()['x_arrivals_test{}'.format(i)][:,column] < 5]\n",
    "    globals()['x_arrivals_test_weekend{}'.format(i)] = globals()['x_arrivals_test{}'.format(i)][globals()['x_arrivals_test{}'.format(i)][:,column] > 4]\n",
    "\n",
    "\n",
    "\n",
    "# Delete the \"day of week\" column and only keep the correct way to present categorical data to your model.\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_weekday{}'.format(i)] = np.delete(globals()['x_departures_train_weekday{}'.format(i)], 1, 1)\n",
    "    globals()['x_departures_train_weekend{}'.format(i)] = np.delete(globals()['x_departures_train_weekend{}'.format(i)], 1, 1)\n",
    "    globals()['x_departures_test_weekday{}'.format(i)] = np.delete(globals()['x_departures_test_weekday{}'.format(i)], 1, 1)\n",
    "    globals()['x_departures_test_weekend{}'.format(i)] = np.delete(globals()['x_departures_test_weekend{}'.format(i)], 1, 1)\n",
    "    \n",
    "    globals()['x_arrivals_train_weekday{}'.format(i)] = np.delete(globals()['x_arrivals_train_weekday{}'.format(i)], 1, 1)\n",
    "    globals()['x_arrivals_train_weekend{}'.format(i)] = np.delete(globals()['x_arrivals_train_weekend{}'.format(i)], 1, 1)\n",
    "    globals()['x_arrivals_test_weekday{}'.format(i)] = np.delete(globals()['x_arrivals_test_weekday{}'.format(i)], 1, 1)\n",
    "    globals()['x_arrivals_test_weekend{}'.format(i)] = np.delete(globals()['x_arrivals_test_weekend{}'.format(i)], 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "# Standardize the input data\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_weekday_std{}'.format(i)] = (globals()['x_departures_train_weekday{}'.format(i)][:,8:] - np.mean(globals()['x_departures_train_weekday{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_departures_train_weekday{}'.format(i)][:,8:], axis=0)\n",
    "    globals()['x_departures_train_weekend_std{}'.format(i)] = (globals()['x_departures_train_weekend{}'.format(i)][:,8:] - np.mean(globals()['x_departures_train_weekend{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_departures_train_weekend{}'.format(i)][:,8:], axis=0)\n",
    "    globals()['x_departures_test_weekday_std{}'.format(i)] = (globals()['x_departures_test_weekday{}'.format(i)][:,8:] - np.mean(globals()['x_departures_test_weekday{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_departures_test_weekday{}'.format(i)][:,8:], axis=0)\n",
    "    globals()['x_departures_test_weekend_std{}'.format(i)] = (globals()['x_departures_test_weekend{}'.format(i)][:,8:] - np.mean(globals()['x_departures_test_weekend{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_departures_test_weekend{}'.format(i)][:,8:], axis=0)\n",
    "\n",
    "    globals()['x_arrivals_train_weekday_std{}'.format(i)] = (globals()['x_arrivals_train_weekday{}'.format(i)][:,8:] - np.mean(globals()['x_arrivals_train_weekday{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_arrivals_train_weekday{}'.format(i)][:,8:], axis=0)\n",
    "    globals()['x_arrivals_train_weekend_std{}'.format(i)] = (globals()['x_arrivals_train_weekend{}'.format(i)][:,8:] - np.mean(globals()['x_arrivals_train_weekend{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_arrivals_train_weekend{}'.format(i)][:,8:], axis=0)\n",
    "    globals()['x_arrivals_test_weekday_std{}'.format(i)] = (globals()['x_arrivals_test_weekday{}'.format(i)][:,8:] - np.mean(globals()['x_arrivals_test_weekday{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_arrivals_test_weekday{}'.format(i)][:,8:], axis=0)\n",
    "    globals()['x_arrivals_test_weekend_std{}'.format(i)] = (globals()['x_arrivals_test_weekend{}'.format(i)][:,8:] - np.mean(globals()['x_arrivals_test_weekend{}'.format(i)][:,8:], axis=0)) / np.std(globals()['x_arrivals_test_weekend{}'.format(i)][:,8:], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# Create a new dataset. Add the ones column, and weekday/weekend dummies.\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_weekday_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_departures_train_weekday{}'.format(i)])), globals()['x_departures_train_weekday{}'.format(i)][:,1:8]]\n",
    "    globals()['x_departures_train_weekend_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_departures_train_weekend{}'.format(i)])), globals()['x_departures_train_weekend{}'.format(i)][:,1:8]]\n",
    "    globals()['x_departures_test_weekday_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_departures_test_weekday{}'.format(i)])), globals()['x_departures_test_weekday{}'.format(i)][:,1:8]]\n",
    "    globals()['x_departures_test_weekend_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_departures_test_weekend{}'.format(i)])), globals()['x_departures_test_weekend{}'.format(i)][:,1:8]]\n",
    "\n",
    "    globals()['x_arrivals_train_weekday_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_arrivals_train_weekday{}'.format(i)])), globals()['x_arrivals_train_weekday{}'.format(i)][:,1:8]]\n",
    "    globals()['x_arrivals_train_weekend_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_arrivals_train_weekend{}'.format(i)])), globals()['x_arrivals_train_weekend{}'.format(i)][:,1:8]]\n",
    "    globals()['x_arrivals_test_weekday_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_arrivals_test_weekday{}'.format(i)])), globals()['x_arrivals_test_weekday{}'.format(i)][:,1:8]]\n",
    "    globals()['x_arrivals_test_weekend_temp{}'.format(i)] = np.c_[np.ones(len(globals()['x_arrivals_test_weekend{}'.format(i)])), globals()['x_arrivals_test_weekend{}'.format(i)][:,1:8]]\n",
    "\n",
    "\n",
    "\n",
    "# Add the standerdized x data to the new dataset to achieve the final, standerdized input dataset.\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_weekday_std{}'.format(i)] = np.c_[globals()['x_departures_train_weekday_temp{}'.format(i)], globals()['x_departures_train_weekday_std{}'.format(i)]]\n",
    "    globals()['x_departures_train_weekend_std{}'.format(i)] = np.c_[globals()['x_departures_train_weekend_temp{}'.format(i)], globals()['x_departures_train_weekend_std{}'.format(i)]]\n",
    "    globals()['x_departures_test_weekday_std{}'.format(i)] = np.c_[globals()['x_departures_test_weekday_temp{}'.format(i)], globals()['x_departures_test_weekday_std{}'.format(i)]]\n",
    "    globals()['x_departures_test_weekend_std{}'.format(i)] = np.c_[globals()['x_departures_test_weekend_temp{}'.format(i)], globals()['x_departures_test_weekend_std{}'.format(i)]]\n",
    "\n",
    "    globals()['x_arrivals_train_weekday_std{}'.format(i)] = np.c_[globals()['x_arrivals_train_weekday_temp{}'.format(i)], globals()['x_arrivals_train_weekday_std{}'.format(i)]]\n",
    "    globals()['x_arrivals_train_weekend_std{}'.format(i)] = np.c_[globals()['x_arrivals_train_weekend_temp{}'.format(i)], globals()['x_arrivals_train_weekend_std{}'.format(i)]]\n",
    "    globals()['x_arrivals_test_weekday_std{}'.format(i)] = np.c_[globals()['x_arrivals_test_weekday_temp{}'.format(i)], globals()['x_arrivals_test_weekday_std{}'.format(i)]]\n",
    "    globals()['x_arrivals_test_weekend_std{}'.format(i)] = np.c_[globals()['x_arrivals_test_weekend_temp{}'.format(i)], globals()['x_arrivals_test_weekend_std{}'.format(i)]]\n",
    "\n",
    "\n",
    "\n",
    "# Create a target dataset filled with departures/arrivals counts² values. \n",
    "for i in range(n):\n",
    "    globals()['y_departures_train_squared{}'.format(i)] = (np.array(globals()['departures_train_lagged{}'.format(i)]['count'])**2).reshape(-1,1)\n",
    "    globals()['y_departures_train_squared{}'.format(i)]= np.concatenate((globals()['y_departures_train_squared{}'.format(i)], np.array(globals()['departures_train_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())).reshape(-1,1)), axis = 1)\n",
    "    globals()['y_arrivals_train_squared{}'.format(i)] = (np.array(globals()['arrivals_train_lagged{}'.format(i)]['count'])**2).reshape(-1,1)\n",
    "    globals()['y_arrivals_train_squared{}'.format(i)]= np.concatenate((globals()['y_arrivals_train_squared{}'.format(i)], np.array(globals()['arrivals_train_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())).reshape(-1,1)), axis = 1)\n",
    "\n",
    "    globals()['y_departures_test_squared{}'.format(i)] = (np.array(globals()['departures_test_lagged{}'.format(i)]['count'])**2).reshape(-1,1)\n",
    "    globals()['y_departures_test_squared{}'.format(i)]= np.concatenate((globals()['y_departures_test_squared{}'.format(i)], np.array(globals()['departures_test_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())).reshape(-1,1)), axis = 1)\n",
    "    globals()['y_arrivals_test_squared{}'.format(i)] = (np.array(globals()['arrivals_test_lagged{}'.format(i)]['count'])**2).reshape(-1,1)\n",
    "    globals()['y_arrivals_test_squared{}'.format(i)]= np.concatenate((globals()['y_arrivals_test_squared{}'.format(i)], np.array(globals()['arrivals_test_lagged{}'.format(i)]['date'].apply(lambda x: x.weekday())).reshape(-1,1)), axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# Split the target dataset into weekday and weekend data.\n",
    "for i in range(n):\n",
    "    globals()['y_departures_train_weekday_squared{}'.format(i)] = globals()['y_departures_train_squared{}'.format(i)][globals()['y_departures_train_squared{}'.format(i)][:,1] < 5]\n",
    "    globals()['y_departures_train_weekend_squared{}'.format(i)] = globals()['y_departures_train_squared{}'.format(i)][globals()['y_departures_train_squared{}'.format(i)][:,1] > 4]\n",
    "    globals()['y_departures_test_weekday_squared{}'.format(i)] = globals()['y_departures_test_squared{}'.format(i)][globals()['y_departures_test_squared{}'.format(i)][:,1] < 5]\n",
    "    globals()['y_departures_test_weekend_squared{}'.format(i)] = globals()['y_departures_test_squared{}'.format(i)][globals()['y_departures_test_squared{}'.format(i)][:,1] > 4]\n",
    "    \n",
    "    globals()['y_arrivals_train_weekday_squared{}'.format(i)] = globals()['y_arrivals_train_squared{}'.format(i)][globals()['y_arrivals_train_squared{}'.format(i)][:,1] < 5]\n",
    "    globals()['y_arrivals_train_weekend_squared{}'.format(i)] = globals()['y_arrivals_train_squared{}'.format(i)][globals()['y_arrivals_train_squared{}'.format(i)][:,1] > 4]\n",
    "    globals()['y_arrivals_test_weekday_squared{}'.format(i)] = globals()['y_arrivals_test_squared{}'.format(i)][globals()['y_arrivals_test_squared{}'.format(i)][:,1] < 5]\n",
    "    globals()['y_arrivals_test_weekend_squared{}'.format(i)] = globals()['y_arrivals_test_squared{}'.format(i)][globals()['y_arrivals_test_squared{}'.format(i)][:,1] > 4]\n",
    "\n",
    "# Delete the \"day of week\" column and only keep the correct way to present categorical data to your model.\n",
    "for i in range(n):\n",
    "    globals()['y_departures_train_weekday_squared{}'.format(i)] = np.delete(globals()['y_departures_train_weekday_squared{}'.format(i)], 1, 1)\n",
    "    globals()['y_departures_train_weekend_squared{}'.format(i)] = np.delete(globals()['y_departures_train_weekend_squared{}'.format(i)], 1, 1)\n",
    "    globals()['y_departures_test_weekday_squared{}'.format(i)] = np.delete(globals()['y_departures_test_weekday_squared{}'.format(i)], 1, 1)\n",
    "    globals()['y_departures_test_weekend_squared{}'.format(i)] = np.delete(globals()['y_departures_test_weekend_squared{}'.format(i)], 1, 1)\n",
    "    \n",
    "    globals()['y_arrivals_train_weekday_squared{}'.format(i)] = np.delete(globals()['y_arrivals_train_weekday_squared{}'.format(i)], 1, 1)\n",
    "    globals()['y_arrivals_train_weekend_squared{}'.format(i)] = np.delete(globals()['y_arrivals_train_weekend_squared{}'.format(i)], 1, 1)\n",
    "    globals()['y_arrivals_test_weekday_squared{}'.format(i)] = np.delete(globals()['y_arrivals_test_weekday_squared{}'.format(i)], 1, 1)\n",
    "    globals()['y_arrivals_test_weekend_squared{}'.format(i)] = np.delete(globals()['y_arrivals_test_weekend_squared{}'.format(i)], 1, 1)\n",
    "\n",
    "# We need y to be a 1D array for the linear regression approach, so reshape again.\n",
    "for i in range(n):\n",
    "    globals()['y_departures_train_weekday_squared{}'.format(i)] = globals()['y_departures_train_weekday_squared{}'.format(i)].reshape(-1)\n",
    "    globals()['y_departures_train_weekend_squared{}'.format(i)] = globals()['y_departures_train_weekend_squared{}'.format(i)].reshape(-1)\n",
    "    globals()['y_departures_test_weekday_squared{}'.format(i)] = globals()['y_departures_test_weekday_squared{}'.format(i)].reshape(-1)\n",
    "    globals()['y_departures_test_weekend_squared{}'.format(i)] = globals()['y_departures_test_weekend_squared{}'.format(i)].reshape(-1)\n",
    "    \n",
    "    globals()['y_arrivals_train_weekday_squared{}'.format(i)] = globals()['y_arrivals_train_weekday_squared{}'.format(i)].reshape(-1)\n",
    "    globals()['y_arrivals_train_weekend_squared{}'.format(i)] = globals()['y_arrivals_train_weekend_squared{}'.format(i)].reshape(-1)\n",
    "    globals()['y_arrivals_test_weekday_squared{}'.format(i)] = globals()['y_arrivals_test_weekday_squared{}'.format(i)].reshape(-1)\n",
    "    globals()['y_arrivals_test_weekend_squared{}'.format(i)] = globals()['y_arrivals_test_weekend_squared{}'.format(i)].reshape(-1)\n",
    "\n",
    "# Standerdize the y data\n",
    "for i in range(n):\n",
    "    globals()['y_departures_train_weekday_squared_std{}'.format(i)] = (globals()['y_departures_train_weekday_squared{}'.format(i)] - np.mean(globals()['y_departures_train_weekday_squared{}'.format(i)], axis=0)) / np.std(globals()['y_departures_train_weekday_squared{}'.format(i)], axis=0)\n",
    "    globals()['y_departures_train_weekend_squared_std{}'.format(i)] = (globals()['y_departures_train_weekend_squared{}'.format(i)] - np.mean(globals()['y_departures_train_weekend_squared{}'.format(i)], axis=0)) / np.std(globals()['y_departures_train_weekend_squared{}'.format(i)], axis=0)\n",
    "    globals()['y_departures_test_weekday_squared_std{}'.format(i)] = (globals()['y_departures_test_weekday_squared{}'.format(i)] - np.mean(globals()['y_departures_test_weekday_squared{}'.format(i)], axis=0)) / np.std(globals()['y_departures_test_weekday_squared{}'.format(i)], axis=0)\n",
    "    globals()['y_departures_test_weekend_squared_std{}'.format(i)] = (globals()['y_departures_test_weekend_squared{}'.format(i)] - np.mean(globals()['y_departures_test_weekend_squared{}'.format(i)], axis=0)) / np.std(globals()['y_departures_test_weekend_squared{}'.format(i)], axis=0)\n",
    "\n",
    "    globals()['y_arrivals_train_weekday_squared_std{}'.format(i)] = (globals()['y_arrivals_train_weekday_squared{}'.format(i)] - np.mean(globals()['y_arrivals_train_weekday_squared{}'.format(i)], axis=0)) / np.std(globals()['y_arrivals_train_weekday_squared{}'.format(i)], axis=0)\n",
    "    globals()['y_arrivals_train_weekend_squared_std{}'.format(i)] = (globals()['y_arrivals_train_weekend_squared{}'.format(i)] - np.mean(globals()['y_arrivals_train_weekend_squared{}'.format(i)], axis=0)) / np.std(globals()['y_arrivals_train_weekend_squared{}'.format(i)], axis=0)\n",
    "    globals()['y_arrivals_test_weekday_squared_std{}'.format(i)] = (globals()['y_arrivals_test_weekday_squared{}'.format(i)] - np.mean(globals()['y_arrivals_test_weekday_squared{}'.format(i)], axis=0)) / np.std(globals()['y_arrivals_test_weekday_squared{}'.format(i)], axis=0)\n",
    "    globals()['y_arrivals_test_weekend_squared_std{}'.format(i)] = (globals()['y_arrivals_test_weekend_squared{}'.format(i)] - np.mean(globals()['y_arrivals_test_weekend_squared{}'.format(i)], axis=0)) / np.std(globals()['y_arrivals_test_weekend_squared{}'.format(i)], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import the linear regression model\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "# Train weekday models\n",
    "for i in range(n):\n",
    "    globals()['model_departures_weekday_squared{}'.format(i)] = Ridge().fit(globals()['x_departures_train_weekday_std{}'.format(i)], globals()['y_departures_train_weekday_squared_std{}'.format(i)])\n",
    "    globals()['model_arrivals_weekday_squared{}'.format(i)] = Ridge().fit(globals()['x_arrivals_train_weekday_std{}'.format(i)], globals()['y_arrivals_train_weekday_squared_std{}'.format(i)])\n",
    "# Train weekend models\n",
    "for i in range(n):\n",
    "    globals()['model_departures_weekend_squared{}'.format(i)] = Ridge().fit(globals()['x_departures_train_weekend_std{}'.format(i)], globals()['y_departures_train_weekend_squared_std{}'.format(i)])\n",
    "    globals()['model_arrivals_weekend_squared{}'.format(i)] = Ridge().fit(globals()['x_arrivals_train_weekend_std{}'.format(i)], globals()['y_arrivals_train_weekend_squared_std{}'.format(i)])\n",
    "\n",
    "# Predict target variable for all clusters weekday test input data\n",
    "for i in range(n):\n",
    "    globals()['y_departures_pred_weekday_squared{}'.format(i)] = globals()['model_departures_weekday_squared{}'.format(i)].predict(globals()['x_departures_test_weekday_std{}'.format(i)])\n",
    "    globals()['y_arrivals_pred_weekday_squared{}'.format(i)] = globals()['model_arrivals_weekday_squared{}'.format(i)].predict(globals()['x_arrivals_test_weekday_std{}'.format(i)])\n",
    "# Predict target variable for all clusters weekend test input data\n",
    "for i in range(n):\n",
    "    globals()['y_departures_pred_weekend_squared{}'.format(i)] = globals()['model_departures_weekend_squared{}'.format(i)].predict(globals()['x_departures_test_weekend_std{}'.format(i)])\n",
    "    globals()['y_arrivals_pred_weekend_squared{}'.format(i)] = globals()['model_arrivals_weekend_squared{}'.format(i)].predict(globals()['x_arrivals_test_weekend_std{}'.format(i)])\n",
    "\n",
    "# Calculate r^2 for all clusters weekday test input data\n",
    "from sklearn.metrics import r2_score\n",
    "for i in range(n):\n",
    "    globals()['r2_departures_weekday_squared{}'.format(i)] = r2_score(globals()['y_departures_test_weekday_squared_std{}'.format(i)], globals()['y_departures_pred_weekday_squared{}'.format(i)])\n",
    "    globals()['r2_arrivals_weekday_squared{}'.format(i)] = r2_score(globals()['y_arrivals_test_weekday_squared_std{}'.format(i)], globals()['y_arrivals_pred_weekday_squared{}'.format(i)])\n",
    "# Calculate r^2 for all clusters weekend test input data\n",
    "for i in range(n):\n",
    "    globals()['r2_departures_weekend_squared{}'.format(i)] = r2_score(globals()['y_departures_test_weekend_squared_std{}'.format(i)], globals()['y_departures_pred_weekend_squared{}'.format(i)])\n",
    "    globals()['r2_arrivals_weekend_squared{}'.format(i)] = r2_score(globals()['y_arrivals_test_weekend_squared_std{}'.format(i)], globals()['y_arrivals_pred_weekend_squared{}'.format(i)])\n",
    "\n",
    "# Print r^2 for all clusters weekday test input data\n",
    "for i in range(n):\n",
    "    print('r2 departures weekday cluster {}: {}'.format(i, globals()['r2_departures_weekday_squared{}'.format(i)]))\n",
    "    print('r2 arrivals weekday cluster {}: {}'.format(i, globals()['r2_arrivals_weekday_squared{}'.format(i)]))\n",
    "# Print r^2 for all clusters weekend test input data\n",
    "for i in range(n):\n",
    "    print('r2 departures weekend cluster {}: {}'.format(i, globals()['r2_departures_weekend_squared{}'.format(i)]))\n",
    "    print('r2 arrivals weekend cluster {}: {}'.format(i, globals()['r2_arrivals_weekend_squared{}'.format(i)]))\n",
    "    \n",
    "\n",
    "# Plot prediction performance for all clusters test data. \n",
    "plt.subplot(2, 2, 1)\n",
    "# Color the bars that fall below the 0.6 threshold red, the others blue.\n",
    "plt.bar(range(n), [globals()['r2_departures_weekday_squared{}'.format(i)] for i in range(n)], color=['red' if globals()['r2_departures_weekday_squared{}'.format(i)] < 0.6 else 'blue' for i in range(n)])\n",
    "plt.ylabel('r2')\n",
    "plt.xlabel('cluster')\n",
    "plt.title('r2 for departures weekday clusters'+', ' +features)\n",
    "# Put the label of the clusters on top of each bin in the histogram. Center the label, give some whitespace between histogram top and label to avoid overlap.\n",
    "# Reminder that we renamed the largest cluster to 0, so we had to account for this.\n",
    "for i in range(n):\n",
    "    plt.text(i, globals()['r2_departures_weekday_squared{}'.format(i)]+0.01, largest_cluster[i], rotation=0, ha='center')\n",
    "# Remove xticks, no meaning\n",
    "plt.xticks([])\n",
    "# Put horizontal line at r^2 = 0.6\n",
    "plt.axhline(y=0.6, color='r', linestyle='-')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "# Color the bars that fall below the 0.6 threshold red, the others blue.\n",
    "plt.bar(range(n), [globals()['r2_arrivals_weekday_squared{}'.format(i)] for i in range(n)], color=['red' if globals()['r2_arrivals_weekday_squared{}'.format(i)] < 0.6 else 'blue' for i in range(n)])\n",
    "plt.ylabel('r2')\n",
    "plt.xlabel('cluster')\n",
    "plt.title('r2 for arrivals weekday clusters'+', ' + features)\n",
    "# Put the label of the clusters on top of each bin in the histogram. Center the label, give some whitespace between histogram top and label to avoid overlap.\n",
    "# Reminder that we renamed the largest cluster to 0, so we had to account for this.\n",
    "for i in range(n):\n",
    "    plt.text(i, globals()['r2_arrivals_weekday_squared{}'.format(i)]+0.01, largest_cluster[i], rotation=0, ha='center')\n",
    "# Remove xticks, no meaning\n",
    "plt.xticks([])\n",
    "# Put horizontal line at r^2 = 0.6\n",
    "plt.axhline(y=0.6, color='r', linestyle='-')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "# Color the bars that fall below the 0.6 threshold red, the others blue.\n",
    "plt.bar(range(n), [globals()['r2_departures_weekend_squared{}'.format(i)] for i in range(n)], color=['red' if globals()['r2_departures_weekend_squared{}'.format(i)] < 0.6 else 'blue' for i in range(n)])\n",
    "plt.ylabel('r2')\n",
    "plt.xlabel('cluster')\n",
    "plt.title('r2 for departures weekend clusters'+', ' +features)\n",
    "# Put the label of the clusters on top of each bin in the histogram. Center the label, give some whitespace between histogram top and label to avoid overlap.\n",
    "# Reminder that we renamed the largest cluster to 0, so we had to account for this.\n",
    "for i in range(n):\n",
    "    plt.text(i, globals()['r2_departures_weekend_squared{}'.format(i)]+0.01, largest_cluster[i], rotation=0, ha='center')\n",
    "# Remove xticks, no meaning\n",
    "plt.xticks([])\n",
    "# Put horizontal line at r^2 = 0.6\n",
    "plt.axhline(y=0.6, color='r', linestyle='-')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "# Color the bars that fall below the 0.6 threshold red, the others blue.\n",
    "plt.bar(range(n), [globals()['r2_arrivals_weekend_squared{}'.format(i)] for i in range(n)], color=['red' if globals()['r2_arrivals_weekend_squared{}'.format(i)] < 0.6 else 'blue' for i in range(n)])\n",
    "plt.ylabel('r2')\n",
    "plt.xlabel('cluster')\n",
    "plt.title('r2 for arrivals weekend clusters'+', ' +features)\n",
    "# Put the label of the clusters on top of each bin in the histogram. Center the label, give some whitespace between histogram top and label to avoid overlap.\n",
    "# Reminder that we renamed the largest cluster to 0, so we had to account for this.\n",
    "for i in range(n):\n",
    "    plt.text(i, globals()['r2_arrivals_weekend_squared{}'.format(i)]+0.01, largest_cluster[i], rotation=0, ha='center')\n",
    "# Remove xticks, no meaning\n",
    "plt.xticks([])\n",
    "# Put horizontal line at r^2 = 0.6\n",
    "plt.axhline(y=0.6, color='r', linestyle='-')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Challenge Part 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the intermezzo chapter, we will apply a recursive approach for predicting the departures/arrivals counts. The approach is as follows: for a snippet of data from the test set (Appendix A19). Predict departures/arrivals counts and counts² using the models created. Supply these predictions into the input dataset as lag value to make a prediction for the next future hour. Continue this recursive process for the following 24 hours. \n",
    "\n",
    "The following code will only work if the model has been fitted for degree = 2, features = \"poly + sine + cosine\", and any lag. (Appendix A20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random snippet, ensure you stay inside the bounds of the test dataset (so not too large n_snippet)\n",
    "n_snippet = 3\n",
    "\n",
    "for i in range(n):\n",
    "    globals()[\"x_departures_test_weekday{}_initial\".format(i)] = globals()[\"x_departures_test_weekday{}\".format(i)][n_snippet*n_lag_dep_test:(n_snippet+1)*n_lag_dep_test]\n",
    "    globals()[\"x_departures_test_weekend{}_initial\".format(i)] = globals()[\"x_departures_test_weekend{}\".format(i)][n_snippet*n_lag_dep_test:(n_snippet+1)*n_lag_dep_test]\n",
    "    globals()[\"x_arrivals_test_weekday{}_initial\".format(i)] = globals()[\"x_arrivals_test_weekday{}\".format(i)][n_snippet*n_lag_dep_test:(n_snippet+1)*n_lag_dep_test]\n",
    "    globals()[\"x_arrivals_test_weekend{}_initial\".format(i)] = globals()[\"x_arrivals_test_weekend{}\".format(i)][n_snippet*n_lag_dep_test:(n_snippet+1)*n_lag_dep_test]\n",
    "    # the indexes used ensure the final entry of the snippet is at 23 PM, allowing us to make a prediction for the next 24 hour day.\n",
    "\n",
    "# We made this snippet from the unstandardised data purely to check whether the final value of the snippet is indeed at 23 PM. If the data was standardised, we would not be able to check this.\n",
    "# Check if the final value is at hour 23, the hour was stored in the 8th column of the dataset. \n",
    "print(x_departures_test_weekday0_initial[:,8])\n",
    "print(len(x_departures_test_weekday0_initial[:,8]))\n",
    "print(x_departures_test_weekend0_initial[:,8])\n",
    "print(len(x_departures_test_weekend0_initial[:,8]))\n",
    "print(x_arrivals_test_weekday0_initial[:,8])\n",
    "print(len(x_arrivals_test_weekday0_initial[:,8]))\n",
    "print(x_arrivals_test_weekend0_initial[:,8])\n",
    "print(len(x_arrivals_test_weekend0_initial[:,8]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we are sure the final hour in the dataset is 23 PM, we can simply use the same snippet indexes on the standardised data. \n",
    "for i in range(n):\n",
    "    globals()[\"x_departures_test_weekday{}_initial_std\".format(i)] = globals()[\"x_departures_test_weekday_std{}\".format(i)][n_snippet*n_lag_dep_test:(n_snippet+1)*n_lag_dep_test]\n",
    "    globals()[\"x_departures_test_weekend{}_initial_std\".format(i)] = globals()[\"x_departures_test_weekend_std{}\".format(i)][n_snippet*n_lag_dep_test:(n_snippet+1)*n_lag_dep_test]\n",
    "    globals()[\"x_arrivals_test_weekday{}_initial_std\".format(i)] = globals()[\"x_arrivals_test_weekday_std{}\".format(i)][n_snippet*n_lag_dep_test:(n_snippet+1)*n_lag_dep_test]\n",
    "    globals()[\"x_arrivals_test_weekend{}_initial_std\".format(i)] = globals()[\"x_arrivals_test_weekend_std{}\".format(i)][n_snippet*n_lag_dep_test:(n_snippet+1)*n_lag_dep_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the recursive approach. (Appendix A21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the next hour for each cluster. Add a new row to the initial dataset. for index 0 add a 1, for index 1 to , add a one in column next to the one there was one already,\n",
    "# for index 8 add the standardized hour, for index 9 add the standardized cos(hour), for index 10 add the standardized sin(hour), for index 11 add the predicted value for the next hour, \n",
    "# for index 12 take the value that was at index 11 in the previous row, for index 13 take index 12 in previous row and so on until index 11+n_lag. for index 11+n_lag+1 use the predicted \n",
    "# value on count squared for the next hour, for index 11+n_lag+2 take the value that was at index 11+n_lag+1 in the previous row, etc.\n",
    "for i in range(n):\n",
    "    for j in range(24):\n",
    "        new_row_departures_test_weekday = np.zeros((1, 10+(n_lag_dep_test + 1)*degree))\n",
    "        new_row_departures_test_weekend = np.zeros((1, 10+(n_lag_dep_test + 1)*degree))\n",
    "        new_row_arrivals_test_weekday = np.zeros((1, 10+(n_lag_arr_test + 1)*degree))\n",
    "        new_row_arrivals_test_weekend = np.zeros((1, 10+(n_lag_arr_test + 1)*degree))\n",
    "        \n",
    "        # Add 1 to index 0\n",
    "        new_row_departures_test_weekday[0,0] = 1\n",
    "        new_row_departures_test_weekend[0,0] = 1\n",
    "        new_row_arrivals_test_weekday[0,0] = 1\n",
    "        new_row_arrivals_test_weekend[0,0] = 1\n",
    "\n",
    "        # Check which index from index 1-7 had a 1 in the first row of initial std dataset, and add a 1 to the next index, if the index was 7, add a 1 to index 1\n",
    "        for k in range(1,8):\n",
    "            if globals()[\"x_departures_test_weekday{}_initial_std\".format(i)][0,k] == 1:\n",
    "                if k == 7:\n",
    "                    new_row_departures_test_weekday[0,1] = 1\n",
    "                else:\n",
    "                    new_row_departures_test_weekday[0,k+1] = 1\n",
    "            if globals()[\"x_departures_test_weekend{}_initial_std\".format(i)][0,k] == 1:\n",
    "                if k == 7:\n",
    "                    new_row_departures_test_weekend[0,1] = 1\n",
    "                else:\n",
    "                    new_row_departures_test_weekend[0,k+1] = 1\n",
    "            if globals()[\"x_arrivals_test_weekday{}_initial_std\".format(i)][0,k] == 1:\n",
    "                if k == 7:\n",
    "                    new_row_arrivals_test_weekday[0,1] = 1\n",
    "                else:\n",
    "                    new_row_arrivals_test_weekday[0,k+1] = 1\n",
    "            if globals()[\"x_arrivals_test_weekend{}_initial_std\".format(i)][0,k] == 1:\n",
    "                if k == 7:\n",
    "                    new_row_arrivals_test_weekend[0,1] = 1\n",
    "                else:\n",
    "                    new_row_arrivals_test_weekend[0,k+1] = 1\n",
    "\n",
    "        \n",
    "        # Add the j'th entry of snippet col 9 to index 8 (note i use the nomenclature: first col = col 1 = index 0)\n",
    "        new_row_departures_test_weekday[0,8] = globals()[\"x_departures_test_weekday{}_initial_std\".format(i)][j,8]\n",
    "        new_row_departures_test_weekend[0,8] = globals()[\"x_departures_test_weekend{}_initial_std\".format(i)][j,8]\n",
    "        new_row_arrivals_test_weekday[0,8] = globals()[\"x_arrivals_test_weekday{}_initial_std\".format(i)][j,8]\n",
    "        new_row_arrivals_test_weekend[0,8] = globals()[\"x_arrivals_test_weekend{}_initial_std\".format(i)][j,8]\n",
    "\n",
    "        # Add the j'th entry of snippet col 10 to index 9\n",
    "        new_row_departures_test_weekday[0,9] = globals()[\"x_departures_test_weekday{}_initial_std\".format(i)][j,9]\n",
    "        new_row_departures_test_weekend[0,9] = globals()[\"x_departures_test_weekend{}_initial_std\".format(i)][j,9]\n",
    "        new_row_arrivals_test_weekday[0,9] = globals()[\"x_arrivals_test_weekday{}_initial_std\".format(i)][j,9]\n",
    "        new_row_arrivals_test_weekend[0,9] = globals()[\"x_arrivals_test_weekend{}_initial_std\".format(i)][j,9]\n",
    "\n",
    "        # Add the j'th entry of snippet col 11 to index 10\n",
    "        new_row_departures_test_weekday[0,10] = globals()[\"x_departures_test_weekday{}_initial_std\".format(i)][j,10]\n",
    "        new_row_departures_test_weekend[0,10] = globals()[\"x_departures_test_weekend{}_initial_std\".format(i)][j,10]\n",
    "        new_row_arrivals_test_weekday[0,10] = globals()[\"x_arrivals_test_weekday{}_initial_std\".format(i)][j,10]\n",
    "        new_row_arrivals_test_weekend[0,10] = globals()[\"x_arrivals_test_weekend{}_initial_std\".format(i)][j,10]\n",
    "\n",
    "        # Add the count model predicted value for the next hour to index 11\n",
    "        new_row_departures_test_weekday[0,11] = globals()[\"model_departures_weekday{}\".format(i)].predict(globals()[\"x_departures_test_weekday{}_initial_std\".format(i)])[-1]\n",
    "        new_row_departures_test_weekend[0,11] = globals()[\"model_departures_weekend{}\".format(i)].predict(globals()[\"x_departures_test_weekend{}_initial_std\".format(i)])[-1]\n",
    "        new_row_arrivals_test_weekday[0,11] = globals()[\"model_arrivals_weekday{}\".format(i)].predict(globals()[\"x_arrivals_test_weekday{}_initial_std\".format(i)])[-1]\n",
    "        new_row_arrivals_test_weekend[0,11] = globals()[\"model_arrivals_weekend{}\".format(i)].predict(globals()[\"x_arrivals_test_weekend{}_initial_std\".format(i)])[-1]\n",
    "\n",
    "        \n",
    "        # Add the value that was at index 11 in the previous row to index 12 and do this for the next n_lag_dep_test-1 indexes\n",
    "        for k in range(1,n_lag_dep_test):\n",
    "            new_row_departures_test_weekday[0,11+k] = globals()[\"x_departures_test_weekday{}_initial_std\".format(i)][-1,11+k-1]\n",
    "            new_row_departures_test_weekend[0,11+k] = globals()[\"x_departures_test_weekend{}_initial_std\".format(i)][-1,11+k-1]\n",
    "            new_row_arrivals_test_weekday[0,11+k] = globals()[\"x_arrivals_test_weekday{}_initial_std\".format(i)][-1,11+k-1]\n",
    "            new_row_arrivals_test_weekend[0,11+k] = globals()[\"x_arrivals_test_weekend{}_initial_std\".format(i)][-1,11+k-1]\n",
    "        \n",
    "        # Add the j'th entry of snippet col 12+n_lag_dep_test (= hour²) to index 11+n_lag_dep_test\n",
    "        new_row_departures_test_weekday[0,11+n_lag_dep_test] = globals()[\"x_departures_test_weekday{}_initial_std\".format(i)][j,11+n_lag_dep_test]\n",
    "        new_row_departures_test_weekend[0,11+n_lag_dep_test] = globals()[\"x_departures_test_weekend{}_initial_std\".format(i)][j,11+n_lag_dep_test]\n",
    "        new_row_arrivals_test_weekday[0,11+n_lag_dep_test] = globals()[\"x_arrivals_test_weekday{}_initial_std\".format(i)][j,11+n_lag_dep_test]\n",
    "        new_row_arrivals_test_weekend[0,11+n_lag_dep_test] = globals()[\"x_arrivals_test_weekend{}_initial_std\".format(i)][j,11+n_lag_dep_test]\n",
    "\n",
    "        # Add the count² model predicted value for the next hour to index 12+n_lag_dep_test\n",
    "        new_row_departures_test_weekday[0,12+n_lag_dep_test] = globals()[\"model_departures_weekday_squared{}\".format(i)].predict(globals()[\"x_departures_test_weekday{}_initial_std\".format(i)])[-1]\n",
    "        new_row_departures_test_weekend[0,12+n_lag_dep_test] = globals()[\"model_departures_weekend_squared{}\".format(i)].predict(globals()[\"x_departures_test_weekend{}_initial_std\".format(i)])[-1]\n",
    "        new_row_arrivals_test_weekday[0,12+n_lag_dep_test] = globals()[\"model_arrivals_weekday_squared{}\".format(i)].predict(globals()[\"x_arrivals_test_weekday{}_initial_std\".format(i)])[-1]\n",
    "        new_row_arrivals_test_weekend[0,12+n_lag_dep_test] = globals()[\"model_arrivals_weekend_squared{}\".format(i)].predict(globals()[\"x_arrivals_test_weekend{}_initial_std\".format(i)])[-1]\n",
    "\n",
    "        # Add the value that was at index 12+n_lag_dep_test in the previous row to index 13+n_lag_dep_test and do this for the next n_lag_dep_test-1 indexes\n",
    "        for k in range(1,n_lag_dep_test):\n",
    "            new_row_departures_test_weekday[0,12+n_lag_dep_test+k] = globals()[\"x_departures_test_weekday{}_initial_std\".format(i)][-1,12+n_lag_dep_test+k-1]\n",
    "            new_row_departures_test_weekend[0,12+n_lag_dep_test+k] = globals()[\"x_departures_test_weekend{}_initial_std\".format(i)][-1,12+n_lag_dep_test+k-1]\n",
    "            new_row_arrivals_test_weekday[0,12+n_lag_dep_test+k] = globals()[\"x_arrivals_test_weekday{}_initial_std\".format(i)][-1,12+n_lag_dep_test+k-1]\n",
    "            new_row_arrivals_test_weekend[0,12+n_lag_dep_test+k] = globals()[\"x_arrivals_test_weekend{}_initial_std\".format(i)][-1,12+n_lag_dep_test+k-1]\n",
    "\n",
    "        # Add the new row to the initial standerdized dataset\n",
    "        globals()[\"x_departures_test_weekday{}_initial_std\".format(i)] = np.append(globals()[\"x_departures_test_weekday{}_initial_std\".format(i)], new_row_departures_test_weekday, axis=0)\n",
    "        globals()[\"x_departures_test_weekend{}_initial_std\".format(i)] = np.append(globals()[\"x_departures_test_weekend{}_initial_std\".format(i)], new_row_departures_test_weekend, axis=0)\n",
    "        globals()[\"x_arrivals_test_weekday{}_initial_std\".format(i)] = np.append(globals()[\"x_arrivals_test_weekday{}_initial_std\".format(i)], new_row_arrivals_test_weekday, axis=0)\n",
    "        globals()[\"x_arrivals_test_weekend{}_initial_std\".format(i)] = np.append(globals()[\"x_arrivals_test_weekend{}_initial_std\".format(i)], new_row_arrivals_test_weekend, axis=0)\n",
    "\n",
    "# Rename dataset for clarity \n",
    "for i in range(n):\n",
    "    globals()[\"x_departures_test_weekday{}_predicted\".format(i)] = globals()[\"x_departures_test_weekday{}_initial_std\".format(i)]\n",
    "    globals()[\"x_departures_test_weekend{}_predicted\".format(i)] = globals()[\"x_departures_test_weekend{}_initial_std\".format(i)]\n",
    "    globals()[\"x_arrivals_test_weekday{}_predicted\".format(i)] = globals()[\"x_arrivals_test_weekday{}_initial_std\".format(i)]\n",
    "    globals()[\"x_arrivals_test_weekend{}_predicted\".format(i)] = globals()[\"x_arrivals_test_weekend{}_initial_std\".format(i)]\n",
    "\n",
    "# For each cluster, extract the counts prediction column, column 11:\n",
    "for i in range(n):\n",
    "    globals()[\"counts_departures_test_weekday{}_predicted\".format(i)] = globals()[\"x_departures_test_weekday{}_predicted\".format(i)][:,11]\n",
    "    globals()[\"counts_departures_test_weekend{}_predicted\".format(i)] = globals()[\"x_departures_test_weekend{}_predicted\".format(i)][:,11]\n",
    "    globals()[\"counts_arrivals_test_weekday{}_predicted\".format(i)] = globals()[\"x_arrivals_test_weekday{}_predicted\".format(i)][:,11]\n",
    "    globals()[\"counts_arrivals_test_weekend{}_predicted\".format(i)] = globals()[\"x_arrivals_test_weekend{}_predicted\".format(i)][:,11]\n",
    "\n",
    "\n",
    "# Use the mean and std of the training data for the y variables to unstanderdize the predicted values. Do not use of the test or full data, since these require values after the scope of the snippet.\n",
    "# Use train because we have access to this data since since it is \"in the past\" from the perspective of the test data. We want to unstanderdize to get an intuitive measure for the amount of bikes\n",
    "# predicted. \n",
    "for i in range(n):\n",
    "    globals()[\"counts_departures_test_weekday{}_predicted_unstd\".format(i)] = globals()[\"counts_departures_test_weekday{}_predicted\".format(i)]*np.std(globals()[\"y_departures_train_weekday{}\".format(i)]) + np.mean(globals()[\"y_departures_train_weekday{}\".format(i)])\n",
    "    globals()[\"counts_departures_test_weekend{}_predicted_unstd\".format(i)] = globals()[\"counts_departures_test_weekend{}_predicted\".format(i)]*np.std(globals()[\"y_departures_train_weekend{}\".format(i)]) + np.mean(globals()[\"y_departures_train_weekend{}\".format(i)])\n",
    "    globals()[\"counts_arrivals_test_weekday{}_predicted_unstd\".format(i)] = globals()[\"counts_arrivals_test_weekday{}_predicted\".format(i)]*np.std(globals()[\"y_arrivals_train_weekday{}\".format(i)]) + np.mean(globals()[\"y_arrivals_train_weekday{}\".format(i)])\n",
    "    globals()[\"counts_arrivals_test_weekend{}_predicted_unstd\".format(i)] = globals()[\"counts_arrivals_test_weekend{}_predicted\".format(i)]*np.std(globals()[\"y_arrivals_train_weekend{}\".format(i)]) + np.mean(globals()[\"y_arrivals_train_weekend{}\".format(i)])\n",
    "\n",
    "    \n",
    "\n",
    "# For each cluster, sum the predicted departures and arrivals for the last 24 hours in the unstanderdized y_dataset and take the difference to see how many bikes have to be imported, or are in surplus\n",
    "# at the end of the day. \n",
    "for i in range(n):\n",
    "    globals()[\"counts_departures_test_weekday{}_predicted_unstd_sum\".format(i)] = np.sum(globals()[\"counts_departures_test_weekday{}_predicted_unstd\".format(i)][-24:])\n",
    "    globals()[\"counts_departures_test_weekend{}_predicted_unstd_sum\".format(i)] = np.sum(globals()[\"counts_departures_test_weekend{}_predicted_unstd\".format(i)][-24:])\n",
    "    globals()[\"counts_arrivals_test_weekday{}_predicted_unstd_sum\".format(i)] = np.sum(globals()[\"counts_arrivals_test_weekday{}_predicted_unstd\".format(i)][-24:])\n",
    "    globals()[\"counts_arrivals_test_weekend{}_predicted_unstd_sum\".format(i)] = np.sum(globals()[\"counts_arrivals_test_weekend{}_predicted_unstd\".format(i)][-24:])\n",
    "\n",
    "    globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)] = globals()[\"counts_departures_test_weekday{}_predicted_unstd_sum\".format(i)] - globals()[\"counts_arrivals_test_weekday{}_predicted_unstd_sum\".format(i)]\n",
    "    globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)] = globals()[\"counts_departures_test_weekend{}_predicted_unstd_sum\".format(i)] - globals()[\"counts_arrivals_test_weekend{}_predicted_unstd_sum\".format(i)]\n",
    "\n",
    "# Round the sums to nearest integer.\n",
    "for i in range(n):\n",
    "    globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)] = int(np.rint(globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)]))\n",
    "    globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)] = int(np.rint(globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)]))\n",
    "\n",
    "     \n",
    "# If the value of the difference is negative, it means that there are more bikes that arrive than leave, so we dont have to import bikes, there is a surplus at the end of the day.\n",
    "# If the value of the difference is positive, it means that there are more bikes that leave than arrive, so we have to import bikes.\n",
    "print(' ')\n",
    "print(\"The amount of bikes to import or the amount left at the end of the day for each cluster, ordered by size of cluster in descending order:\")\n",
    "for i in range(n):\n",
    "    print('-----------------------------------')\n",
    "    print('----------- cluster {} -------------'.format(largest_cluster[i]))\n",
    "    print('-----------------------------------')\n",
    "    if globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)] > 0:\n",
    "        print('When the 24 hour sippet is taken on the selected weekday: for cluster {}, import {} bikes'.format(largest_cluster[i],globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)]))\n",
    "    else:\n",
    "        print('When the 24 hour sippet is taken on the selected weekday: for cluster {}, you will be left with {} bikes at the end of the day. No bikes to be imported.'.format(largest_cluster[i],-globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)]))\n",
    "    if globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)] > 0:\n",
    "        print('When the 24 hour sippet is taken on the selected weekend day: for cluster {}, import {} bikes'.format(largest_cluster[i],globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)]))\n",
    "    else:\n",
    "        print('When the 24 hour sippet is taken on the selected weekend day: for cluster {}, you will be left with {} bikes at the end of the day. No bikes to be imported.'.format(largest_cluster[i],-globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)]))\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "\n",
    "\n",
    "\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "\n",
    "\n",
    "# Used the unstanderdized value instead of standerdized one followed by unstanderdization using the mean and std of the training data for the y variables, because that would not have yielded the \n",
    "# actual present value. We can directly use the actual present value, so why not? \n",
    "for i in range(n):\n",
    "    globals()[\"y_departures_test_weekday{}_actual_unstd\".format(i)] = globals()[\"y_departures_test_weekday{}\".format(i)][(n_snippet+1)*n_lag_dep_test:(n_snippet+2)*n_lag_dep_test]\n",
    "    globals()[\"y_departures_test_weekend{}_actual_unstd\".format(i)] = globals()[\"y_departures_test_weekend{}\".format(i)][(n_snippet+1)*n_lag_dep_test:(n_snippet+2)*n_lag_dep_test]\n",
    "    globals()[\"y_arrivals_test_weekday{}_actual_unstd\".format(i)] = globals()[\"y_arrivals_test_weekday{}\".format(i)][(n_snippet+1)*n_lag_dep_test:(n_snippet+2)*n_lag_dep_test]\n",
    "    globals()[\"y_arrivals_test_weekend{}_actual_unstd\".format(i)] = globals()[\"y_arrivals_test_weekend{}\".format(i)][(n_snippet+1)*n_lag_dep_test:(n_snippet+2)*n_lag_dep_test]\n",
    "\n",
    "\n",
    "# Sum the actual values for the last 24 hours of the actual y data, and take the difference to see how many bikes have to be imported, or are in surplus at the end of the day.\n",
    "for i in range(n):\n",
    "    globals()[\"y_departures_test_weekday{}_actual_unstd_sum\".format(i)] = np.sum(globals()[\"y_departures_test_weekday{}_actual_unstd\".format(i)])\n",
    "    globals()[\"y_departures_test_weekend{}_actual_unstd_sum\".format(i)] = np.sum(globals()[\"y_departures_test_weekend{}_actual_unstd\".format(i)])\n",
    "    globals()[\"y_arrivals_test_weekday{}_actual_unstd_sum\".format(i)] = np.sum(globals()[\"y_arrivals_test_weekday{}_actual_unstd\".format(i)])\n",
    "    globals()[\"y_arrivals_test_weekend{}_actual_unstd_sum\".format(i)] = np.sum(globals()[\"y_arrivals_test_weekend{}_actual_unstd\".format(i)])\n",
    "\n",
    "    globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)] = globals()[\"y_departures_test_weekday{}_actual_unstd_sum\".format(i)] - globals()[\"y_arrivals_test_weekday{}_actual_unstd_sum\".format(i)]\n",
    "    globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)] = globals()[\"y_departures_test_weekend{}_actual_unstd_sum\".format(i)] - globals()[\"y_arrivals_test_weekend{}_actual_unstd_sum\".format(i)]\n",
    "\n",
    "# Round the sums to nearest integer\n",
    "for i in range(n):\n",
    "    globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)] = int(np.rint(globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)]))\n",
    "    globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)] = int(np.rint(globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)]))\n",
    "\n",
    "# Get the difference between the actual values and the predicted values for the last 24 hours of the actual y data to see how many bikes our model was off by.\n",
    "for i in range(n):\n",
    "    globals()[\"difference_test_weekday{}_pred_vs_act\".format(i)] = globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)] - globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)]\n",
    "    globals()[\"difference_test_weekend{}_pred_vs_act\".format(i)] = globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)] - globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)]\n",
    "\n",
    "# Round the sums to nearest integer\n",
    "for i in range(n):\n",
    "    globals()[\"difference_test_weekday{}_pred_vs_act\".format(i)] = np.abs(int(np.rint(globals()[\"difference_test_weekday{}_pred_vs_act\".format(i)])))\n",
    "    globals()[\"difference_test_weekend{}_pred_vs_act\".format(i)] = np.abs(int(np.rint(globals()[\"difference_test_weekend{}_pred_vs_act\".format(i)])))\n",
    "\n",
    "print(' ')\n",
    "print(\"The actual amount of bikes to import or the amount left at the end of the day for each cluster, ordered by size of cluster in descending order:\")\n",
    "for i in range(n):\n",
    "    print('-----------------------------------')\n",
    "    print('----------- cluster {} -------------'.format(largest_cluster[i]))\n",
    "    print('-----------------------------------')\n",
    "    if globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)] > 0:\n",
    "        print('When the 24 hour sippet is taken on the selected weekday: for cluster {}, import {} bikes'.format(largest_cluster[i],globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)]))\n",
    "        print(\"our prediction was off by {} bikes\".format(globals()[\"difference_test_weekday{}_pred_vs_act\".format(i)]))\n",
    "    else:\n",
    "        print('When the 24 hour sippet is taken on the selected weekday: for cluster {}, you will be left with {} bikes at the end of the day. No bikes to be imported.'.format(largest_cluster[i],-globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)]))\n",
    "        print(\"our prediction was off by {} bikes\".format(globals()[\"difference_test_weekday{}_pred_vs_act\".format(i)]))\n",
    "    if globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)] > 0:\n",
    "        print('When the  24 hour sippet is taken on the selected weekend day: for cluster {}, import {} bikes'.format(largest_cluster[i],globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)]))\n",
    "        print(\"our prediction was off by {} bikes\".format(globals()[\"difference_test_weekday{}_pred_vs_act\".format(i)]))\n",
    "    else:\n",
    "        print('When the 24 hour sippet is taken on the selected weekend day: for cluster {}, you will be left with {} bikes at the end of the day. No bikes to be imported.'.format(largest_cluster[i],-globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)]))\n",
    "        print(\"our prediction was off by {} bikes\".format(globals()[\"difference_test_weekday{}_pred_vs_act\".format(i)]))\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the performance of the predictions: plot the actual value in histogram, in the same bin, plot the predicted value in a different color.\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='Actual Deficit/Surplus')\n",
    "orange_patch = mpatches.Patch(color='orange', label='Predicted Deficit/Surplus')\n",
    "explanation_patch = mpatches.Patch(label = \"Positive value means deficit in bikes, import these bikes\")\n",
    "\n",
    "# Color list: \n",
    "color_blue = []\n",
    "color_orange = []\n",
    "for i in range(n):\n",
    "    color_blue.append(\"blue\")\n",
    "    color_orange.append(\"orange\")\n",
    "\n",
    "# Label list\n",
    "label_actual = []\n",
    "label_predicted = []\n",
    "for i in range(n):\n",
    "    label_actual.append(\"actual\")\n",
    "    label_predicted.append(\"predicted\")\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(n), [globals()['difference_test_weekday{}_actual_unstd_sum'.format(i)] for i in range(n)], color=color_blue, label=label_actual, alpha = 0.4)\n",
    "for i in range(n):\n",
    "    if globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)] < 0 and globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)]>0:\n",
    "        plt.text(i, globals()['difference_test_weekday{}_predicted_unstd_sum'.format(i)]+8, largest_cluster[i], rotation=0, ha='center')\n",
    "    if globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)] > 0 and globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)]<0:\n",
    "        plt.text(i, globals()['difference_test_weekday{}_actual_unstd_sum'.format(i)]+8, largest_cluster[i], rotation=0, ha='center')    \n",
    "    if globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)] < 0 and globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)]<0:\n",
    "        if np.abs(globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)]) > np.abs(globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)]):\n",
    "            plt.text(i, globals()['difference_test_weekday{}_actual_unstd_sum'.format(i)]-20, largest_cluster[i], rotation=0, ha='center')\n",
    "        if np.abs(globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)]) < np.abs(globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)]): \n",
    "            plt.text(i, globals()['difference_test_weekday{}_predicted_unstd_sum'.format(i)]-20, largest_cluster[i], rotation=0, ha='center')\n",
    "    if globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)] > 0 and globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)]>0:         \n",
    "        if np.abs(globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)]) > np.abs(globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)]):\n",
    "            plt.text(i, globals()['difference_test_weekday{}_actual_unstd_sum'.format(i)]+8, largest_cluster[i], rotation=0, ha='center')\n",
    "        if np.abs(globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)]) < np.abs(globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)]): \n",
    "            plt.text(i, globals()['difference_test_weekday{}_predicted_unstd_sum'.format(i)]+8, largest_cluster[i], rotation=0, ha='center')\n",
    "    if globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)] == 0 and globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)] == 0:\n",
    "        plt.text(i, 0, largest_cluster[i], rotation=0, ha='center')\n",
    "    if globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)] == 0 and globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)] > 0:\n",
    "        plt.text(i, globals()['difference_test_weekday{}_predicted_unstd_sum'.format(i)]+8, largest_cluster[i], rotation=0, ha='center')\n",
    "    if globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)] == 0 and globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)] < 0:\n",
    "        plt.text(i, globals()['difference_test_weekday{}_predicted_unstd_sum'.format(i)]-20, largest_cluster[i], rotation=0, ha='center')\n",
    "    if globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)] > 0 and globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)] == 0:\n",
    "        plt.text(i, globals()['difference_test_weekday{}_actual_unstd_sum'.format(i)]+8, largest_cluster[i], rotation=0, ha='center')\n",
    "    if globals()[\"difference_test_weekday{}_actual_unstd_sum\".format(i)] < 0 and globals()[\"difference_test_weekday{}_predicted_unstd_sum\".format(i)] == 0:\n",
    "        plt.text(i, globals()['difference_test_weekday{}_actual_unstd_sum'.format(i)]-20, largest_cluster[i], rotation=0, ha='center')\n",
    "plt.bar(range(n), [globals()['difference_test_weekday{}_predicted_unstd_sum'.format(i)] for i in range(n)], color=color_orange, label=label_predicted, alpha = 0.4)\n",
    "plt.title('Deficit/Surplus of bikes for 24 hour snippet on weekday')\n",
    "plt.xlabel(\"cluster\")\n",
    "plt.xticks([])\n",
    "plt.ylabel('Deficit/Surplus')\n",
    "plt.legend(loc='upper right')\n",
    "plt.legend(handles=[blue_patch, orange_patch])\n",
    "text_box = \"Positive histograms indicate bikes have to be imported\"\n",
    "plt.text(0.985, 0.01, text_box, transform=plt.gca().transAxes,\n",
    "         fontsize=8, verticalalignment='bottom', horizontalalignment='right',\n",
    "         bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.5'))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(n), [globals()['difference_test_weekend{}_actual_unstd_sum'.format(i)] for i in range(n)], color=color_blue, label=label_actual, alpha = 0.4)\n",
    "for i in range(n):\n",
    "    if globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)] < 0 and globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)]>0:\n",
    "        plt.text(i, globals()['difference_test_weekend{}_predicted_unstd_sum'.format(i)]+8, largest_cluster[i], rotation=0, ha='center')\n",
    "    if globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)] > 0 and globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)]<0:\n",
    "        plt.text(i, globals()['difference_test_weekend{}_actual_unstd_sum'.format(i)]+8, largest_cluster[i], rotation=0, ha='center')    \n",
    "    if globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)] < 0 and globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)]<0:\n",
    "        if np.abs(globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)]) > np.abs(globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)]):\n",
    "            plt.text(i, globals()['difference_test_weekend{}_actual_unstd_sum'.format(i)]-20, largest_cluster[i], rotation=0, ha='center')\n",
    "        if np.abs(globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)]) < np.abs(globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)]): \n",
    "            plt.text(i, globals()['difference_test_weekend{}_predicted_unstd_sum'.format(i)]-20, largest_cluster[i], rotation=0, ha='center')\n",
    "    if globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)] > 0 and globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)]>0:         \n",
    "        if np.abs(globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)]) > np.abs(globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)]):\n",
    "            plt.text(i, globals()['difference_test_weekend{}_actual_unstd_sum'.format(i)]+8, largest_cluster[i], rotation=0, ha='center')\n",
    "        if np.abs(globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)]) < np.abs(globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)]): \n",
    "            plt.text(i, globals()['difference_test_weekend{}_predicted_unstd_sum'.format(i)]+8, largest_cluster[i], rotation=0, ha='center')\n",
    "    if globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)] == 0 and globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)] == 0:\n",
    "        plt.text(i, 0, largest_cluster[i], rotation=0, ha='center')\n",
    "    if globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)] == 0 and globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)] > 0:\n",
    "        plt.text(i, globals()['difference_test_weekend{}_predicted_unstd_sum'.format(i)]+8, largest_cluster[i], rotation=0, ha='center')\n",
    "    if globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)] == 0 and globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)] < 0:\n",
    "        plt.text(i, globals()['difference_test_weekend{}_predicted_unstd_sum'.format(i)]-20, largest_cluster[i], rotation=0, ha='center')\n",
    "    if globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)] > 0 and globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)] == 0:\n",
    "        plt.text(i, globals()['difference_test_weekend{}_actual_unstd_sum'.format(i)]+8, largest_cluster[i], rotation=0, ha='center')\n",
    "    if globals()[\"difference_test_weekend{}_actual_unstd_sum\".format(i)] < 0 and globals()[\"difference_test_weekend{}_predicted_unstd_sum\".format(i)] == 0:\n",
    "        plt.text(i, globals()['difference_test_weekend{}_actual_unstd_sum'.format(i)]-20, largest_cluster[i], rotation=0, ha='center')\n",
    "plt.bar(range(n), [globals()['difference_test_weekend{}_predicted_unstd_sum'.format(i)] for i in range(n)], color=color_orange, label=label_predicted, alpha = 0.4)\n",
    "plt.title('Deficit/Surplus of bikes for 24 hour snippet on weekend')\n",
    "plt.xlabel(\"cluster\")\n",
    "plt.xticks([])\n",
    "plt.ylabel('Deficit/Surplus')\n",
    "plt.legend(loc='upper right')\n",
    "plt.legend(handles=[blue_patch, orange_patch])\n",
    "text_box = \"Positive histograms indicate bikes have to be imported\"\n",
    "plt.text(0.985, 0.01, text_box, transform=plt.gca().transAxes,\n",
    "         fontsize=8, verticalalignment='bottom', horizontalalignment='right',\n",
    "         bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relatively poor prediction accuracy from the model can be explained by two main hypotheses:\n",
    "1) The models used for the prediction were not all perfect, some clusters had models with prediction accuracies close to R² = 0.6, hence prediction quality was not perfect for all clusters.\n",
    "2) Even if the model had a large R² value, using the model recursively using its outputs would yield bad results if the amount of recursions is large (which is the case). An accuracy of 90% becomes one of 8% when applied 24 times using its output.\n",
    "\n",
    "Nevertheless, no improvement in the models has been found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================================================================================================================================================================================================\n",
    "======================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(n):\n",
    "#     globals()[\"x_departures_test_weekday{}_std_hour_values\".format(i)] = np.unique(globals()[\"x_departures_test_weekday_std{}\".format(i)][:,8])\n",
    "#     globals()[\"x_departures_test_weekend{}_std_hour_values\".format(i)] = np.unique(globals()[\"x_departures_test_weekend_std{}\".format(i)][:,8])\n",
    "#     globals()[\"x_arrivals_test_weekday{}_std_hour_values\".format(i)] = np.unique(globals()[\"x_arrivals_test_weekday_std{}\".format(i)][:,8])\n",
    "#     globals()[\"x_arrivals_test_weekend{}_std_hour_values\".format(i)] = np.unique(globals()[\"x_arrivals_test_weekend_std{}\".format(i)][:,8])\n",
    "\n",
    "# # ALso get the 24 hour representations for poly, sin(hour) and cos(hour) when necessary, indexes come from knowledge of data format. \n",
    "# if features == \"poly\":\n",
    "#     for i in range(n):\n",
    "#         globals()[\"x_departures_test_weekday{}_std_poly_hour_values\".format(i)] = np.unique(globals()[\"x_departures_test_weekday_std{}\".format(i)][:,8+n_lag_dep_test+1])\n",
    "#         globals()[\"x_departures_test_weekend{}_std_poly_hour_values\".format(i)] = np.unique(globals()[\"x_departures_test_weekend_std{}\".format(i)][:,8+n_lag_dep_test+1])\n",
    "#         globals()[\"x_arrivals_test_weekday{}_std_poly_hour_values\".format(i)] = np.unique(globals()[\"x_arrivals_test_weekday_std{}\".format(i)][:,8+n_lag_arr_test+1])\n",
    "#         globals()[\"x_arrivals_test_weekend{}_std_poly_hour_values\".format(i)] = np.unique(globals()[\"x_arrivals_test_weekend_std{}\".format(i)][:,8+n_lag_arr_test+1])\n",
    "    \n",
    "# if features == \"poly + sine\":\n",
    "#     for i in range(n):\n",
    "#         globals()[\"x_departures_test_weekday{}_std_poly_hour_values\".format(i)] = np.unique(globals()[\"x_departures_test_weekday_std{}\".format(i)][:,9+n_lag_dep_test+1])\n",
    "#         globals()[\"x_departures_test_weekend{}_std_poly_hour_values\".format(i)] = np.unique(globals()[\"x_departures_test_weekend_std{}\".format(i)][:,9+n_lag_dep_test+1])\n",
    "#         globals()[\"x_arrivals_test_weekday{}_std_poly_hour_values\".format(i)] = np.unique(globals()[\"x_arrivals_test_weekday_std{}\".format(i)][:,9+n_lag_arr_test+1])\n",
    "#         globals()[\"x_arrivals_test_weekend{}_std_poly_hour_values\".format(i)] = np.unique(globals()[\"x_arrivals_test_weekend_std{}\".format(i)][:,9+n_lag_arr_test+1])\n",
    "\n",
    "#     for i in range(n):\n",
    "#         globals()[\"x_departures_test_weekday{}_std_sin_hour_values\".format(i)] = np.unique(globals()[\"x_departures_test_weekday_std{}\".format(i)][:,9])\n",
    "#         globals()[\"x_departures_test_weekend{}_std_sin_hour_values\".format(i)] = np.unique(globals()[\"x_departures_test_weekend_std{}\".format(i)][:,9])\n",
    "#         globals()[\"x_arrivals_test_weekday{}_std_sin_hour_values\".format(i)] = np.unique(globals()[\"x_arrivals_test_weekday_std{}\".format(i)][:,9])\n",
    "#         globals()[\"x_arrivals_test_weekend{}_std_sin_hour_values\".format(i)] = np.unique(globals()[\"x_arrivals_test_weekend_std{}\".format(i)][:,9])    \n",
    "\n",
    "# if features == \"poly + sine + cosine\":\n",
    "#     for i in range(n):\n",
    "#         globals()[\"x_departures_test_weekday{}_std_poly_hour_values\".format(i)] = np.unique(globals()[\"x_departures_test_weekday_std{}\".format(i)][:,10+n_lag_dep_test+1])\n",
    "#         globals()[\"x_departures_test_weekend{}_std_poly_hour_values\".format(i)] = np.unique(globals()[\"x_departures_test_weekend_std{}\".format(i)][:,10+n_lag_dep_test+1])\n",
    "#         globals()[\"x_arrivals_test_weekday{}_std_poly_hour_values\".format(i)] = np.unique(globals()[\"x_arrivals_test_weekday_std{}\".format(i)][:,10+n_lag_arr_test+1])\n",
    "#         globals()[\"x_arrivals_test_weekend{}_std_poly_hour_values\".format(i)] = np.unique(globals()[\"x_arrivals_test_weekend_std{}\".format(i)][:,10+n_lag_arr_test+1])\n",
    "\n",
    "#     for i in range(n):\n",
    "#         globals()[\"x_departures_test_weekday{}_std_sin_hour_values\".format(i)] = np.unique(globals()[\"x_departures_test_weekday_std{}\".format(i)][:,10])\n",
    "#         globals()[\"x_departures_test_weekend{}_std_sin_hour_values\".format(i)] = np.unique(globals()[\"x_departures_test_weekend_std{}\".format(i)][:,10])\n",
    "#         globals()[\"x_arrivals_test_weekday{}_std_sin_hour_values\".format(i)] = np.unique(globals()[\"x_arrivals_test_weekday_std{}\".format(i)][:,10])\n",
    "#         globals()[\"x_arrivals_test_weekend{}_std_sin_hour_values\".format(i)] = np.unique(globals()[\"x_arrivals_test_weekend_std{}\".format(i)][:,10])\n",
    "\n",
    "#     for i in range(n):\n",
    "#         globals()[\"x_departures_test_weekday{}_std_cos_hour_values\".format(i)] = np.unique(globals()[\"x_departures_test_weekday_std{}\".format(i)][:,9])\n",
    "#         globals()[\"x_departures_test_weekend{}_std_cos_hour_values\".format(i)] = np.unique(globals()[\"x_departures_test_weekend_std{}\".format(i)][:,9])\n",
    "#         globals()[\"x_arrivals_test_weekday{}_std_cos_hour_values\".format(i)] = np.unique(globals()[\"x_arrivals_test_weekday_std{}\".format(i)][:,9])\n",
    "#         globals()[\"x_arrivals_test_weekend{}_std_cos_hour_values\".format(i)] = np.unique(globals()[\"x_arrivals_test_weekend_std{}\".format(i)][:,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_copy2.copy()\n",
    "\n",
    "#do same manipulations on df as done previously for the 5 largest clusters, but this time for all clusters\n",
    "df_departures = df[['starttime', 'pick_label']].copy()\n",
    "df_arrivals = df[['stoptime', 'drop_label']].copy()\n",
    "\n",
    "\n",
    "# rename label columns\n",
    "df_departures.rename(columns={'pick_label':'label'}, inplace=True)\n",
    "df_arrivals.rename(columns={'drop_label':'label'}, inplace=True)\n",
    "\n",
    "# Extract date and time components\n",
    "def extract_date_time_components(df, time_column):\n",
    "    df['hour'] = df[time_column].dt.hour\n",
    "    df['day'] = df[time_column].dt.day\n",
    "    df['month'] = df[time_column].dt.month\n",
    "    df.drop(columns=[time_column], inplace=True)\n",
    "\n",
    "extract_date_time_components(df_departures, 'starttime')\n",
    "extract_date_time_components(df_arrivals, 'stoptime')\n",
    "\n",
    "# Remove trip_id index\n",
    "df_departures.reset_index(drop=True, inplace=True)\n",
    "df_arrivals.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Count the number of rows with the same label, hour, and save it in a new column\n",
    "def count(df):\n",
    "    counts = df.groupby(['label', 'hour']).size().reset_index(name='count')\n",
    "    df = df.merge(counts, on=['label', 'hour'], how='left')\n",
    "    return df\n",
    "df_departures = count(df_departures)\n",
    "df_arrivals = count(df_arrivals)\n",
    "\n",
    "df_departures_copy = df_departures.copy()\n",
    "df_arrivals_copy = df_arrivals.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check we have included all clusters\n",
    "print(np.sort(df_arrivals.label.unique()))\n",
    "print(np.sort(df_departures.label.unique()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that cluster 12 is the group of red outliers in the map of all end stations, apparently, this cluster only has arrivals and no departures. As a consequence, no bikes have to be moved to stations within this cluster, and the cluster can be ignored for this section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_departures = df_departures[df_departures['label'] != 12]\n",
    "df_arrivals = df_arrivals[df_arrivals['label'] != 12]\n",
    "print(np.sort(df_arrivals.label.unique()))\n",
    "print(np.sort(df_departures.label.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the method presented in the previous section, we will train prediction models to predict the change in arrivals and departures over the course of time. This time, however, models for all the relevant clusters will have to be trained. The method will be similar, however. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include all relevant clusters\n",
    "n = 19\n",
    "\n",
    "# redifine largest cluster\n",
    "largest_cluster = df['pick_label'].value_counts().nlargest(n).index[:n]\n",
    "print(largest_cluster)\n",
    "\n",
    "# split data\n",
    "df_departures_train, df_departures_test = split_train_test(df_departures)\n",
    "df_arrivals_train, df_arrivals_test = split_train_test(df_arrivals)\n",
    "\n",
    "# split data per label for n\n",
    "for i in range(n):\n",
    "    globals()['df_departures_train_{}'.format(i)] = df_departures_train[df_departures_train['label'] == largest_cluster[i]]\n",
    "    globals()['df_departures_test_{}'.format(i)] = df_departures_test[df_departures_test['label'] == largest_cluster[i]]\n",
    "    globals()['df_arrivals_train_{}'.format(i)] = df_arrivals_train[df_arrivals_train['label'] == largest_cluster[i]]\n",
    "    globals()['df_arrivals_test_{}'.format(i)] = df_arrivals_test[df_arrivals_test['label'] == largest_cluster[i]]\n",
    "\n",
    "    # split x and y \n",
    "    globals()['x_departures_train_{}'.format(i)], globals()['y_departures_train_{}'.format(i)] = create_x_y(globals()['df_departures_train_{}'.format(i)])\n",
    "    globals()['x_departures_test_{}'.format(i)], globals()['y_departures_test_{}'.format(i)] = create_x_y(globals()['df_departures_test_{}'.format(i)])\n",
    "    globals()['x_arrivals_train_{}'.format(i)], globals()['y_arrivals_train_{}'.format(i)] = create_x_y(globals()['df_arrivals_train_{}'.format(i)])\n",
    "    globals()['x_arrivals_test_{}'.format(i)], globals()['y_arrivals_test_{}'.format(i)] = create_x_y(globals()['df_arrivals_test_{}'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add polynomial features\n",
    "n_poly = 10\n",
    "for i in range(n):\n",
    "    globals()['x_departures_train_{}_poly'.format(i)] = add_polynomial(globals()['x_departures_train_{}'.format(i)], n_poly)\n",
    "    globals()['x_departures_test_{}_poly'.format(i)] = add_polynomial(globals()['x_departures_test_{}'.format(i)], n_poly)\n",
    "    globals()['x_arrivals_train_{}_poly'.format(i)] = add_polynomial(globals()['x_arrivals_train_{}'.format(i)], n_poly)\n",
    "    globals()['x_arrivals_test_{}_poly'.format(i)] = add_polynomial(globals()['x_arrivals_test_{}'.format(i)], n_poly)\n",
    "\n",
    "# train lin reg model per label with polynomial features\n",
    "for i in range(n):\n",
    "    globals()['reg_departures_{}_poly'.format(i)] = LinearRegression().fit(globals()['x_departures_train_{}_poly'.format(i)], globals()['y_departures_train_{}'.format(i)])\n",
    "    globals()['reg_arrivals_{}_poly'.format(i)] = LinearRegression().fit(globals()['x_arrivals_train_{}_poly'.format(i)], globals()['y_arrivals_train_{}'.format(i)])\n",
    "\n",
    "    # print r^2 score\n",
    "    print('r^2 score for departures label {}: {}'.format(largest_cluster[i], r2_score(globals()['y_departures_test_{}'.format(i)], globals()['reg_departures_{}_poly'.format(i)].predict(globals()['x_departures_test_{}_poly'.format(i)]))))\n",
    "    print('r^2 score for arrivals label {}: {}'.format(largest_cluster[i], r2_score(globals()['y_arrivals_test_{}'.format(i)], globals()['reg_arrivals_{}_poly'.format(i)].predict(globals()['x_arrivals_test_{}_poly'.format(i)]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with new data from january 2019 to variable month, take march 2019 for now, for predicions in the future. \n",
    "df_arrivals_future = df_arrivals_copy.copy()\n",
    "df_departures_future = df_departures_copy.copy()\n",
    "\n",
    "#remove cluster 12 for same reasons as before\n",
    "df_arrivals_future = df_arrivals_future[df_arrivals_future['label'] != 12]\n",
    "df_departures_future = df_departures_future[df_departures_future['label'] != 12]\n",
    "print(np.sort(df_arrivals_future.label.unique()))\n",
    "print(np.sort(df_departures_future.label.unique()))\n",
    "\n",
    "# define amount of months you want to look ahead from jan 2019. \n",
    "month_end = 3\n",
    "df_arrivals_future = df_arrivals_future[df_arrivals_future['month'] <= month_end]\n",
    "df_departures_future = df_departures_future[df_departures_future['month'] <= month_end]\n",
    "\n",
    "#get rid of the counts column for the future data, as we will be predicting this\n",
    "df_arrivals_future.drop(columns=['count'], inplace=True)\n",
    "df_departures_future.drop(columns=['count'], inplace=True)\n",
    "\n",
    "#get rid of month column as it is not needed (similarly as before)\n",
    "df_arrivals_future.drop(columns=['month'], inplace=True)\n",
    "df_departures_future.drop(columns=['month'], inplace=True)\n",
    "\n",
    "# split data per label for n\n",
    "for i in range(n):\n",
    "    globals()['df_departures_future_{}'.format(i)] = df_departures_future[df_departures_future['label'] == largest_cluster[i]]\n",
    "    globals()['df_arrivals_future_{}'.format(i)] = df_arrivals_future[df_arrivals_future['label'] == largest_cluster[i]]\n",
    "\n",
    "#add as many polynomial features as we did for the training data\n",
    "for i in range(n):\n",
    "    globals()['df_departures_future_{}_poly'.format(i)] = add_polynomial(globals()['df_departures_future_{}'.format(i)], n_poly)\n",
    "    globals()['df_arrivals_future_{}_poly'.format(i)] = add_polynomial(globals()['df_arrivals_future_{}'.format(i)], n_poly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict future data\n",
    "for i in range(n):\n",
    "    globals()['df_departures_future_{}_poly'.format(i)]['count'] = globals()['reg_departures_{}_poly'.format(i)].predict(globals()['df_departures_future_{}_poly'.format(i)])\n",
    "    globals()['df_arrivals_future_{}_poly'.format(i)]['count'] = globals()['reg_arrivals_{}_poly'.format(i)].predict(globals()['df_arrivals_future_{}_poly'.format(i)])\n",
    "\n",
    "# compute the difference between the predicted arrivals and the predicted departures for each cluster per hour and store in new dataframe\n",
    "for i in range(n):\n",
    "    globals()['df_arrivals_future_{}_poly'.format(i)]['difference'] = globals()['df_arrivals_future_{}_poly'.format(i)]['count'] - globals()['df_departures_future_{}_poly'.format(i)]['count']\n",
    "\n",
    "# sum the differences per hour for a specific cluster per day\n",
    "for i in range(n):\n",
    "    globals()['df_arrivals_future_{}_poly'.format(i)] = globals()['df_arrivals_future_{}_poly'.format(i)].groupby(['label', 'day']).sum().reset_index()\n",
    "\n",
    "# print per day for a specific cluster the difference between predicted arrivals and predicted departures\n",
    "for i in range(n):\n",
    "    print(globals()['df_arrivals_future_{}_poly'.format(i)].head())\n",
    "    \n",
    "\n",
    "# sum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Component "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) What is your research question?\n",
    "    - Can the addition of NY daily weather data in combination with an increase in prediction accuracy?\n",
    "    - Is there a trend in the weather data that can be used to improve the prediction accuracy?\n",
    "    - Can the addition of weather data improve the prediction accuracy for the clusters with the least amount of data?\n",
    "\n",
    "<br>\n",
    "\n",
    "2) Which data are you planning to use and from where are you planning to get it?\n",
    "    - We will use both the trips_2018.csv file provided by the course instructors and a self-gathered dataset containing the 2018 daily weather data in NYC, with the assumption that the weather at Central Park is representative of the weather in the entire spatial scope of the bike stations included in the dataset. A link to the data can be found [here](https://www.weather.gov/wrh/climate?wfo=okx)\n",
    "\n",
    "<br>\n",
    "\n",
    "3) Which methods are you planning to use?\n",
    "    - We will attempt to compare different models for regression and evaluate their performance. The models we will use are: Linear Regression, Ridge Regression, Lasso Regression, Random Forest Regression, and Neural Networks.\n",
    "\n",
    "<br>\n",
    "\n",
    "4) Which benchmark are you going to use? \n",
    "    - The benchmark will be the performance of the prediction model introduced in Part 2 of the mandatory assignment. \n",
    "    \n",
    "<br>\n",
    "\n",
    "5) Which metrics and methods are you planning for validation? \n",
    "    - The R^2 value of the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear all variables for memory overflow reasons\n",
    "%reset -f\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def count(df):\n",
    "    counts = df.groupby(['label', 'hour', 'day', 'month']).size().reset_index(name='count')\n",
    "    df = df.merge(counts, on=['label', 'hour', 'day', 'month'], how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "#open weather data 2018, read textfile and convert to dataframe\n",
    "text = open('data/2018_weather_data.txt', 'r')\n",
    "weather_data = text.read()\n",
    "weather_data = weather_data.split('\\n')\n",
    "weather_data = [i.split('\\t') for i in weather_data[1:]]\n",
    "weather_data = pd.DataFrame(weather_data)\n",
    "weather_data.columns = ['Date', 'Max_temp', 'Min_temp', 'Avg_temp', 'Departure_temp', 'HDD', 'CDD', 'Precipitation', 'Snowfall', 'Snow_depth']\n",
    "weather_data = weather_data.dropna()\n",
    "# day month\n",
    "weather_data['Date'] = pd.to_datetime(weather_data['Date'])\n",
    "weather_data['day'] = weather_data['Date'].dt.day\n",
    "weather_data['month'] = weather_data['Date'].dt.month\n",
    "weather_data['weekend'] = np.where(weather_data['Date'].dt.dayofweek < 5, 0, 1)\n",
    "weather_data['day_of_week'] = weather_data['Date'].dt.dayofweek\n",
    "weather_data['year'] = weather_data['Date'].dt.year\n",
    "weather_data['day_of_year'] = weather_data['Date'].dt.dayofyear\n",
    "weather_data.drop(['Date'], axis=1, inplace=True)\n",
    "\n",
    "weather_data.replace('T', 0, inplace=True)\n",
    "weather_data.replace('M', 0, inplace=True)\n",
    "\n",
    "# convert temperatures to celcius\n",
    "weather_data['Max_temp'] = (weather_data['Max_temp'].astype(float) - 32) * 5/9\n",
    "weather_data['Min_temp'] = (weather_data['Min_temp'].astype(float) - 32) * 5/9\n",
    "weather_data['Avg_temp'] = (weather_data['Avg_temp'].astype(float) - 32) * 5/9\n",
    "weather_data['Departure_temp'] = (weather_data['Departure_temp'].astype(float) - 32) * 5/9\n",
    "\n",
    "\n",
    "# # check for nan\n",
    "# print(weather_data.isnull().sum())\n",
    "\n",
    "# print(weather_data.head())\n",
    "\n",
    "df = pd.read_csv('data/Trips_2018.csv')\n",
    "df.rename(columns={'Unnamed: 0': 'trip_id'}, inplace=True)\n",
    "df.set_index('trip_id', inplace=True)\n",
    "df = df.dropna()\n",
    "df['starttime'] = pd.to_datetime(df['starttime'], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "df['stoptime'] = pd.to_datetime(df['stoptime'], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "df = df[~np.isnan(df['start_station_id'])]\n",
    "df = df[~np.isnan(df['end_station_id'])]\n",
    "# get rid of Canada outlier\n",
    "df = df[df['start_station_longitude'] < -73.6]\n",
    "df = df[df['end_station_longitude'] < -73.6]\n",
    "df = pd.get_dummies(df, columns=['usertype'], dtype=int, drop_first=True)\n",
    "\n",
    "# print(df.columns)\n",
    "# print(df.head())\n",
    "\n",
    "df_arrival = df.drop(['start_station_latitude', 'start_station_longitude', 'start_station_id', 'starttime'], axis=1)\n",
    "df_departure = df.drop(['end_station_latitude', 'end_station_longitude', 'end_station_id', 'stoptime'], axis=1)\n",
    "\n",
    "# make lat and long called that and time\n",
    "df_arrival.rename(columns={'end_station_latitude': 'latitude', 'end_station_longitude': 'longitude'}, inplace=True)\n",
    "df_departure.rename(columns={'start_station_latitude': 'latitude', 'start_station_longitude': 'longitude'}, inplace=True)\n",
    "df_arrival.rename(columns={'stoptime': 'time'}, inplace=True)\n",
    "df_departure.rename(columns={'starttime': 'time'}, inplace=True)\n",
    "\n",
    "print(df_arrival.columns)\n",
    "print(df_departure.columns)\n",
    "\n",
    "# add hour, day, month, weekend, day of week\n",
    "df_arrival['hour'] = df_arrival['time'].dt.hour\n",
    "df_departure['hour'] = df_departure['time'].dt.hour\n",
    "df_arrival['day'] = df_arrival['time'].dt.day\n",
    "df_departure['day'] = df_departure['time'].dt.day\n",
    "df_arrival['month'] = df_arrival['time'].dt.month\n",
    "df_departure['month'] = df_departure['time'].dt.month\n",
    "df_arrival['weekend'] = np.where(df_arrival['time'].dt.dayofweek < 5, 0, 1)\n",
    "df_departure['weekend'] = np.where(df_departure['time'].dt.dayofweek < 5, 0, 1)\n",
    "df_arrival['day_of_week'] = df_arrival['time'].dt.dayofweek\n",
    "df_departure['day_of_week'] = df_departure['time'].dt.dayofweek\n",
    "df_arrival['day_of_year'] = df_arrival['time'].dt.dayofyear\n",
    "df_departure['day_of_year'] = df_departure['time'].dt.dayofyear\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=20, random_state=0, n_init='auto').fit(df_arrival[['latitude', 'longitude']])\n",
    "\n",
    "# add labels using k means predict\n",
    "df_arrival['label'] = kmeans.predict(df_arrival[['latitude', 'longitude']])\n",
    "df_departure['label'] = kmeans.predict(df_departure[['latitude', 'longitude']])\n",
    "\n",
    "# count\n",
    "df_arrival = count(df_arrival)\n",
    "df_departure = count(df_departure)\n",
    "\n",
    "print(df_arrival.columns)\n",
    "print(df_departure.columns)\n",
    "\n",
    "# drop trip id, lat, long, \n",
    "df_arrival.drop(['tripduration', 'latitude', 'longitude', 'end_station_id', 'bikeid', 'birth_year', 'gender', 'usertype_Subscriber'], axis=1, inplace=True)\n",
    "df_departure.drop(['tripduration', 'latitude', 'longitude', 'start_station_id', 'bikeid', 'birth_year', 'gender', 'usertype_Subscriber'], axis=1, inplace=True)\n",
    "\n",
    "print(df_arrival.columns)\n",
    "print(df_departure.columns)\n",
    "\n",
    "# drop nans\n",
    "df_arrival = df_arrival.dropna()\n",
    "df_departure = df_departure.dropna()\n",
    "weather_data = weather_data.dropna()\n",
    "\n",
    "# change weather data float\n",
    "weather_data = weather_data.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add visualisation from exploratiory data processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tripduration</th>\n",
       "      <th>time</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>bikeid</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>gender</th>\n",
       "      <th>usertype_Subscriber</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>weekend</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>970</td>\n",
       "      <td>2018-01-01 13:50:57.434</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>31956</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>723</td>\n",
       "      <td>2018-01-01 15:33:30.182</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>32536</td>\n",
       "      <td>1969</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>496</td>\n",
       "      <td>2018-01-01 15:39:18.337</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>16069</td>\n",
       "      <td>1956</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>306</td>\n",
       "      <td>2018-01-01 15:40:13.372</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>31781</td>\n",
       "      <td>1974</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>2018-01-01 18:14:51.568</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>30319</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tripduration                    time  start_station_id   latitude   \n",
       "0           970 2018-01-01 13:50:57.434              72.0  40.767272  \\\n",
       "1           723 2018-01-01 15:33:30.182              72.0  40.767272   \n",
       "2           496 2018-01-01 15:39:18.337              72.0  40.767272   \n",
       "3           306 2018-01-01 15:40:13.372              72.0  40.767272   \n",
       "4           306 2018-01-01 18:14:51.568              72.0  40.767272   \n",
       "\n",
       "   longitude  bikeid  birth_year  gender  usertype_Subscriber  hour  day   \n",
       "0 -73.993929   31956        1992       1                    1    13    1  \\\n",
       "1 -73.993929   32536        1969       1                    1    15    1   \n",
       "2 -73.993929   16069        1956       1                    1    15    1   \n",
       "3 -73.993929   31781        1974       1                    1    15    1   \n",
       "4 -73.993929   30319        1992       1                    1    18    1   \n",
       "\n",
       "   month  weekend  day_of_week  day_of_year  label  count  \n",
       "0      1        0            0            1      6     29  \n",
       "1      1        0            0            1      6     36  \n",
       "2      1        0            0            1      6     36  \n",
       "3      1        0            0            1      6     36  \n",
       "4      1        0            0            1      6     31  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only keep largest label data\n",
    "df_arrival_largest = df_arrival[df_arrival['label'] == df_arrival['label'].value_counts().nlargest(1).index[0]]\n",
    "df_arrival_largest.head()\n",
    "df_departure_largest = df_departure[df_departure['label'] == df_departure['label'].value_counts().nlargest(1).index[0]]\n",
    "df_departure_largest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexi\\AppData\\Local\\Temp\\ipykernel_21368\\1027465442.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_arrival_largest.drop(['label'], axis=1, inplace=True)\n",
      "C:\\Users\\alexi\\AppData\\Local\\Temp\\ipykernel_21368\\1027465442.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_departure_largest.drop(['label'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# drop label\n",
    "df_arrival_largest.drop(['label'], axis=1, inplace=True)\n",
    "df_departure_largest.drop(['label'], axis=1, inplace=True)\n",
    "\n",
    "weather_data.drop(['year'], axis=1, inplace=True)\n",
    "# merge with weather data\n",
    "df_arrival_largest = df_arrival_largest.merge(weather_data, on=['day', 'month', 'weekend', 'day_of_week', 'day_of_year'], how='left')\n",
    "df_departure_largest = df_departure_largest.merge(weather_data, on=['day', 'month', 'weekend', 'day_of_week', 'day_of_year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tripduration</th>\n",
       "      <th>time</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>bikeid</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>gender</th>\n",
       "      <th>usertype_Subscriber</th>\n",
       "      <th>hour</th>\n",
       "      <th>...</th>\n",
       "      <th>count</th>\n",
       "      <th>Max_temp</th>\n",
       "      <th>Min_temp</th>\n",
       "      <th>Avg_temp</th>\n",
       "      <th>Departure_temp</th>\n",
       "      <th>HDD</th>\n",
       "      <th>CDD</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>Snowfall</th>\n",
       "      <th>Snow_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>723</td>\n",
       "      <td>2018-01-01 15:45:33.341</td>\n",
       "      <td>3255.0</td>\n",
       "      <td>40.750585</td>\n",
       "      <td>-73.994685</td>\n",
       "      <td>32536</td>\n",
       "      <td>1969</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>-7.222222</td>\n",
       "      <td>-13.888889</td>\n",
       "      <td>-10.555556</td>\n",
       "      <td>-30.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>496</td>\n",
       "      <td>2018-01-01 15:47:35.172</td>\n",
       "      <td>525.0</td>\n",
       "      <td>40.755942</td>\n",
       "      <td>-74.002116</td>\n",
       "      <td>16069</td>\n",
       "      <td>1956</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>-7.222222</td>\n",
       "      <td>-13.888889</td>\n",
       "      <td>-10.555556</td>\n",
       "      <td>-30.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>306</td>\n",
       "      <td>2018-01-01 15:45:20.191</td>\n",
       "      <td>447.0</td>\n",
       "      <td>40.763707</td>\n",
       "      <td>-73.985162</td>\n",
       "      <td>31781</td>\n",
       "      <td>1974</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>-7.222222</td>\n",
       "      <td>-13.888889</td>\n",
       "      <td>-10.555556</td>\n",
       "      <td>-30.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>434</td>\n",
       "      <td>2018-01-02 08:13:28.767</td>\n",
       "      <td>173.0</td>\n",
       "      <td>40.760683</td>\n",
       "      <td>-73.984527</td>\n",
       "      <td>30525</td>\n",
       "      <td>1983</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>211</td>\n",
       "      <td>-3.333333</td>\n",
       "      <td>-10.555556</td>\n",
       "      <td>-6.944444</td>\n",
       "      <td>-26.388889</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>366</td>\n",
       "      <td>2018-01-02 08:16:21.896</td>\n",
       "      <td>479.0</td>\n",
       "      <td>40.760193</td>\n",
       "      <td>-73.991255</td>\n",
       "      <td>27439</td>\n",
       "      <td>1974</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>211</td>\n",
       "      <td>-3.333333</td>\n",
       "      <td>-10.555556</td>\n",
       "      <td>-6.944444</td>\n",
       "      <td>-26.388889</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tripduration                    time  end_station_id   latitude  longitude   \n",
       "0           723 2018-01-01 15:45:33.341          3255.0  40.750585 -73.994685  \\\n",
       "1           496 2018-01-01 15:47:35.172           525.0  40.755942 -74.002116   \n",
       "2           306 2018-01-01 15:45:20.191           447.0  40.763707 -73.985162   \n",
       "3           434 2018-01-02 08:13:28.767           173.0  40.760683 -73.984527   \n",
       "4           366 2018-01-02 08:16:21.896           479.0  40.760193 -73.991255   \n",
       "\n",
       "   bikeid  birth_year  gender  usertype_Subscriber  hour  ...  count   \n",
       "0   32536        1969       1                    1    15  ...     45  \\\n",
       "1   16069        1956       1                    1    15  ...     45   \n",
       "2   31781        1974       1                    1    15  ...     45   \n",
       "3   30525        1983       1                    1     8  ...    211   \n",
       "4   27439        1974       1                    1     8  ...    211   \n",
       "\n",
       "   Max_temp   Min_temp   Avg_temp  Departure_temp   HDD  CDD  Precipitation   \n",
       "0 -7.222222 -13.888889 -10.555556      -30.111111  52.0  0.0            0.0  \\\n",
       "1 -7.222222 -13.888889 -10.555556      -30.111111  52.0  0.0            0.0   \n",
       "2 -7.222222 -13.888889 -10.555556      -30.111111  52.0  0.0            0.0   \n",
       "3 -3.333333 -10.555556  -6.944444      -26.388889  45.0  0.0            0.0   \n",
       "4 -3.333333 -10.555556  -6.944444      -26.388889  45.0  0.0            0.0   \n",
       "\n",
       "   Snowfall  Snow_depth  \n",
       "0       0.0         0.0  \n",
       "1       0.0         0.0  \n",
       "2       0.0         0.0  \n",
       "3       0.0         0.0  \n",
       "4       0.0         0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_arrival_largest.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tripduration</th>\n",
       "      <th>time</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>bikeid</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>gender</th>\n",
       "      <th>usertype_Subscriber</th>\n",
       "      <th>hour</th>\n",
       "      <th>...</th>\n",
       "      <th>count</th>\n",
       "      <th>Max_temp</th>\n",
       "      <th>Min_temp</th>\n",
       "      <th>Avg_temp</th>\n",
       "      <th>Departure_temp</th>\n",
       "      <th>HDD</th>\n",
       "      <th>CDD</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>Snowfall</th>\n",
       "      <th>Snow_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>970</td>\n",
       "      <td>2018-01-01 13:50:57.434</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>31956</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>-7.222222</td>\n",
       "      <td>-13.888889</td>\n",
       "      <td>-10.555556</td>\n",
       "      <td>-30.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>723</td>\n",
       "      <td>2018-01-01 15:33:30.182</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>32536</td>\n",
       "      <td>1969</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>-7.222222</td>\n",
       "      <td>-13.888889</td>\n",
       "      <td>-10.555556</td>\n",
       "      <td>-30.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>496</td>\n",
       "      <td>2018-01-01 15:39:18.337</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>16069</td>\n",
       "      <td>1956</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>-7.222222</td>\n",
       "      <td>-13.888889</td>\n",
       "      <td>-10.555556</td>\n",
       "      <td>-30.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>306</td>\n",
       "      <td>2018-01-01 15:40:13.372</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>31781</td>\n",
       "      <td>1974</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>-7.222222</td>\n",
       "      <td>-13.888889</td>\n",
       "      <td>-10.555556</td>\n",
       "      <td>-30.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>2018-01-01 18:14:51.568</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>30319</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>-7.222222</td>\n",
       "      <td>-13.888889</td>\n",
       "      <td>-10.555556</td>\n",
       "      <td>-30.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tripduration                    time  start_station_id   latitude   \n",
       "0           970 2018-01-01 13:50:57.434              72.0  40.767272  \\\n",
       "1           723 2018-01-01 15:33:30.182              72.0  40.767272   \n",
       "2           496 2018-01-01 15:39:18.337              72.0  40.767272   \n",
       "3           306 2018-01-01 15:40:13.372              72.0  40.767272   \n",
       "4           306 2018-01-01 18:14:51.568              72.0  40.767272   \n",
       "\n",
       "   longitude  bikeid  birth_year  gender  usertype_Subscriber  hour  ...   \n",
       "0 -73.993929   31956        1992       1                    1    13  ...  \\\n",
       "1 -73.993929   32536        1969       1                    1    15  ...   \n",
       "2 -73.993929   16069        1956       1                    1    15  ...   \n",
       "3 -73.993929   31781        1974       1                    1    15  ...   \n",
       "4 -73.993929   30319        1992       1                    1    18  ...   \n",
       "\n",
       "   count  Max_temp   Min_temp   Avg_temp  Departure_temp   HDD  CDD   \n",
       "0     29 -7.222222 -13.888889 -10.555556      -30.111111  52.0  0.0  \\\n",
       "1     36 -7.222222 -13.888889 -10.555556      -30.111111  52.0  0.0   \n",
       "2     36 -7.222222 -13.888889 -10.555556      -30.111111  52.0  0.0   \n",
       "3     36 -7.222222 -13.888889 -10.555556      -30.111111  52.0  0.0   \n",
       "4     31 -7.222222 -13.888889 -10.555556      -30.111111  52.0  0.0   \n",
       "\n",
       "   Precipitation  Snowfall  Snow_depth  \n",
       "0            0.0       0.0         0.0  \n",
       "1            0.0       0.0         0.0  \n",
       "2            0.0       0.0         0.0  \n",
       "3            0.0       0.0         0.0  \n",
       "4            0.0       0.0         0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_departure_largest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop time\n",
    "df_arrival_largest.drop(['time'], axis=1, inplace=True)\n",
    "df_departure_largest.drop(['time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test month >= 10 train\n",
    "df_arrival_largest_train = df_arrival_largest[df_arrival_largest['month'] < 10]\n",
    "df_arrival_largest_test = df_arrival_largest[df_arrival_largest['month'] >= 10]\n",
    "df_departure_largest_train = df_departure_largest[df_departure_largest['month'] < 10]\n",
    "df_departure_largest_test = df_departure_largest[df_departure_largest['month'] >= 10]\n",
    "\n",
    "# get rid of row nan\n",
    "df_arrival_largest_train = df_arrival_largest_train.dropna()\n",
    "df_arrival_largest_test = df_arrival_largest_test.dropna()\n",
    "df_departure_largest_train = df_departure_largest_train.dropna()\n",
    "df_departure_largest_test = df_departure_largest_test.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x y split\n",
    "def create_x_y(df):\n",
    "    x = df.drop(['count'], axis=1)\n",
    "    y = df['count']\n",
    "    return x, y\n",
    "\n",
    "x_arrivals_train, y_arrivals_train = create_x_y(df_arrival_largest_train)\n",
    "x_arrivals_test, y_arrivals_test = create_x_y(df_arrival_largest_test)\n",
    "x_departures_train, y_departures_train = create_x_y(df_departure_largest_train)\n",
    "x_departures_test, y_departures_test = create_x_y(df_departure_largest_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tripduration             int64\n",
      "end_station_id         float64\n",
      "latitude               float64\n",
      "longitude              float64\n",
      "bikeid                   int64\n",
      "birth_year               int64\n",
      "gender                   int64\n",
      "usertype_Subscriber      int32\n",
      "hour                     int32\n",
      "day                      int32\n",
      "month                    int32\n",
      "weekend                  int32\n",
      "day_of_week              int32\n",
      "day_of_year              int32\n",
      "Max_temp               float64\n",
      "Min_temp               float64\n",
      "Avg_temp               float64\n",
      "Departure_temp         float64\n",
      "HDD                    float64\n",
      "CDD                    float64\n",
      "Precipitation          float64\n",
      "Snowfall               float64\n",
      "Snow_depth             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print column data types\n",
    "print(x_arrivals_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep unique\n",
    "x_arrivals_train = x_arrivals_train.drop_duplicates()\n",
    "x_arrivals_test = x_arrivals_test.drop_duplicates()\n",
    "x_departures_train = x_departures_train.drop_duplicates()\n",
    "x_departures_test = x_departures_test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 11.4 TiB for an array with shape (1249980, 1249980) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexi\\OneDrive - Delft University of Technology\\Minor Abroad\\Intro to Business Analytics\\IntroBA\\reportv1.ipynb Cell 134\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexi/OneDrive%20-%20Delft%20University%20of%20Technology/Minor%20Abroad/Intro%20to%20Business%20Analytics/IntroBA/reportv1.ipynb#Z1655sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     x_test_std \u001b[39m=\u001b[39m (x_test \u001b[39m-\u001b[39m x_test\u001b[39m.\u001b[39mmean(axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)) \u001b[39m/\u001b[39m x_test\u001b[39m.\u001b[39mstd(axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexi/OneDrive%20-%20Delft%20University%20of%20Technology/Minor%20Abroad/Intro%20to%20Business%20Analytics/IntroBA/reportv1.ipynb#Z1655sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x_train_std, x_test_std\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alexi/OneDrive%20-%20Delft%20University%20of%20Technology/Minor%20Abroad/Intro%20to%20Business%20Analytics/IntroBA/reportv1.ipynb#Z1655sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m x_arrivals_train_std, x_arrivals_test_std \u001b[39m=\u001b[39m standardise(x_arrivals_train, x_arrivals_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexi/OneDrive%20-%20Delft%20University%20of%20Technology/Minor%20Abroad/Intro%20to%20Business%20Analytics/IntroBA/reportv1.ipynb#Z1655sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m x_departures_train_std, x_departures_test_std \u001b[39m=\u001b[39m standardise(x_departures_train, x_departures_test)\n",
      "\u001b[1;32mc:\\Users\\alexi\\OneDrive - Delft University of Technology\\Minor Abroad\\Intro to Business Analytics\\IntroBA\\reportv1.ipynb Cell 134\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexi/OneDrive%20-%20Delft%20University%20of%20Technology/Minor%20Abroad/Intro%20to%20Business%20Analytics/IntroBA/reportv1.ipynb#Z1655sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstandardise\u001b[39m(x_train, x_test):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alexi/OneDrive%20-%20Delft%20University%20of%20Technology/Minor%20Abroad/Intro%20to%20Business%20Analytics/IntroBA/reportv1.ipynb#Z1655sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     x_train_std \u001b[39m=\u001b[39m (x_train \u001b[39m-\u001b[39;49m x_train\u001b[39m.\u001b[39;49mmean(axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)) \u001b[39m/\u001b[39m x_train\u001b[39m.\u001b[39mstd(axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexi/OneDrive%20-%20Delft%20University%20of%20Technology/Minor%20Abroad/Intro%20to%20Business%20Analytics/IntroBA/reportv1.ipynb#Z1655sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     x_test_std \u001b[39m=\u001b[39m (x_test \u001b[39m-\u001b[39m x_test\u001b[39m.\u001b[39mmean(axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)) \u001b[39m/\u001b[39m x_test\u001b[39m.\u001b[39mstd(axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexi/OneDrive%20-%20Delft%20University%20of%20Technology/Minor%20Abroad/Intro%20to%20Business%20Analytics/IntroBA/reportv1.ipynb#Z1655sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x_train_std, x_test_std\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\ops\\common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m     79\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arraylike.py:194\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__sub__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__sub__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m--> 194\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_arith_method(other, operator\u001b[39m.\u001b[39;49msub)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:7465\u001b[0m, in \u001b[0;36mDataFrame._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   7462\u001b[0m axis: Literal[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# only relevant for Series other case\u001b[39;00m\n\u001b[0;32m   7463\u001b[0m other \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mmaybe_prepare_scalar_for_op(other, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[axis],))\n\u001b[1;32m-> 7465\u001b[0m \u001b[39mself\u001b[39m, other \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49malign_method_FRAME(\u001b[39mself\u001b[39;49m, other, axis, flex\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, level\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m   7467\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dispatch_frame_op(other, op, axis\u001b[39m=\u001b[39maxis)\n\u001b[0;32m   7468\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(new_data)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\ops\\__init__.py:329\u001b[0m, in \u001b[0;36malign_method_FRAME\u001b[1;34m(left, right, axis, flex, level)\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m left\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mequals(right\u001b[39m.\u001b[39mindex):\n\u001b[0;32m    323\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    324\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mOperands are not aligned. Do \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    325\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m`left, right = left.align(right, axis=1, copy=False)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    326\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mbefore operating.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    327\u001b[0m             )\n\u001b[1;32m--> 329\u001b[0m     left, right \u001b[39m=\u001b[39m left\u001b[39m.\u001b[39;49malign(\n\u001b[0;32m    330\u001b[0m         right, join\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mouter\u001b[39;49m\u001b[39m\"\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, level\u001b[39m=\u001b[39;49mlevel, copy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    331\u001b[0m     )\n\u001b[0;32m    332\u001b[0m     right \u001b[39m=\u001b[39m _maybe_align_series_as_frame(left, right, axis)\n\u001b[0;32m    334\u001b[0m \u001b[39mreturn\u001b[39;00m left, right\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:4994\u001b[0m, in \u001b[0;36mDataFrame.align\u001b[1;34m(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)\u001b[0m\n\u001b[0;32m   4980\u001b[0m \u001b[39m@doc\u001b[39m(NDFrame\u001b[39m.\u001b[39malign, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_shared_doc_kwargs)\n\u001b[0;32m   4981\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39malign\u001b[39m(\n\u001b[0;32m   4982\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4992\u001b[0m     broadcast_axis: Axis \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   4993\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m-> 4994\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49malign(\n\u001b[0;32m   4995\u001b[0m         other,\n\u001b[0;32m   4996\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m   4997\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   4998\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   4999\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   5000\u001b[0m         fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[0;32m   5001\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m   5002\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit,\n\u001b[0;32m   5003\u001b[0m         fill_axis\u001b[39m=\u001b[39;49mfill_axis,\n\u001b[0;32m   5004\u001b[0m         broadcast_axis\u001b[39m=\u001b[39;49mbroadcast_axis,\n\u001b[0;32m   5005\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:9449\u001b[0m, in \u001b[0;36mNDFrame.align\u001b[1;34m(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)\u001b[0m\n\u001b[0;32m   9437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_align_frame(\n\u001b[0;32m   9438\u001b[0m         other,\n\u001b[0;32m   9439\u001b[0m         join\u001b[39m=\u001b[39mjoin,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9446\u001b[0m         fill_axis\u001b[39m=\u001b[39mfill_axis,\n\u001b[0;32m   9447\u001b[0m     )\n\u001b[0;32m   9448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, ABCSeries):\n\u001b[1;32m-> 9449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_align_series(\n\u001b[0;32m   9450\u001b[0m         other,\n\u001b[0;32m   9451\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m   9452\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   9453\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   9454\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   9455\u001b[0m         fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[0;32m   9456\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m   9457\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit,\n\u001b[0;32m   9458\u001b[0m         fill_axis\u001b[39m=\u001b[39;49mfill_axis,\n\u001b[0;32m   9459\u001b[0m     )\n\u001b[0;32m   9460\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   9461\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsupported type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(other)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:9583\u001b[0m, in \u001b[0;36mNDFrame._align_series\u001b[1;34m(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis)\u001b[0m\n\u001b[0;32m   9581\u001b[0m \u001b[39mif\u001b[39;00m lidx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   9582\u001b[0m     bm_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_block_manager_axis(\u001b[39m1\u001b[39m)\n\u001b[1;32m-> 9583\u001b[0m     fdata \u001b[39m=\u001b[39m fdata\u001b[39m.\u001b[39;49mreindex_indexer(join_index, lidx, axis\u001b[39m=\u001b[39;49mbm_axis)\n\u001b[0;32m   9585\u001b[0m \u001b[39mif\u001b[39;00m copy \u001b[39mand\u001b[39;00m fdata \u001b[39mis\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr:\n\u001b[0;32m   9586\u001b[0m     fdata \u001b[39m=\u001b[39m fdata\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:740\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    737\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mRequested axis not found in manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    739\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 740\u001b[0m     new_blocks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slice_take_blocks_ax0(\n\u001b[0;32m    741\u001b[0m         indexer,\n\u001b[0;32m    742\u001b[0m         fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[0;32m    743\u001b[0m         only_slice\u001b[39m=\u001b[39;49monly_slice,\n\u001b[0;32m    744\u001b[0m         use_na_proxy\u001b[39m=\u001b[39;49muse_na_proxy,\n\u001b[0;32m    745\u001b[0m     )\n\u001b[0;32m    746\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    747\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[0;32m    748\u001b[0m         blk\u001b[39m.\u001b[39mtake_nd(\n\u001b[0;32m    749\u001b[0m             indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\n\u001b[0;32m    756\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:856\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39mfor\u001b[39;00m blkno, mgr_locs \u001b[39min\u001b[39;00m libinternals\u001b[39m.\u001b[39mget_blkno_placements(blknos, group\u001b[39m=\u001b[39mgroup):\n\u001b[0;32m    852\u001b[0m     \u001b[39mif\u001b[39;00m blkno \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m    853\u001b[0m         \u001b[39m# If we've got here, fill_value was not lib.no_default\u001b[39;00m\n\u001b[0;32m    855\u001b[0m         blocks\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 856\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_na_block(\n\u001b[0;32m    857\u001b[0m                 placement\u001b[39m=\u001b[39;49mmgr_locs,\n\u001b[0;32m    858\u001b[0m                 fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[0;32m    859\u001b[0m                 use_na_proxy\u001b[39m=\u001b[39;49muse_na_proxy,\n\u001b[0;32m    860\u001b[0m             )\n\u001b[0;32m    861\u001b[0m         )\n\u001b[0;32m    862\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    863\u001b[0m         blk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks[blkno]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:925\u001b[0m, in \u001b[0;36mBaseBlockManager._make_na_block\u001b[1;34m(self, placement, fill_value, use_na_proxy)\u001b[0m\n\u001b[0;32m    920\u001b[0m dtype, fill_value \u001b[39m=\u001b[39m infer_dtype_from_scalar(fill_value)\n\u001b[0;32m    921\u001b[0m \u001b[39m# error: Argument \"dtype\" to \"empty\" has incompatible type \"Union[dtype,\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[39m# ExtensionDtype]\"; expected \"Union[dtype, None, type, _SupportsDtype, str,\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[39m# Tuple[Any, int], Tuple[Any, Union[int, Sequence[int]]], List[Any], _DtypeDict,\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[39m# Tuple[Any, Any]]\"\u001b[39;00m\n\u001b[1;32m--> 925\u001b[0m block_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mempty(block_shape, dtype\u001b[39m=\u001b[39;49mdtype)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    926\u001b[0m block_values\u001b[39m.\u001b[39mfill(fill_value)\n\u001b[0;32m    927\u001b[0m \u001b[39mreturn\u001b[39;00m new_block_2d(block_values, placement\u001b[39m=\u001b[39mplacement)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 11.4 TiB for an array with shape (1249980, 1249980) and data type float64"
     ]
    }
   ],
   "source": [
    "# standardise and keep df\n",
    "def standardise(x_train, x_test):\n",
    "    x_train_std = (x_train - x_train.mean(axis = 1)) / x_train.std(axis = 1)\n",
    "    x_test_std = (x_test - x_test.mean(axis = 1)) / x_test.std(axis = 1)\n",
    "    return x_train_std, x_test_std\n",
    "\n",
    "x_arrivals_train_std, x_arrivals_test_std = standardise(x_arrivals_train, x_arrivals_test)\n",
    "x_departures_train_std, x_departures_test_std = standardise(x_departures_train, x_departures_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ones\n",
    "x_arrivals_train_std = np.hstack((np.ones((x_arrivals_train_std.shape[0], 1)), x_arrivals_train_std))\n",
    "x_arrivals_test_std = np.hstack((np.ones((x_arrivals_test_std.shape[0], 1)), x_arrivals_test_std))\n",
    "x_departures_train_std = np.hstack((np.ones((x_departures_train_std.shape[0], 1)), x_departures_train_std))\n",
    "x_departures_test_std = np.hstack((np.ones((x_departures_test_std.shape[0], 1)), x_departures_test_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_arrivals_train_std' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexi\\OneDrive - Delft University of Technology\\Minor Abroad\\Intro to Business Analytics\\IntroBA\\reportv1.ipynb Cell 135\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexi/OneDrive%20-%20Delft%20University%20of%20Technology/Minor%20Abroad/Intro%20to%20Business%20Analytics/IntroBA/reportv1.ipynb#Z1663sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# find nan\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alexi/OneDrive%20-%20Delft%20University%20of%20Technology/Minor%20Abroad/Intro%20to%20Business%20Analytics/IntroBA/reportv1.ipynb#Z1663sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39margwhere(np\u001b[39m.\u001b[39misnan(x_arrivals_train_std)))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexi/OneDrive%20-%20Delft%20University%20of%20Technology/Minor%20Abroad/Intro%20to%20Business%20Analytics/IntroBA/reportv1.ipynb#Z1663sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39margwhere(np\u001b[39m.\u001b[39misnan(x_arrivals_test_std)))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexi/OneDrive%20-%20Delft%20University%20of%20Technology/Minor%20Abroad/Intro%20to%20Business%20Analytics/IntroBA/reportv1.ipynb#Z1663sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39margwhere(np\u001b[39m.\u001b[39misnan(x_departures_train_std)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_arrivals_train_std' is not defined"
     ]
    }
   ],
   "source": [
    "# find nan\n",
    "print(np.argwhere(np.isnan(x_arrivals_train_std)))\n",
    "print(np.argwhere(np.isnan(x_arrivals_test_std)))\n",
    "print(np.argwhere(np.isnan(x_departures_train_std)))\n",
    "print(np.argwhere(np.isnan(x_departures_test_std)))\n",
    "\n",
    "# find nan in y\n",
    "print(np.argwhere(np.isnan(y_arrivals_train)))\n",
    "print(np.argwhere(np.isnan(y_arrivals_test)))\n",
    "print(np.argwhere(np.isnan(y_departures_train)))\n",
    "print(np.argwhere(np.isnan(y_departures_test)))\n",
    "\n",
    "# print data types\n",
    "print(x_arrivals_train_std.dtype)\n",
    "print(x_arrivals_test_std.dtype)\n",
    "print(x_departures_train_std.dtype)\n",
    "print(x_departures_test_std.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge model\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_arrivals = Ridge(alpha=0.1)\n",
    "ridge_departures = Ridge(alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/fit model\n",
    "ridge_arrivals.fit(x_arrivals_train_std, y_arrivals_train)\n",
    "ridge_departures.fit(x_departures_train_std, y_departures_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r^2 and visualise predictions\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('r^2 score for arrivals: {}'.format(r2_score(y_arrivals_test, ridge_arrivals.predict(x_arrivals_test_std))))\n",
    "print('r^2 score for departures: {}'.format(r2_score(y_departures_test, ridge_departures.predict(x_departures_test_std))))\n",
    "\n",
    "# plot predictions vs actual\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_arrivals_test, ridge_arrivals.predict(x_arrivals_test_std))\n",
    "plt.title('Predictions vs actual arrivals')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_departures_test, ridge_departures.predict(x_departures_test_std))\n",
    "plt.title('Predictions vs actual departures')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column with the hour of the day\n",
    "df_departures['hour'] = df_departures['Time'].apply(lambda x: x.hour)\n",
    "df_arrivals['hour'] = df_arrivals['Time'].apply(lambda x: x.hour)\n",
    "\n",
    "#create a new column with the day since the start of the year\n",
    "df_departures['day'] = df_departures['Date'].apply(lambda x: x.dayofyear)\n",
    "df_arrivals['day'] = df_arrivals['Date'].apply(lambda x: x.dayofyear)\n",
    "\n",
    "#drop time columns\n",
    "df_departures.drop(columns=['Time'], inplace=True)\n",
    "df_arrivals.drop(columns=['Time'], inplace=True)\n",
    "\n",
    "#get rid of index as it has no real meaning. \n",
    "df_departures.reset_index(drop=True, inplace=True)\n",
    "df_arrivals.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_departures.head()\n",
    "print(df_departures.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group data per hour and sum the counts, add new column with the sum\n",
    "counts = df_departures.groupby(['hour', 'day', 'pick_label']).size().reset_index(name='count')\n",
    "df_departures = df_departures.merge(counts, on=['hour', 'day', 'pick_label'], how='left')\n",
    "counts = df_arrivals.groupby(['hour', 'day', 'drop_label']).size().reset_index(name='count')\n",
    "df_arrivals = df_arrivals.merge(counts, on=['hour', 'day', 'drop_label'], how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_departures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arrivals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of duplicates (ghosts of grouby function, no clue where they come from)\n",
    "df_departures.drop_duplicates(inplace=True)\n",
    "df_arrivals.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some sanity checks: \n",
    "print(\"Shape of dataset: \",df_departures.shape)\n",
    "print(\"Number of unique days: \",len(df_departures['Date'].unique()))\n",
    "print(\"Number of unique hours: \",len(df_departures['hour'].unique()))\n",
    "print(\"Number of unique labels: \",len(df_departures['pick_label'].unique()))\n",
    "print(\"Expected shape after grouping =\", 24*365*19, \". Assuming that cluster 12 was removed as argued previously (only contained sations where arrivals were clocked but no departures).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_departures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are entries with T and M in our weather data. What do these mean?\n",
    "- T - This means that only a trace of precipitation has fallen.  Trace is defined as less than the smallest measurable amount.  That threshold is below for the different precipitation measurements:\n",
    "    - Liquid precipitation (rain, showers) - Less than 0.005 inches\n",
    "    - Snowfall - Less than 0.05 inches\n",
    "    - Snow depth on the ground - Less than 0.5 inches\n",
    "- M - Means that the data is missing.  This can happen for a variety of reasons including the data did not make a quality check, there was an equipment outage, or even the observer was not available at a manual station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are thus not usefull, so replace all T or M entries with 0\n",
    "df_departures.replace('T', 0, inplace=True)\n",
    "df_departures.replace('M', 0, inplace=True)\n",
    "df_arrivals.replace('T', 0, inplace=True)\n",
    "df_arrivals.replace('M', 0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print correlation matrix to see if there are any strong correlations between counts and other features\n",
    "df_departures.corr().style.background_gradient(cmap='coolwarm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arrivals.corr().style.background_gradient(cmap='coolwarm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that only max_temp, min_temp, avg_temp, CDD and hour have strong correlation with counts. This gives reason to believe that we should only account for those features as predictors in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Neural network using keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#import split train val test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# #how to split train and test\n",
    "# def split_train_test(df):\n",
    "#     df_train = df[df['month'] < 11]\n",
    "#     df_test = df[df['month'] >= 11]\n",
    "#     return df_train, df_test\n",
    "\n",
    "# #split data into train and test set\n",
    "# df_departures_train, df_departures_test = split_train_test(df_departures)\n",
    "# df_arrivals_train, df_arrivals_test = split_train_test(df_arrivals)\n",
    "\n",
    "# split data into train, val and test set\n",
    "df_departures_train, df_departures_test = train_test_split(df_departures, test_size=0.2)\n",
    "df_departures_train, df_departures_val = train_test_split(df_departures_train, test_size=0.2)\n",
    "df_arrivals_train, df_arrivals_test = train_test_split(df_arrivals, test_size=0.2)\n",
    "df_arrivals_train, df_arrivals_val = train_test_split(df_arrivals_train, test_size=0.2)\n",
    "\n",
    "\n",
    "# # Now that we have obtained the count, drop month and day since we have a date column. Try the alternative (delete date column) as well to see which one works better. \n",
    "# # We only needed these for the groupby function. \n",
    "# df_departures.drop(columns=['month', 'day'], inplace=True)\n",
    "# df_arrivals.drop(columns=['month', 'day'], inplace=True)\n",
    "\n",
    "\n",
    "#split data into x and y\n",
    "x_departures_train = df_departures_train[['hour', 'day']].values\n",
    "x_departures_train = np.asarray(x_departures_train).astype(np.float32)\n",
    "x_departures_val = df_departures_val[['hour', 'day']].values\n",
    "x_departures_val = np.asarray(x_departures_val).astype(np.float32)\n",
    "x_departures_test = df_departures_test[['hour', 'day']].values\n",
    "x_departures_test = np.asarray(x_departures_test).astype(np.float32)\n",
    "y_departures_train = df_departures_train['count'].values\n",
    "y_departures_train = np.asarray(y_departures_train).astype(np.float32)\n",
    "y_departures_val = df_departures_val['count'].values\n",
    "y_departures_val = np.asarray(y_departures_val).astype(np.float32)\n",
    "y_departures_test = df_departures_test['count'].values\n",
    "y_departures_test = np.asarray(y_departures_test).astype(np.float32)\n",
    "x_arrivals_train = df_arrivals_train[['hour', 'day']].values\n",
    "x_arrivals_train = np.asarray(x_arrivals_train).astype(np.float32)\n",
    "x_arrivals_val = df_arrivals_val[['hour', 'day']].values\n",
    "x_arrivals_val = np.asarray(x_arrivals_val).astype(np.float32)\n",
    "x_arrivals_test = df_arrivals_test[['hour', 'day']].values\n",
    "x_arrivals_test = np.asarray(x_arrivals_test).astype(np.float32)\n",
    "y_arrivals_train = df_arrivals_train['count'].values\n",
    "y_arrivals_train = np.asarray(y_arrivals_train).astype(np.float32)\n",
    "y_arrivals_val = df_arrivals_val['count'].values\n",
    "y_arrivals_val = np.asarray(y_arrivals_val).astype(np.float32)\n",
    "y_arrivals_test = df_arrivals_test['count'].values\n",
    "y_arrivals_test = np.asarray(y_arrivals_test).astype(np.float32)\n",
    "\n",
    "#standerdize data\n",
    "x_departures_train[:,0] = (x_departures_train[:,0] - np.mean(x_departures_train[:,0])) / np.std(x_departures_train[:,0])\n",
    "x_departures_train[:,1] = (x_departures_train[:,1] - np.mean(x_departures_train[:,1])) / np.std(x_departures_train[:,1])\n",
    "x_departures_val[:,0] = (x_departures_val[:,0] - np.mean(x_departures_val[:,0])) / np.std(x_departures_val[:,0])\n",
    "x_departures_val[:,1] = (x_departures_val[:,1] - np.mean(x_departures_val[:,1])) / np.std(x_departures_val[:,1])\n",
    "x_departures_test[:,0] = (x_departures_test[:,0] - np.mean(x_departures_test[:,0])) / np.std(x_departures_test[:,0])\n",
    "x_departures_test[:,1] = (x_departures_test[:,1] - np.mean(x_departures_test[:,1])) / np.std(x_departures_test[:,1])\n",
    "x_arrivals_train[:,0] = (x_arrivals_train[:,0] - np.mean(x_arrivals_train[:,0])) / np.std(x_arrivals_train[:,0])\n",
    "x_arrivals_train[:,1] = (x_arrivals_train[:,1] - np.mean(x_arrivals_train[:,1])) / np.std(x_arrivals_train[:,1])\n",
    "x_arrivals_val[:,0] = (x_arrivals_val[:,0] - np.mean(x_arrivals_val[:,0])) / np.std(x_arrivals_val[:,0])\n",
    "x_arrivals_val[:,1] = (x_arrivals_val[:,1] - np.mean(x_arrivals_val[:,1])) / np.std(x_arrivals_val[:,1])\n",
    "x_arrivals_test[:,0] = (x_arrivals_test[:,0] - np.mean(x_arrivals_test[:,0])) / np.std(x_arrivals_test[:,0])\n",
    "x_arrivals_test[:,1] = (x_arrivals_test[:,1] - np.mean(x_arrivals_test[:,1])) / np.std(x_arrivals_test[:,1])\n",
    "\n",
    "y_departures_train = (y_departures_train - np.mean(y_departures_train)) / np.std(y_departures_train)\n",
    "y_departures_val = (y_departures_val - np.mean(y_departures_val)) / np.std(y_departures_val)\n",
    "y_departures_test = (y_departures_test - np.mean(y_departures_test)) / np.std(y_departures_test)\n",
    "y_arrivals_train = (y_arrivals_train - np.mean(y_arrivals_train)) / np.std(y_arrivals_train)\n",
    "y_arrivals_val = (y_arrivals_val - np.mean(y_arrivals_val)) / np.std(y_arrivals_val)\n",
    "y_arrivals_test = (y_arrivals_test - np.mean(y_arrivals_test)) / np.std(y_arrivals_test)\n",
    "\n",
    "#Train neural network\n",
    "model_departures = Sequential()\n",
    "model_departures.add(Dense(1000, activation='relu', input_dim=2))\n",
    "model_departures.add(Dropout(0.5))\n",
    "model_departures.add(Dense(1000, activation='relu'))\n",
    "model_departures.add(Dropout(0.5))\n",
    "model_departures.add(Dense(1, activation='relu'))\n",
    "\n",
    "model_departures.compile(loss='mean_squared_error',\n",
    "                         optimizer='adam',\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "# model_departures.fit(x_departures_train, y_departures_train,\n",
    "#                      epochs=20,\n",
    "#                      batch_size=128)\n",
    "model_departures.fit(x_departures_train, y_departures_train,\n",
    "                        epochs=20,\n",
    "                        batch_size=128,\n",
    "                        validation_data=(x_departures_val, y_departures_val))\n",
    "\n",
    "#Train neural network\n",
    "model_arrivals = Sequential()\n",
    "model_arrivals.add(Dense(1000, activation='relu', input_dim=2))\n",
    "model_arrivals.add(Dropout(0.5))\n",
    "model_arrivals.add(Dense(1000, activation='relu'))\n",
    "model_arrivals.add(Dropout(0.5))\n",
    "model_arrivals.add(Dense(1, activation='relu'))\n",
    "\n",
    "model_arrivals.compile(loss='mean_squared_error',\n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# model_arrivals.fit(x_arrivals_train, y_arrivals_train, \n",
    "#                    epochs=20,\n",
    "#                    batch_size=128)\n",
    "model_arrivals.fit(x_arrivals_train, y_arrivals_train, \n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(x_arrivals_val, y_arrivals_val))\n",
    "\n",
    "#predict data\n",
    "y_departures_pred = model_departures.predict(x_departures_test)\n",
    "y_arrivals_pred = model_arrivals.predict(x_arrivals_test)\n",
    "\n",
    "#plot data\n",
    "plt.scatter(y_departures_test, y_departures_pred)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()\n",
    "\n",
    "#plot data\n",
    "plt.scatter(y_arrivals_test, y_arrivals_pred)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()\n",
    "\n",
    "#print r^2 score\n",
    "print('r^2 score for departures: {}'.format(r2_score(y_departures_test, y_departures_pred)))\n",
    "print('r^2 score for arrivals: {}'.format(r2_score(y_arrivals_test, y_arrivals_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#That was bad, try a CNN instead \n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras import backend as K\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#reshape data\n",
    "x_departures_train = df_departures_train[['Max_temp', 'Min_temp', 'Avg_temp', 'CDD', 'hour']].values\n",
    "x_departures_train = np.asarray(x_departures_train).astype(np.float32)\n",
    "x_departures_train = x_departures_train.reshape(x_departures_train.shape[0], 1, 5, 1)\n",
    "x_departures_test = df_departures_test[['Max_temp', 'Min_temp', 'Avg_temp', 'CDD', 'hour']].values\n",
    "x_departures_test = np.asarray(x_departures_test).astype(np.float32)\n",
    "x_departures_test = x_departures_test.reshape(x_departures_test.shape[0], 1, 5, 1)\n",
    "y_departures_train = df_departures_train['count'].values\n",
    "y_departures_train = np.asarray(y_departures_train).astype(np.float32)\n",
    "y_departures_test = df_departures_test['count'].values\n",
    "y_departures_test = np.asarray(y_departures_test).astype(np.float32)\n",
    "x_arrivals_train = df_arrivals_train[['Max_temp', 'Min_temp', 'Avg_temp', 'CDD', 'hour']].values\n",
    "x_arrivals_train = np.asarray(x_arrivals_train).astype(np.float32)\n",
    "x_arrivals_train = x_arrivals_train.reshape(x_arrivals_train.shape[0], 1, 5, 1)\n",
    "x_arrivals_test = df_arrivals_test[['Max_temp', 'Min_temp', 'Avg_temp', 'CDD', 'hour']].values\n",
    "x_arrivals_test = np.asarray(x_arrivals_test).astype(np.float32)\n",
    "x_arrivals_test = x_arrivals_test.reshape(x_arrivals_test.shape[0], 1, 5, 1)\n",
    "y_arrivals_train = df_arrivals_train['count'].values\n",
    "y_arrivals_train = np.asarray(y_arrivals_train).astype(np.float32)\n",
    "y_arrivals_test = df_arrivals_test['count'].values\n",
    "y_arrivals_test = np.asarray(y_arrivals_test).astype(np.float32)\n",
    "\n",
    "#train convolutional neural network\n",
    "model_departures = Sequential()\n",
    "model_departures.add(Conv2D(32, kernel_size=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(1, 5, 1)))\n",
    "model_departures.add(BatchNormalization())\n",
    "model_departures.add(Conv2D(32, (1, 1), activation='relu'))\n",
    "model_departures.add(BatchNormalization())\n",
    "model_departures.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model_departures.add(Dropout(0.25))\n",
    "model_departures.add(Flatten())\n",
    "model_departures.add(Dense(128, activation='relu'))\n",
    "model_departures.add(BatchNormalization())\n",
    "model_departures.add(Dropout(0.5))\n",
    "model_departures.add(Dense(1, activation='relu'))\n",
    "\n",
    "model_departures.compile(loss='mean_squared_error',\n",
    "                            optimizer='adam',\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "model_departures.fit(x_departures_train, y_departures_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128)\n",
    "\n",
    "#train convolutional neural network\n",
    "model_arrivals = Sequential()\n",
    "model_arrivals.add(Conv2D(32, kernel_size=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(1, 5, 1)))\n",
    "model_arrivals.add(BatchNormalization())\n",
    "model_arrivals.add(Conv2D(32, (1, 1), activation='relu'))\n",
    "model_arrivals.add(BatchNormalization())\n",
    "model_arrivals.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model_arrivals.add(Dropout(0.25))\n",
    "model_arrivals.add(Flatten())\n",
    "model_arrivals.add(Dense(128, activation='relu'))\n",
    "model_arrivals.add(BatchNormalization())\n",
    "model_arrivals.add(Dropout(0.5))\n",
    "model_arrivals.add(Dense(1, activation='relu'))\n",
    "\n",
    "model_arrivals.compile(loss='mean_squared_error',\n",
    "                            optimizer='adam',\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "model_arrivals.fit(x_arrivals_train, y_arrivals_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128)\n",
    "\n",
    "#predict data\n",
    "y_departures_pred = model_departures.predict(x_departures_test)\n",
    "y_arrivals_pred = model_arrivals.predict(x_arrivals_test)\n",
    "\n",
    "#plot data\n",
    "plt.scatter(y_departures_test, y_departures_pred)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()\n",
    "\n",
    "#plot data\n",
    "plt.scatter(y_arrivals_test, y_arrivals_pred)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()\n",
    "\n",
    "#print r^2 score\n",
    "print('r^2 score for departures: {}'.format(r2_score(y_departures_test, y_departures_pred)))\n",
    "print('r^2 score for arrivals: {}'.format(r2_score(y_arrivals_test, y_arrivals_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_departures_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_departures_train, y_departures_train = create_x_y(df_departures_train)\n",
    "# def create_x_y(df):\n",
    "#     x = df[['hour', 'day', 'label', 'month']].values\n",
    "#     y = df['count'].values\n",
    "#     return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================================================================================================================================================================================================\n",
    "======================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A\n",
    "\n",
    "This appendix contains intuitive explanations for the choices made in the report. \n",
    "\n",
    "- **Appendix A1:** The weekday and weekend split has been made due to the observation of morning and evening rushes on weekdays, which were absent in the weekends. Hence predictions would be better when the data is split up, and a separate model tailored to the presence/absence of rushes would be beneficial. \n",
    "\n",
    "- **Appendix A2:** This is because the absence of departures data caused errors in the parametric code. This could be fixed by making separate code for this cluster specifically, however, since it was already not required to do it for all clusters and the data available for this cluster was very sparse (hence the model would not perform too well), we decided to neglect this cluster in the prediction challenges of part 2 & 3.\n",
    "\n",
    "- **Appendix A3:** Since each data entry has 2 temporal variables, aggregating the data entries by hour would yield aggregation based on only one of the two hours present, aggregation on the departure times only for example. Hence, splitting up the dataset into departures and arrivals allows us to aggregate the data entries per cluster, hour, and date while keeping all information necessary for prediction of the departures and arrivals later on in pt. 3\n",
    "\n",
    "- **Appendix A4:** They either yielded poor prediction quality if included, or are simply not necessary information anymore for prediction\n",
    "\n",
    "- **Appendix A5:** The amount of missing datapoints within each cluster varies significantly between clusters and can be a good explanation for the poor performance of other models in the future. This is because the more missing datapoints, the more nonexistent lagged information the model will use (see further), hence the poorer the information the model can use for prediction, hence the poorer prediction performance.\n",
    "\n",
    "- **Appendix A6:** Since we will be including lagged departures/arrivals counts in our input data, we have to evaluate the implications of this. \n",
    "- At any moment in time, when predicting for the following hour, you will have all lagged data available to you, so there is no problem. \n",
    "- If one desires to predict further than one hour in advance, a problem occurs. Say one desires to predict 2 hours into the future. This prediction requires knowledge of the departures/arrivals count at 1 hour in the future, which, unless you can predict the future yourself (in that case please contact us), the data is unavailable for. Hence, as soon as one desires to predict further into the future than 1 hour, the inclusion of the lagged counts in the input dataset will become an issue. This problem is especially prevalent in pt. 3 of the assignment. We will fix this issue by recursively adding the prediction to the input dataset and predicting again, which is the only solution to acquire the lagged counts of future values necessary to predict further into the future with the model trained on this input data.\n",
    "\n",
    "- **Appendix A7:** Avg_tripduration and lags have been left out since the prediction model predicting this variable performed very poorly. We need a prediction model for this variable due to the issue stated above. And since the model performed so poorly, the variable could not be included as it would yield increasingly bad predictions the further we went into the future for pt.3. Hence, only the lags on departures/arrivals counts have been included. This has proven to yield a very similar and slightly better accuracy as the model with avg_tripduration and lags included, hence leaving it out of the input highly benefitted pt.3, while keeping performance on pt.2 satisfactory. Avg_gender, month and date all yielded poor prediction quality. The \"day of week\" (Mon, Tue, ...) will be added instead. \n",
    "\n",
    "- **Appendix A8:** For the first day entry, we don't have any previous datapoints, so we cannot add lagged features for the first datapoint. For the second entry, we only have one lagged datapoint, etc. There aren't sufficient previous datapoints to make lagged data for the first 30 data entries, hence they will be dropped by the BuildLaggedFeatures function. Our explanation can be proven by looking at the date and hour of the first entry in the lagged dataset visible above, remember the dates and hours have been sorted. \n",
    "\n",
    "- **Appendix A9:** 1 column of ones, 1 column of the \"day of week\" as a continuous variable (0-6), 7 columns of dummy variables for the \"day of week\", 1 column for the hour, and 24 columns of lagged departures/arrivals counts.\n",
    "\n",
    "- **Appendix A10:** 1 column for the ones, 1 column for the \"day of week\" as continuous number 0-6, 7 columns for the \"day of week\" dummies, 1 column for the hour, n_lag cols of lagged departures/arrivals counts. This leaves us with (10 + n_lag) columns. Then we add polynomial features to the hour and departures/arrivals counts lags, and concatenate the resulting columns. \n",
    "\n",
    "- **Appendix A11:** 1 column for the ones, 1 column for the \"day of week\" as continuous number 0-6, 7 columns for the \"day of week\" dummies, 1 column for the hour, 1 column for sin(hour), n_lag columns of lagged departures/arrivals counts. This leaves us with (10 + n_lag) columns. Then we add polynomial features to the hour and departures/arrivals counts lags, and concatenate the resulting columns.\n",
    "\n",
    "- **Appendix A12:** 1 column for the ones, 1 column for the \"day of week\" as continuous number 0-6, 7 columns for the \"day of week\" dummies, 1 column for the hour, 1 column for sin(hour), 1 column for cos(hour), n_lag columns of lagged departures/arrivals counts. This leaves us with (10 + n_lag) columns. Then we add polynomial features to the hour and departures/arrivals counts lags, and concatenate the resulting columns.\n",
    "\n",
    "- **Appendix A13:** This is because, for weekdays, morning and evening rush hours have been observed which were absent on the weekends. Hence, by splitting up the dataset, the model could make more accurate predictions, which is what was observed when comparing the non-split-up and split-up prediction performances.  \n",
    "\n",
    "- **Appendix A14:** We desire the model to predict the number of departures/arrivals happening one hour after the lagged counts input data for each cluster, for weekdays or weekends. That is, using departures/arrivals counts occurring in that cluster for the past 24 hours, predict the number of departures/arrivals happening the next hour. Thus, the target data is simply the number of departures/arrivals happening at that hour after the past 24-hour lagged data. This data will be extracted in the cell below. Additionally, we again split the target data into weekdays and weekends to provide the models with the correct target data. \n",
    "\n",
    "- **Appendix A15:** For the input datasets: Note that the first column in all input datasets only consists of ones, hence the standard deviation of all the data in this column will be 0 and standardizing the input data with this column included will yield a division by zero error. To avoid this, we will neglect the first column in the standardisation process and then add the column of ones back in. Additionally, the \"day of week\" categorical columns stored in columns 2-8 will also be excluded from the standardization process, as standardising these dummy variables is nonsensical. For the target variable datasets: we can safely standardise the entirety of the dataset. \n",
    "\n",
    "- **Appendix A16:** Due to the standardization process, we will not use Lasso, since a lot of the data is on a small scale. This means that the coefficients will be small as well. Lasso would drag these still relevant coefficients to 0 and caused very poor performance. Ridge yielded more accurate results than LinearRegression.\n",
    "\n",
    "- **Appendix A17:** The decreasing prediction performance trend is most likely because we ordered the clusters by the largest to the smallest amount of datapoints (previously visible in the shape checks). This means that the clusters to the right have fewer datapoints to use for prediction. Additionally, the missing datapoints are dispersed in time rather than showing a regular trend (*), making the lack of data even more detrimental. Why? This dispersed lack of data will cause frequent occurrences of the situation where you don't have a lagged departures/arrivals count datapoint at the correct hour at your disposal. This means that you will use a lagged departures/arrivals counts datapoint from a previous hour. Hence, to predict in the future, you will use lagged data, not from the previous 30 hours, but from the previous 36 (if 6 hours are missing), making predictions worse.\n",
    "\n",
    "- **Appendix A18:** For pt. 3 we will predict 24 hours into the future. However, as explained previously, to predict one hour into the future, our model requires data on the previous 24 hours. This data, since we predict the future, is not always available. In order to solve this issue, we apply a step approach where we implement the model prediction for the previous future hour timestamp as input to the model such that it can predict the following future hour timestamp. The current issue is that our model uses lags on both departures/arrivals counts, and departures/arrivals counts². We already have a model which can predict the departures/arrivals counts for the next hour, however, the question is how we will obtain a value for the departures/arrivals counts². \n",
    "\n",
    "    An intuitive solution would be to simply square the prediction output for the departures/arrivals counts. However, there is a vital mistake with this approach. Our departures/arrivals count predictor model namely returns the standardised departures/arrivals counts value, a consequence of supplying the model with both input and output standardised data. One could ask themselves why the target variable was standardised in the first place, this was because this yielded better prediction accuracy (if no standardisation was present, some R^2 were even negative). Thus, the prediction output cannot simply be squared, since the square of a standard deviation is not equal to the standard deviation of the squares, and thus our prediction would be wrong. \n",
    "\n",
    "    Unstanderdizing the predicted departures/arrivals counts, squaring and standardizing would also not be an option. An important remark for this method is that one has to use the training dataset distribution (mean and std) for this method. Usage of the test set is not allowed, since in pt.3 we use a snippet from the test set to make the future prediction. While in real life we would predict further than the scope of the test set, currently we do not have information on this future data. For this reason, we should not use the distribution of the test set, since acquiring the means and std from the entire test set means using data which goes beyond our snippet, and which is thus future data we would normally have no information on. The same reason holds for why we cannot use the mean and std from the full data (train + test). We have attempted to perform this step with the distribution from the training set only, however, this yielded poor results. \n",
    "\n",
    "    The final remaining option is to then create a model for predicting the standardised departures/arrivals counts² values, which is the approach we chose. Note that the creation of the input dataset should be exactly the same as before. If we were to use another dataset, we would have to check whether all elements used in the input were available in the future as well. Luckily, the same input dataset yielded acceptable results. The reason why we recomputed the exact same dataset was to see if for the prediction at hand a different combination of poly degree, lag, and added sine or cosine features would be more beneficial. To our surprise, using the exact same combination as before yielded the best results. Luckily recomputation of the input dataset takes very little time, so we left it here for you to play around with ;). \n",
    "\n",
    "- **Appendix A19:** A question one might ask is why we make use of a snippet from the test set for this part. We chose a snippet from the test set rather than the training set since this allows us to use the distribution (mean, std) from the training set (as this data is \"in the past\" for the test set and thus safe to use). Furthermore, we chose a snippet from the test set rather than creating random data, since random data does not have the underlying distribution one would normally encounter for the counts of the bikes. This would make the lagged features quite useless in predicting the next hour. \n",
    "\n",
    "- **Appendix A20:** The reason for degree = 2 is that we only have prediction models for departures/arrivals counts and counts², a higher degree would require prediction models for higher degrees. While we could perfectly create these models and make them available such that one can use models taking into account higher degree features for the current future prediction, degree 2 has proven to yield the highest prediction accuracy, so no higher degree is made available for this step. Then, we could also perfectly add the possibility to change the features taken into account to \"poly\", \"poly + sine\" or \"poly + sine + cosine\", however again, \"poly + sine + cosine\" has proven to yield the highest prediction accuracy, so instead of making the other options available (which would have required a number of if statements since a different amount of features jumbles up the order of the columns), for brevity's and accuracy's sake, we only made the \"poly + sine + cosine\" option available.\n",
    "\n",
    "- **Appendix A21:** Note that a snippet has to end at 11 PM due to the determination of the \"day of week\" dummy variable. Also note that we need to input the standardised value for each version of an hour (standard, sine, cosine). We can find these in the snippet of data we took from the test set. Since the last data entry in the snippet corresponds to 11 PM, the value in the standard hour column is the standardised value for 11 PM, the value in the cos(hour) column will be the standardized value for cos(11 PM), same for sine. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
