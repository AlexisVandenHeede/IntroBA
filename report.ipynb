{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Business Analytics Assignment 1\n",
    "\n",
    "### Contributors\n",
    "- StanisÅ‚aw Howard\n",
    "- Alexis Van den Heede, s231860\n",
    "- Matthias Van Mechelen\n",
    "- Sven Palac, s231799\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 803. MiB for an array with shape (6, 17548339) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Matth\\OneDrive\\Documents\\GitHub\\IntroBA\\report.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documents/GitHub/IntroBA/report.ipynb#Y125sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mdata/Trips_2018.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1721\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1718\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1719\u001b[0m         new_rows \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(index)\n\u001b[1;32m-> 1721\u001b[0m     df \u001b[39m=\u001b[39m DataFrame(col_dict, columns\u001b[39m=\u001b[39;49mcolumns, index\u001b[39m=\u001b[39;49mindex)\n\u001b[0;32m   1723\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_currow \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_rows\n\u001b[0;32m   1724\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    703\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    704\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    705\u001b[0m     )\n\u001b[0;32m    707\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    708\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 709\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    710\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    711\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    479\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 481\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:153\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    150\u001b[0m axes \u001b[39m=\u001b[39m [columns, index]\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mblock\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 153\u001b[0m     \u001b[39mreturn\u001b[39;00m create_block_manager_from_column_arrays(\n\u001b[0;32m    154\u001b[0m         arrays, axes, consolidate\u001b[39m=\u001b[39;49mconsolidate, refs\u001b[39m=\u001b[39;49mrefs\n\u001b[0;32m    155\u001b[0m     )\n\u001b[0;32m    156\u001b[0m \u001b[39melif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:2137\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[1;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_block_manager_from_column_arrays\u001b[39m(\n\u001b[0;32m   2120\u001b[0m     arrays: \u001b[39mlist\u001b[39m[ArrayLike],\n\u001b[0;32m   2121\u001b[0m     axes: \u001b[39mlist\u001b[39m[Index],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2133\u001b[0m     \u001b[39m# These last three are sufficient to allow us to safely pass\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m     \u001b[39m#  verify_integrity=False below.\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2137\u001b[0m         blocks \u001b[39m=\u001b[39m _form_blocks(arrays, consolidate, refs)\n\u001b[0;32m   2138\u001b[0m         mgr \u001b[39m=\u001b[39m BlockManager(blocks, axes, verify_integrity\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   2139\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:2215\u001b[0m, in \u001b[0;36m_form_blocks\u001b[1;34m(arrays, consolidate, refs)\u001b[0m\n\u001b[0;32m   2212\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(dtype\u001b[39m.\u001b[39mtype, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m)):\n\u001b[0;32m   2213\u001b[0m     dtype \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39mobject\u001b[39m)\n\u001b[1;32m-> 2215\u001b[0m values, placement \u001b[39m=\u001b[39m _stack_arrays(\u001b[39mlist\u001b[39;49m(tup_block), dtype)\n\u001b[0;32m   2216\u001b[0m \u001b[39mif\u001b[39;00m is_dtlike:\n\u001b[0;32m   2217\u001b[0m     values \u001b[39m=\u001b[39m ensure_wrapped_if_datetimelike(values)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:2255\u001b[0m, in \u001b[0;36m_stack_arrays\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   2252\u001b[0m first \u001b[39m=\u001b[39m arrays[\u001b[39m0\u001b[39m]\n\u001b[0;32m   2253\u001b[0m shape \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(arrays),) \u001b[39m+\u001b[39m first\u001b[39m.\u001b[39mshape\n\u001b[1;32m-> 2255\u001b[0m stacked \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(shape, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m   2256\u001b[0m \u001b[39mfor\u001b[39;00m i, arr \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(arrays):\n\u001b[0;32m   2257\u001b[0m     stacked[i] \u001b[39m=\u001b[39m arr\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 803. MiB for an array with shape (6, 17548339) and data type float64"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/Trips_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun this to save time if you make a typo and need original df again\n",
    "df = df_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col 0 is unnamed, change name to trip_id convert column to index\n",
    "df.rename(columns={'Unnamed: 0':'trip_id'}, inplace=True)\n",
    "df.set_index('trip_id', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  convert start time and stop time to datetime objects\n",
    "df['starttime'] = pd.to_datetime(df['starttime'], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "df['stoptime'] = pd.to_datetime(df['stoptime'], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make user type dummy, get shape\n",
    "df = pd.get_dummies(df, columns=['usertype'], dtype=int, drop_first=True)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting coordinates\n",
    "def plot_map(coords):\n",
    "    plt.scatter(coords[:,0], coords[:,1], s=0.75)\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lat and long\n",
    "coords = df[['start_station_longitude','start_station_latitude']].values\n",
    "coords = np.unique(coords, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(coords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice outlier (Canada, Montreal). quickly get rid of it to get an overview of the rest of the data. We will not yet delete the outlier datapoint from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of outlier longitude > -73.6\n",
    "coords = coords[coords[:,0] < -73.6]\n",
    "print(coords.shape) #shape is 917 here as it still includes the grid anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(coords)\n",
    "# shows the unique start and end stations present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice very structured grid in top right corner. After investigation these data points were NaN's. We will rid of these data points, and at the same time we get rid of the canada outlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print where nan is in df, get rid of nan (= get rid of gridded outliers), create copy of df and start station coordinates\n",
    "print(df.isnull().sum())\n",
    "df = df[~np.isnan(df['start_station_id'])]\n",
    "df = df[~np.isnan(df['end_station_id'])]\n",
    "# get rid of Canada outlier\n",
    "df = df[df['start_station_longitude'] < -73.6]\n",
    "df = df[df['end_station_longitude'] < -73.6]\n",
    "# get lat and long\n",
    "coords_start = df[['start_station_longitude','start_station_latitude']].values\n",
    "coords_start = np.unique(coords_start, axis=0)\n",
    "coords_start_copy = coords_start.copy()\n",
    "print(coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(coords_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of distinct end stations does not match amount of start stations as seen from comparing shapes, create copy of end station coordinates.\n",
    "coords_end = df[['end_station_longitude','end_station_latitude']].values\n",
    "coords_end = np.unique(coords_end, axis=0)\n",
    "print(coords_end.shape)\n",
    "print(coords_start.shape)\n",
    "coords_end_copy = coords_end.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what does the difference look like visually?\n",
    "plot_map(coords_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight the end stations that are not in the start stations on the map of all en\n",
    "coords_end_not_in_start = []\n",
    "for i in range(len(coords_end)):\n",
    "    if coords_end[i] not in coords_start:\n",
    "        coords_end_not_in_start.append(coords_end[i])\n",
    "coords_end_not_in_start = np.array(coords_end_not_in_start)\n",
    "plt.scatter(coords_start[:,0], coords_start[:,1], s=0.75)\n",
    "plt.ylabel('Latitude')\n",
    "plt.xlabel('Longitude')\n",
    "plt.scatter(coords_end_not_in_start[:,0], coords_end_not_in_start[:,1], s=0.75, c='r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are more end stations than start stations and all start stations are also an end station, the clustering should be done using the end stations in order to cluster every station considered in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering the stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kmeans\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make kmeans model\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(coords_end) # using coords here instead of df to increase speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on coords instead of DataFrame to increase speed, then predict to save labels on df, model is trained on same data as you predict the cluster for, so the cluster they belong to will be the same one as they belonged to during convergence of the Kmeans cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['drop_label'] = kmeans.predict(df[['end_station_longitude','end_station_latitude']].values)\n",
    "df['pick_label'] = kmeans.predict(df[['start_station_longitude','start_station_latitude']].values)\n",
    "df_copy2 = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with cluster colour & centriods\n",
    "plt.scatter(coords_end[:,0], coords_end[:,1], c=kmeans.labels_, s=5)\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c='red', marker=\"x\", s=50)\n",
    "plt.ylabel('Latitude')\n",
    "plt.xlabel('Longitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell for coords of starting stations and the fully edited dataframe\n",
    "coords_start = coords_start_copy.copy()\n",
    "coords_end = coords_end_copy.copy()\n",
    "df = df_copy2.copy()\n",
    "print(df.shape) # to check if no data loss, should be (17548339, 15)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now part 2**\n",
    "\n",
    "We need to predict the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find n largest pick cluster, randomly chose pick label over drop label\n",
    "n = 1\n",
    "largest_cluster = df['pick_label'].value_counts().nlargest(n).index[:n]\n",
    "print(largest_cluster)\n",
    "# get all rows with largest cluster \n",
    "df = df[df['pick_label'].isin(largest_cluster)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st plot amount of predictions per hour per cluster\n",
    "grouby_label = df.groupby(['pick_label', df['starttime'].dt.hour]).size().reset_index(name='count')\n",
    "grouby_label = grouby_label.pivot(index='starttime', columns='pick_label', values='count')\n",
    "grouby_label.plot(figsize=(20,10), legend=True)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Hour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by splitting up the dataset. Necessary because how are you going to aggregate the data by hour if each datapoint has 2 temporal parameters (start and end station times). Note that we can only use the hour and cluster as input to our model, as these are the only variables we have certain information on for the future datapoints for which we predict the demand, so we only have to copy these variables over in the new datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset\n",
    "df_departures = df[[\"starttime\", \"pick_label\"]]\n",
    "df_arrivals = df[[\"stoptime\", \"drop_label\"]]\n",
    "\n",
    "#replace the starttime datetime object by a column with hour and cols with date DD, MM. For df_departures and stoptime, DD, MM for df_arrivals\n",
    "df_departures[\"start_hour\"] = df_departures[\"starttime\"].dt.hour\n",
    "df_departures[\"start_day\"] = df_departures[\"starttime\"].dt.day\n",
    "df_departures[\"start_month\"] = df_departures[\"starttime\"].dt.month\n",
    "df_arrivals[\"stop_hour\"] = df_arrivals[\"stoptime\"].dt.hour\n",
    "df_arrivals[\"stop_day\"] = df_arrivals[\"stoptime\"].dt.day\n",
    "df_arrivals[\"stop_month\"] = df_arrivals[\"stoptime\"].dt.month\n",
    "df_departures.drop(columns=[\"starttime\"], inplace=True)\n",
    "df_arrivals.drop(columns=[\"stoptime\"], inplace=True)\n",
    "\n",
    "\n",
    "#remove trip_id index. We will aggregate the data by hour, so this will lost its meaning. \n",
    "df_departures.reset_index(drop=True, inplace=True)\n",
    "df_arrivals.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Count the amount of dataframe rows that have the same label, hour, day and month. and add the count as extra column\n",
    "df_departures = df_departures.groupby([\"pick_label\", \"start_hour\", \"start_day\", \"start_month\"]).size().reset_index(name=\"count\")\n",
    "df_arrivals = df_arrivals.groupby([\"drop_label\", \"stop_hour\", \"stop_day\", \"stop_month\"]).size().reset_index(name=\"count\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: days close to eachother being linked in prediction is already included in the model as you have days and months in your model. And i dont think that it matters that i split up days and months (my guess what taht the model believes the first day of each month is closely related which it isnt. But the month is also included, nevertheless it is a fair thought. Maybe it is better to include the day and month as a counter instead, this will avoid the first day of each month being related! (as yes the months differentiate them, but the first of jun is still close in dist to the first of jan in my method, while with a continuous counter this would really not be the case. Change later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arrivals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_departures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train two Random forest regressors. One to predict the amount of departures and one to predict the amount of arrivals. The data has to be split as required in the assignment: Training data contains data from januari - October. Test data contains data from November - December. Hence we will sort the data by month and exploit this sort to make the split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the data by month in ascending order\n",
    "df_departures.sort_values(by=[\"start_month\"], inplace=True)\n",
    "df_arrivals.sort_values(by=[\"stop_month\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "#split the data into train and test data, training data from month 1-10, test data from month 11-12. \n",
    "X_train_dep = df_departures[df_departures[\"start_month\"] < 11].drop(columns=[\"count\"])\n",
    "X_test_dep = df_departures[df_departures[\"start_month\"] >= 11].drop(columns=[\"count\"])\n",
    "y_train_dep = df_departures[df_departures[\"start_month\"] < 11][\"count\"]\n",
    "y_test_dep = df_departures[df_departures[\"start_month\"] >= 11][\"count\"]\n",
    "X_train_arr = df_arrivals[df_arrivals[\"stop_month\"] < 11].drop(columns=[\"count\"])\n",
    "X_test_arr = df_arrivals[df_arrivals[\"stop_month\"] >= 11].drop(columns=[\"count\"])\n",
    "y_train_arr = df_arrivals[df_arrivals[\"stop_month\"] < 11][\"count\"]\n",
    "y_test_arr = df_arrivals[df_arrivals[\"stop_month\"] >= 11][\"count\"]\n",
    "\n",
    "#control check\n",
    "print(X_train_dep[\"start_month\"].unique())\n",
    "print(X_train_arr[\"stop_month\"].unique())\n",
    "print(X_test_dep[\"start_month\"].unique())\n",
    "print(X_test_arr[\"stop_month\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the decision tree regressors\n",
    "reg_dep = RandomForestRegressor(random_state=0)\n",
    "reg_arr = RandomForestRegressor(random_state=0)\n",
    "reg_dep.fit(X_train_dep, y_train_dep)\n",
    "reg_arr.fit(X_train_arr, y_train_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the amount of departures and arrivals\n",
    "y_pred_dep = reg_dep.predict(X_test_dep)\n",
    "y_pred_arr = reg_arr.predict(X_test_arr)\n",
    "\n",
    "#calculate the r2 score\n",
    "r2_dep = r2_score(y_test_dep, y_pred_dep)\n",
    "r2_arr = r2_score(y_test_arr, y_pred_arr)\n",
    "print(\"R2 departures: \", r2_dep)\n",
    "print(\"R2 arrivals: \", r2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if month  are influencing the prediction in a bad way\n",
    "X_train_dep = df_departures[df_departures[\"start_month\"] < 11].drop(columns=[\"count\", \"start_month\"])\n",
    "X_test_dep = df_departures[df_departures[\"start_month\"] >= 11].drop(columns=[\"count\", \"start_month\"])\n",
    "y_train_dep = df_departures[df_departures[\"start_month\"] < 11][\"count\"]\n",
    "y_test_dep = df_departures[df_departures[\"start_month\"] >= 11][\"count\"]\n",
    "X_train_arr = df_arrivals[df_arrivals[\"stop_month\"] < 11].drop(columns=[\"count\", \"stop_month\"])\n",
    "X_test_arr = df_arrivals[df_arrivals[\"stop_month\"] >= 11].drop(columns=[\"count\", \"stop_month\"])\n",
    "y_train_arr = df_arrivals[df_arrivals[\"stop_month\"] < 11][\"count\"]\n",
    "y_test_arr = df_arrivals[df_arrivals[\"stop_month\"] >= 11][\"count\"]\n",
    "\n",
    "#train the decision tree regressors\n",
    "reg_dep = RandomForestRegressor(random_state=0)\n",
    "reg_arr = RandomForestRegressor(random_state=0)\n",
    "reg_dep.fit(X_train_dep, y_train_dep)\n",
    "reg_arr.fit(X_train_arr, y_train_arr)\n",
    "\n",
    "#predict the amount of departures and arrivals\n",
    "y_pred_dep = reg_dep.predict(X_test_dep)\n",
    "y_pred_arr = reg_arr.predict(X_test_arr)\n",
    "\n",
    "#calculate the r2 score\n",
    "r2_dep = r2_score(y_test_dep, y_pred_dep)\n",
    "r2_arr = r2_score(y_test_arr, y_pred_arr)\n",
    "print(\"R2 departures: \", r2_dep)\n",
    "print(\"R2 arrivals: \", r2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid serach for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "#split the data into train and test data, training data from month 1-10, test data from month 11-12.\n",
    "X_train_dep = df_departures[df_departures[\"start_month\"] < 11].drop(columns=[\"count\"])\n",
    "X_test_dep = df_departures[df_departures[\"start_month\"] >= 11].drop(columns=[\"count\"])\n",
    "y_train_dep = df_departures[df_departures[\"start_month\"] < 11][\"count\"]\n",
    "y_test_dep = df_departures[df_departures[\"start_month\"] >= 11][\"count\"]\n",
    "X_train_arr = df_arrivals[df_arrivals[\"stop_month\"] < 11].drop(columns=[\"count\"])\n",
    "X_test_arr = df_arrivals[df_arrivals[\"stop_month\"] >= 11].drop(columns=[\"count\"])\n",
    "y_train_arr = df_arrivals[df_arrivals[\"stop_month\"] < 11][\"count\"]\n",
    "y_test_arr = df_arrivals[df_arrivals[\"stop_month\"] >= 11][\"count\"]\n",
    "\n",
    "#control check\n",
    "print(X_train_dep[\"start_month\"].unique())\n",
    "print(X_train_arr[\"stop_month\"].unique())\n",
    "print(X_test_dep[\"start_month\"].unique())\n",
    "print(X_test_arr[\"stop_month\"].unique())\n",
    "\n",
    "#make a scorer for the grid search\n",
    "scorer = make_scorer(r2_score)\n",
    "\n",
    "#make a parameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100, 200],\n",
    "    \"max_depth\": [None, 5, 10, 20, 30],\n",
    "    \"min_samples_split\": [2, 5, 10, 20, 50],\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10, 20, 50]\n",
    "}\n",
    "\n",
    "#make a grid search for the departures\n",
    "grid_dep = GridSearchCV(RandomForestRegressor(random_state=0), param_grid, cv=3, scoring=scorer)\n",
    "grid_dep.fit(X_train_dep, y_train_dep)\n",
    "\n",
    "#make a grid search for the arrivals\n",
    "grid_arr = GridSearchCV(RandomForestRegressor(random_state=0), param_grid, cv=3, scoring=scorer)\n",
    "grid_arr.fit(X_train_arr, y_train_arr)\n",
    "\n",
    "#predict the amount of departures and arrivals\n",
    "y_pred_dep = grid_dep.predict(X_test_dep)\n",
    "y_pred_arr = grid_arr.predict(X_test_arr)\n",
    "\n",
    "#calculate the r2 score\n",
    "r2_dep = r2_score(y_test_dep, y_pred_dep)\n",
    "r2_arr = r2_score(y_test_arr, y_pred_arr)\n",
    "print(\"R2 departures: \", r2_dep)\n",
    "print(\"R2 arrivals: \", r2_arr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(grid_dep.best_params_)\n",
    "print(grid_arr.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Month makes model worse prodbably due to split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad R^2 so feature engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lag function\n",
    "def buildLaggedFeatures(s,columns, lag=2,dropna=True):\n",
    "    '''\n",
    "    From http://stackoverflow.com/questions/20410312/how-to-create-a-lagged-data-structure-using-pandas-dataframe\n",
    "    Builds a new DataFrame to facilitate regressing over all possible lagged features\n",
    "    '''\n",
    "    if type(s) is pd.DataFrame:\n",
    "        new_dict={}\n",
    "        for c in s.columns:\n",
    "            new_dict[c]=s[c]\n",
    "        for col_name in columns:\n",
    "            new_dict[col_name]=s[col_name]\n",
    "            # create lagged Series\n",
    "            for l in range(1,lag+1):\n",
    "                new_dict['%s_lag%d' %(col_name,l)]=s[col_name].shift(l)\n",
    "        res=pd.DataFrame(new_dict,index=s.index)\n",
    "\n",
    "    elif type(s) is pd.Series:\n",
    "        the_range=range(lag+1)\n",
    "        res=pd.concat([s.shift(i) for i in the_range],axis=1)\n",
    "        res.columns=['lag_%d' %i for i in the_range]\n",
    "    else:\n",
    "        print('Only works for DataFrame or Series')\n",
    "        return None\n",
    "    if dropna:\n",
    "        return res.dropna()\n",
    "    else:\n",
    "        return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lag features\n",
    "lag = 0\n",
    "df_departures = buildLaggedFeatures(df_departures, [\"count\"], lag=lag)\n",
    "df_arrivals = buildLaggedFeatures(df_arrivals, [\"count\"], lag=lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_departures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arrivals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re train forest\n",
    "X_train_dep = df_departures[df_departures[\"start_month\"] < 11].drop(columns=[\"count\", \"start_month\"])\n",
    "X_test_dep = df_departures[df_departures[\"start_month\"] >= 11].drop(columns=[\"count\", \"start_month\"])\n",
    "y_train_dep = df_departures[df_departures[\"start_month\"] < 11][\"count\"]\n",
    "y_test_dep = df_departures[df_departures[\"start_month\"] >= 11][\"count\"]\n",
    "X_train_arr = df_arrivals[df_arrivals[\"stop_month\"] < 11].drop(columns=[\"count\", \"stop_month\"])\n",
    "X_test_arr = df_arrivals[df_arrivals[\"stop_month\"] >= 11].drop(columns=[\"count\", \"stop_month\"])\n",
    "y_train_arr = df_arrivals[df_arrivals[\"stop_month\"] < 11][\"count\"]\n",
    "y_test_arr = df_arrivals[df_arrivals[\"stop_month\"] >= 11][\"count\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_dep = RandomForestRegressor(random_state=0)\n",
    "reg_arr = RandomForestRegressor(random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_dep.fit(X_train_dep, y_train_dep)\n",
    "reg_arr.fit(X_train_arr, y_train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_dep = reg_dep.predict(X_test_dep)\n",
    "y_pred_arr = reg_arr.predict(X_test_arr)\n",
    "\n",
    "r2_dep = r2_score(y_test_dep, y_pred_dep)\n",
    "r2_arr = r2_score(y_test_arr, y_pred_arr)\n",
    "print(\"R2 departures: \", r2_dep)\n",
    "print(\"R2 arrivals: \", r2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistical regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "# add square of each x\n",
    "X_train_dep = df_departures[df_departures[\"start_month\"] < 11].drop(columns=[\"count\", \"start_month\"])\n",
    "X_test_dep = df_departures[df_departures[\"start_month\"] >= 11].drop(columns=[\"count\", \"start_month\"])\n",
    "y_train_dep = df_departures[df_departures[\"start_month\"] < 11][\"count\"]\n",
    "y_test_dep = df_departures[df_departures[\"start_month\"] >= 11][\"count\"]\n",
    "X_train_arr = df_arrivals[df_arrivals[\"stop_month\"] < 11].drop(columns=[\"count\", \"stop_month\"])\n",
    "X_test_arr = df_arrivals[df_arrivals[\"stop_month\"] >= 11].drop(columns=[\"count\", \"stop_month\"])\n",
    "y_train_arr = df_arrivals[df_arrivals[\"stop_month\"] < 11][\"count\"]\n",
    "y_test_arr = df_arrivals[df_arrivals[\"stop_month\"] >= 11][\"count\"]\n",
    "\n",
    "X_train_dep[\"start_hour^2\"] = X_train_dep[\"start_hour\"]**2\n",
    "X_train_dep[\"start_day^2\"] = X_train_dep[\"start_day\"]**2\n",
    "X_test_dep[\"start_hour^2\"] = X_test_dep[\"start_hour\"]**2\n",
    "X_test_dep[\"start_day^2\"] = X_test_dep[\"start_day\"]**2\n",
    "X_train_arr[\"stop_hour^2\"] = X_train_arr[\"stop_hour\"]**2\n",
    "X_train_arr[\"stop_day^2\"] = X_train_arr[\"stop_day\"]**2\n",
    "X_test_arr[\"stop_hour^2\"] = X_test_arr[\"stop_hour\"]**2\n",
    "X_test_arr[\"stop_day^2\"] = X_test_arr[\"stop_day\"]**2\n",
    "\n",
    "\n",
    "reg_dep = LinearRegression()\n",
    "reg_arr = LinearRegression()\n",
    "reg_dep.fit(X_train_dep, y_train_dep)\n",
    "reg_arr.fit(X_train_arr, y_train_arr)\n",
    "y_pred_dep = reg_dep.predict(X_test_dep)\n",
    "y_pred_arr = reg_arr.predict(X_test_arr)\n",
    "r2_dep = r2_score(y_test_dep, y_pred_dep)\n",
    "r2_arr = r2_score(y_test_arr, y_pred_arr)\n",
    "print(\"R2 departures: \", r2_dep)\n",
    "print(\"R2 arrivals: \", r2_arr)\n",
    "\n",
    "print(reg_dep.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predicted vs actual amount of departures and arrivals for all clusters\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs = axs.ravel()\n",
    "axs[0].scatter(y_test_dep, y_pred_dep, s=0.75)\n",
    "axs[0].set_xlabel(\"Actual\")\n",
    "axs[0].set_ylabel(\"Predicted\")\n",
    "axs[0].set_title(\"Departures: All clusters, R2: \" + str(round(r2_dep, 3)))\n",
    "axs[1].scatter(y_test_arr, y_pred_arr, s=0.75)\n",
    "axs[1].set_xlabel(\"Actual\")\n",
    "axs[1].set_ylabel(\"Predicted\")\n",
    "axs[1].set_title(\"Arrivals: All clusters, R2: \" + str(round(r2_arr, 3)))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot the predicted vs actual amount of departures and arrivals for each cluster individually and put all plots in a compact grid. \n",
    "# THis allows us to visually evaluate how good the model is at predicting for each cluster. \n",
    "fig, axs = plt.subplots(int(n_clusters/2), 4, figsize=(20, 40))\n",
    "axs = axs.ravel()\n",
    "for i in range(int(2*n_clusters)):\n",
    "    if i%2 == 0:\n",
    "        axs[i].scatter(y_test_dep[X_test_dep[\"label\"] == i//2], y_pred_dep[X_test_dep[\"label\"] == i//2], s=0.75)\n",
    "        axs[i].set_xlabel(\"Actual\")\n",
    "        axs[i].set_ylabel(\"Predicted\")\n",
    "        axs[i].set_title(\"Departures cluster \" + str(i//2))\n",
    "        plt.tight_layout()\n",
    "    else:\n",
    "        axs[i].scatter(y_test_arr[X_test_arr[\"label\"] == int(np.floor(i//2))], y_pred_arr[X_test_arr[\"label\"] == int(np.floor(i//2))], s=0.75)\n",
    "        axs[i].set_xlabel(\"Actual\")\n",
    "        axs[i].set_ylabel(\"Predicted\")\n",
    "        axs[i].set_title(\"Arrivals cluster \" + str(int(np.floor(i//2))))\n",
    "        plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that cluster 12 contains little datapoints, this will likely be the cluster which contains the end stations which were not start stations (verify maybe?), this only works when we keep n_clusters = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree to predict count\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data per label, extract data on useful variables for training model, split data, train model, print score.\n",
    "for i in range(n_clusters):\n",
    "    df_label = df[df['label'] == i]\n",
    "    df_label = df_label[['hour', 'usertype_Subscriber', 'label']]\n",
    "    X = df_label[['hour', 'usertype_Subscriber']]\n",
    "    y = df_label['label']\n",
    "    # data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"label: \", i, \" score: \", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "# R^2\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'r^2 = {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a nn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# split data per label\n",
    "for i in range(20):\n",
    "    df_label = df[df['label'] == i]\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_label[['hour','start_station_longitude','start_station_latitude']], df_label['usertype_Subscriber'], test_size=0.2, random_state=0)\n",
    "    # scale data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    # make nn\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(10,10,10), max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"label:\",i,\"score:\",clf.score(X_test, y_test))\n",
    "    \n",
    "\n",
    "\n",
    "# get accuracy\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "# R^2\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'r^2 = {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st plot amount of predictions per hour per cluster\n",
    "grouby_label = df.groupby(['label', df['starttime'].dt.hour]).size().reset_index(name='count')\n",
    "grouby_label = grouby_label.pivot(index='starttime', columns='label', values='count')\n",
    "grouby_label.plot(figsize=(20,10))\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Hour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouby_cluster = df.groupby(['label'])\n",
    "grouby_cluster.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group same label and hour of start together\n",
    "grouby_cluster = df.groupby(['label', df['starttime'].dt.hour])\n",
    "grouby_cluster.head()\n",
    "# average all other columns\n",
    "grouby_cluster = grouby_cluster.mean()\n",
    "grouby_cluster.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of predictions per hour per cluster in df\n",
    "df['demand'] = df.groupby(['label', df['starttime'].dt.hour])['label'].transform('count')\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouby_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test (80/20)\n",
    "train = df.sample(frac=0.8,random_state=42)\n",
    "test = df.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make linear regression model per cluster/label\n",
    "models = []\n",
    "for i in range(20):\n",
    "    X = grouby_label[i].index.values.reshape(-1, 1)\n",
    "    y = grouby_label[i].values\n",
    "    regr = linear_model.LinearRegression()\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(type(X))\n",
    "    print(type(y))\n",
    "    print(X[0])\n",
    "    print(y[0])\n",
    "    regr.fit(X, y)\n",
    "    models.append(regr)\n",
    "    \n",
    "\n",
    "# for i in range(20):\n",
    "    # X = train[train['label'] == i]['starttime'].dt.hour.values.reshape(-1, 1)\n",
    "    # y = train[train['label'] == i]['tripduration'].values\n",
    "    # regr = linear_model.LinearRegression()\n",
    "    # regr.fit(X, y)\n",
    "    # models.append(regr)\n",
    "# THIS ISNT USING THE RIGHT X YET, not sure how the groupby df is working rn\n",
    "\n",
    "# for i in range(20):\n",
    "#     X = grouby_label[i].index.values.reshape(-1, 1)\n",
    "#     y = grouby_label[i].values\n",
    "#     regr = linear_model.LinearRegression()\n",
    "#     regr.fit(X, y)\n",
    "#     models.append(regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "preds = []\n",
    "for i in range(20):\n",
    "    preds.append(models[i].predict(grouby_label[i].index.values.reshape(-1, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions 4x5 grid\n",
    "fig, axs = plt.subplots(4, 5, figsize=(20, 10))\n",
    "axs = axs.ravel()\n",
    "for i in range(20):\n",
    "    axs[i].plot(grouby_label[i].index.values, grouby_label[i].values, color='black')\n",
    "    axs[i].plot(grouby_label[i].index.values, preds[i], color='blue', linewidth=3)\n",
    "    axs[i].set_title('Cluster ' + str(i))\n",
    "    axs[i].set_ylabel('Count')\n",
    "    axs[i].set_xlabel('Hour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the linear regression models for each cluster\n",
    "models = {}\n",
    "\n",
    "# Train a linear regression model for each cluster\n",
    "for cluster_label in range(20):\n",
    "    # Filter the data for the current cluster\n",
    "    cluster_data = df[df['label'] == cluster_label]\n",
    "    \n",
    "    # Extract features (X) and target (y)\n",
    "    X = cluster_data[['starttime']].values\n",
    "    y = cluster_data['demand'].values\n",
    "    \n",
    "    # Create and train the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Store the model in the dictionary\n",
    "    models[cluster_label] = model\n",
    "\n",
    "# Create an empty DataFrame to store the predictions\n",
    "predictions_df = pd.DataFrame()\n",
    "\n",
    "# Make predictions for each cluster and add them to the DataFrame\n",
    "for cluster_label in range(20):\n",
    "    # Extract the hours for which you want to make predictions\n",
    "    hours_to_predict = grouby_label.index.values\n",
    "    \n",
    "    # Create a feature matrix with these hours\n",
    "    X_predict = hours_to_predict.reshape(-1, 1)\n",
    "    \n",
    "    # Make predictions using the model for the current cluster\n",
    "    predictions = models[cluster_label].predict(X_predict)\n",
    "    \n",
    "    # Add the predictions to the DataFrame\n",
    "    predictions_df['Cluster_' + str(cluster_label)] = predictions\n",
    "\n",
    "# Plot the predictions\n",
    "predictions_df['Hour'] = hours_to_predict\n",
    "predictions_df.set_index('Hour', inplace=True)\n",
    "predictions_df.plot(figsize=(20, 10))\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Hour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matthias Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[\"start_station_id\"].unique()))\n",
    "print(len(df[\"end_station_id\"].unique()))\n",
    "coords = df[['start_station_longitude','start_station_latitude']].values\n",
    "coords = np.unique(coords, axis=0)\n",
    "print(len(coords))\n",
    "coords = df[['end_station_longitude','end_station_latitude']].values\n",
    "coords = np.unique(coords, axis=0)\n",
    "print(len(coords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy dataset and distinct coordinates\n",
    "df = df_matt.copy()\n",
    "coords = coords_copy.copy()\n",
    "\n",
    "# Create an additional column in df with the distinct station_id using the kmeans model\n",
    "for i in range(len(coords)):\n",
    "    df.loc[(data['start_station_longitude'] == coords[i][0]) & (df['start_station_latitude'] == coords[i][1]), 'station_id'] = i\n",
    "df.head()\n",
    "# print(df[\"station_id\"].unique())\n",
    "\n",
    "\n",
    "#split dataset up. Create two datasets, one with \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code treasury\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Create an empty dictionary to store the linear regression models for each station for stations within only one of the 20 clusters\n",
    "models = {}\n",
    "\n",
    "# Train a linear regression model for each station within the specified cluster\n",
    "for station_id in df[\"station_id\"].unique():\n",
    "    # Filter the data for the current station\n",
    "    station_data = df[df['station_id'] == station_id]\n",
    "    \n",
    "    # Extract features (X) and target (y)\n",
    "    X = station_data[['starttime']].values\n",
    "    y = station_data['demand'].values\n",
    "    \n",
    "    # Create and train the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Store the model in the dictionary\n",
    "    models[station_id] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaboration # not included in wordcouter.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what does bike id mean? unique id for a certain bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
